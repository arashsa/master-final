\chapter{Background}
\label{chap:background}

This chapter provides an introduction to dependency grammar and reviews the state-of-the-art in dependency-based parsing. We start by exploring dependency grammar in order to provide the reader with some background, and to formally define a set of properties that will be used as basis for our discussion on dependency parsing. We examine two main approaches to dependency parsing: the \textit{grammar-driven} and the \textit{data-driven}. These are not mutually exclusive, and as we shall see, there are approaches that are based on both. We then introduce syntactic and semantic dependency parsing. As a superficial starting point we can state that the dependency relations in the former are represented predominantly in the research community as \textit{tree} data structures, while the latter are predominantly represented as \textit{graph} data structures. The difference in the choice of data-structure and the representation of dependency structures, as we will argue in this chapter, has an impact on the possibilities for representing various types of syntactic and semantic information. However, when introducing dependency grammar, we will only use the term graph as the need to differentiate between trees and graphs are only of interest to our discussion when dealing with the computational aspects of dependency parsing

We do not attempt at providing a comprehensive historical review of dependency grammar, nor an in-depth formal description of the various approaches and algorithms to dependency-based parsing. Our main objective in this chapter is to present the reader with the necessary background and context for our research and analysis that will be presented in the subsequent chapters. We start this chapter off with a short review of dependency grammar.

\section{Dependency Grammar}
\label{grammar}

The early roots of dependency grammar can possibly be traced back to P\={a}\d{n}ini's grammar of Sanskrit written in approximately 350/250 BC \cite{Kruijff:02}. However, the modern study of  dependency grammar is first presented in the works of \citeauthor{Tes:15}. In his seminal work, \textit{Elements of Structural Syntax}, \citeauthor{Tes:15} presents a theory of syntax by focusing on what he calls a \textit{connection} and a \textit{dependency}:

\begin{displayquote}
The sentence is an \textit{organized whole}; its constituent parts are the \textit{words}. Every word that functions as part of a sentence is no longer isolated as in the dictionary: the mind perceives \textit{connections} between the word and its neighbors; the totality of these connections forms the scaffolding of the sentence. The structural connections establish relations of \textit{dependency} among the words. Each such connection in principle links a \textit{superior} term and an \textit{inferior} term. The superior term receives the name \textit{governor (r\'{e}gissant)}; the inferior term receives the name \textit{dependent (subordonn\'{e})}. \cite{Tes:15}.
\end{displayquote}

It is these \textit{connections}, according to \citeauthor{Tes:15}, that make a sentence meaningful: \say{[W]ithout them the sentence would not be intelligible} \cite{Tes:15}. The \textit{connections}, more commonly referred to in the research community as \textit{dependencies}, are used to create a hierarchy between the words in a sentence, where one word acts as the superior of another. This stands in contrast to constituency grammar where the relationships between lexical units can be both lexical and grouped together as phrases. In dependency grammar the lexical units are always atomic, and the dependency relations are all bi-lexical. 

From the works of \citeauthor{Tes:15}, the field of dependency grammar has grown into a wide range of traditions that have explored the notion of dependency from a variety of different perspectives. Among these are the Prague School's Functional Generative Description, Meaning-Text Theory, and Hudson's Word Grammar \cite{Sgall:86, Mel:88, Hudson:90}. We will not give a detailed exposition on the differences and similarities between the various approaches to dependency grammar, but rather focus on the aspects that are informative as a precursor to our section on dependency parsing. What follows is a concise and formal definition of dependency grammar and a set of criteria that can be used for determining dependencies in a sentence. We introduce and define some of the terminology that will be used throughout our thesis.

\subsection{Defining the Dependency Graph}
\label{definitions}

A dependency can be described as a binary asymmetrical relation between the lexical units of a sentence, i.e. as an arrow pointing from one lexical unit to another. Formally, we describe the dependencies in a sentence $\vec{w} = w_1 ... w_n$ as a directed graph on the set of positions $\vec{w}$ that contain an edge $i \rightarrow j$ if and only if $w_j$ depends on $w_i$ \cite{Kuhl:10}. Directed edges between the lexical units are used in order to represent a dependency going from one lexical unit to another.

The common term used in the research literature for the lexical unit that stands at the beginning of the arrow is \textit{head}. For the lexical unit that is pointed to by the arrow-head the term \textit{dependent} is most common. An exception to this rule is the lexical unit that acts as the entry-point of the graph, usually the main verb of the sentence. In order for this lexical unit to have a head, a common strategy is to add an artificial unit to the sentence, often named \textit{root}, that acts as its head. In addition, there can be \textit{labels} added to the dependencies defining the type of dependency between a head and a dependent. There is no agreement on a unified set of labels, and a variety of different schemes have been proposed for the labels used in different theories and representations.

Examining the dependency graph in figure \ref{dep1} we can visualise the formal description thus far: the artificially added element \textit{root}, the verb `brought' that is the entry-point of the graph and has \textit{root} as its head, and several labeled dependencies between the heads and dependents in the sentence. As an example we examine the dependency between the verb `brought' and the noun `Bob': the head of this dependency is the verb, the dependent is the noun, and the edge that connects them has the label `subj'. The label in this example is used to encode syntactic information: `Bob' acts as the nominal subject of the copula verb `brought'. Now that we have given a brief introduction to dependency graphs, we turn our attention to a set of criteria that has been proposed for determining the dependency relations in a sentence.

\begin{figure}
    \begin{dependency}[]
        \begin{deptext}[column sep=1em, row sep=.1ex]
            Bob \& brought \& the \& cake \& to \& Alice \& . \\
        \end{deptext}
        \deproot[edge unit distance=3.4ex]{2}{root}
        \depedge{2}{1}{nsubj}
        \depedge{2}{4}{obj}
        \depedge{4}{3}{det}
        \depedge{2}{5}{prep}
        \depedge{5}{6}{pobj}
        \depedge[edge unit distance=2.3ex]{2}{7}{punct}
    \end{dependency}
    \caption{A dependency graph with labeled edges.}
    \label{dep1}
\end{figure}

\subsection{A set of criteria}
\label{criteria}

The criteria for establishing the dependencies in a sentence, i.e. determining which lexical units should be head and dependents in a sentence, are of central concern to dependency grammar. \citeauthor{}{Niv:05} proposes a set of criteria, with reference to \citeauthor{Zwicky:85} and \citeauthor{Hudson:90}, for establishing dependencies and determining the head \textit{H} and dependent \textit{D} in a construct \textit{C} \cite{Zwicky:85, Hudson:90, Niv:05}:

\begin{enumerate}
\item (\textit{H}) determines the syntactic category of (\textit{C}) and can often replace (\textit{C}).
\item (\textit{H}) determines the semantic category of (\textit{C}), whereas (\textit{D}) gives semantic specification.
\item (\textit{H}) is mandatory, whereas (\textit{D}) can be optional.
\item (\textit{H}) selects the category of (\textit{D}) and whether it is mandatory or optional.
\item The form of (\textit{D}), whether it is agreement or government, depends on (\textit{H}).
\item The position of (\textit{D}) is specified in relation to (\textit{H}).
\end{enumerate}

Different traditions of dependency grammar diverge in the interpretation and use of a specific set of criteria for identifying dependencies. The list above encompass a set of syntactic and semantic criteria for establishing dependencies, though it is not an exhaustive list. 

\paragraph{Endocentric Constructions} This is a term used for constructions where the dependent is optional and not selected by its head, and where the head can replace the whole without affecting the syntactic structure of the sentence. In terms of the categories above, they are all endocentric with the exception of number 4. \cite{KublerEtAl:09}. The term \textit{head-modifier} is often used in the research literature to describe a construct where the head modifies the dependent either syntactically or semantically. Head-modifer constructs usually fall within the definition of an endocentric construct \cite{Niv:05}.

\paragraph{Exocentric Constructions} These are constructions that fail on the first criteria, where the head can substitute the whole construct, but may satisfy the other constructs. The term \textit{head-complement} is often used in recent syntactic theories to describe an endocentric construct \cite{Niv:05}.

\paragraph{Valency} The term \textit{valency} is used to determine the distinction between complements and modifiers. In most theoretical frameworks the term valency is used in relation to the semantic predicate-argument structure that is associated with verbs, nouns and adjectives (for the most part). It is used as a way to describe a construct where the lexeme impose some type of requirement on its dependent that determines how to interpret it as a semantic predicate \cite{Niv:05}.

\paragraph{Projectivity} This is a technical term that sets a boundary on the type of dependencies that are allowed within a graph. This is a restriction that will be important when we discuss dependency parsing in section \ref{parsing}. A dependency graph is projective if and only if for all its edges $w_i \rightarrow w_j$ in a sentence $\vec{w} = w_1 ... w_n$, they adhere to the restriction that if $w_i \rightarrow w_k$ then $i < k < j$ when $i < j$, or $j < k < i$ when $j < i$ \cite{KublerEtAl:09}. Non-projective dependency parsers do not have such a limit, and allow dependencies with crossing edges. 

\paragraph{Single-head constraint} This is a term that we will encounter in the section on dependency parsing. Here a constraint is set on each dependent in a sentence so that they may only have a single head. This limitation greatly reduces the flexibility of a dependency structure, and as we shall see in section \ref{syntactic-semantic}, reduces the possibilities of capturing certain semantic information.\\

There exists a large variety of different traditions and theoretical frameworks for dependency grammar. What we have presented here is a short introduction to some aspects of these theories as a foundation for section \ref{parsing} on dependency-based parsing. We do not dive into the finer details of dependency grammar as the detailed aspects of these theories have little connection to dependency parsing. The theories of dependency grammar are only indirectly linked to the techniques used in dependency parsing. \citeauthor{Niv:05} notes that the connection between dependency grammar and dependency-based parsing is only indirect, and states that one should think of it as parsing with dependency \textit{representations} rather than with a dependency grammar \cite{Niv:05}. The work presented in our thesis is also based on practical considerations such as the effectiveness and accuracy of different parsing techniques, and less focused on particular grammatical issues.

Now that we have given an outline of dependency grammar, we turn our attention to dependency parsing. In doing so we follow \citeA{Carrol:00} in distinguishing between two main approaches to dependency parsing, a \textit{grammar-driven approach} and a \textit{data-driven approach}. Common to both approaches is that their aim is to produce a labeled dependency structure (an example of which is seen in table \ref{dep1}). The outcome is a structure where the words of a sentence are connected by dependency relations. In the following two sections we introduce a set of examples of these two approaches to dependency parsing.

\section{Dependency Parsing}
\label{parsing}

Dependency parsing is a natural language processing task that aims at producing dependency structures for a sentence in a given language. The early approaches to dependency parsing were built on techniques that assigned dependencies based on linguistic theories and formal grammars. However, as \citeauthor{Niv:05} points out, even though some dependency parsers are intimately tied with a particular theory, it is more often the case that a parser is based on a \textit{representation} rather than a formal theory, as opposed to for instance constituency based parsing \cite{Niv:05}. The representation that is used may not have a grounding in a formally strict framework, but rather be the result of some theoretical underpinning coupled with the intuition of the researchers that create the representation. As mentioned previously, the grammar-driven approach can be supplemented with a statistical and/or data-driven part, either for disambiguation or for parts of the predication.

The more recent literature on dependency parsing has a larger focus on data-driven approaches, as these are constantly showing progress in bot accuracy, speed and robustness. The data-driven approach is based on different types of \textit{machine learning} algorithms that are trained on linguistically annotated data to build models that can automatically predict plausible dependency structures for a given sentence. We first examine the grammar-driven approach before moving on to a discussion on data-driven dependency-based parsers.

\subsection{Grammar-Driven}
\label{grammar-driven}

The grammar-driven approach to dependency parsing relies on explicitly defined grammars to produce a dependency graph. The parsing of a sentence is defined as an analysis in relation to a grammar, and if successful the sentence is then said to belong to the language defined by that grammar. The earliest works on dependency parsing were closely related to context-free grammars \cite{KublerEtAl:09}. These methods use production rules in a context-free grammar to produce dependencies. Standard chart parsing methods can be used for implementation such as the CKY \cite{Younger:67} and Earley's algorithm \cite{Early:70}. The rules themselves can take the form of production or constraint rules, see \citeauthor{KublerEtAl:09} for details \cite{KublerEtAl:09}. 

\citeauthor{Gaifman:65} proposes a set of three rules for a \textit{dependency system}. The rules are similar to context-free grammars in that they map a sentence $\vec{w} = w_1 ... w_n$ to a sequence of categories $X_1, ..., X_n$ as long as a set of conditions are upheld. These conditions where a set of restrictions that among others led to a dependency structure that was a directed tree which upheld certain restrictions such as the single-head restriction and projectivity. \cite{Gaifman:65}.

\citeauthor{Niv:05} points out that rules of the form that we find in \citeA{KublerEtAl:09, Gaifman:65}. \citeauthor{Niv:05} points out that these early attempts, and an erroneous conclusion that dependency grammar is only a restricted form of context-free grammar, led to a period of approximately twenty-five years of relative lack of interest towards dependency grammar within natural language processing \cite{Niv:05}. 

\subsection{Data-Driven}
\label{data-driven}

\section{Syntactic and Semantic Representations}
\label{syntactic-semantic}

\subsection{}
\label{}