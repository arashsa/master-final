\chapter{Background}
\label{chap:background}

This chapter provides an introduction to dependency grammar and reviews the state-of-the-art in dependency-based parsing. We start by exploring dependency grammar in order to provide the reader with an insight to the theoretical foundations of dependency parsing, and to formally define a set of properties that will be used as basis for our thesis. We examine two main approaches to dependency parsing: the \textit{grammar-driven} and the \textit{data-driven}. These are not mutually exclusive, and as we shall see, there are approaches that are based on both. We then introduce syntactic and semantic dependency parsing. As a superficial starting point we can state that the dependency relations in the former are represented predominantly in the research community as \textit{tree} data structures, while the latter are predominantly represented as \textit{graph} data structures. The difference in the choice of data-structure and the way dependencies are represented, as we will argue in this chapter, impacts the possibilities for representing various types of syntactic and semantic information. However, when introducing dependency grammar, we will only use the term graph as the need to differentiate between trees and graphs are not necessary unless we are dealing with the computational aspects of parsing.

We do not attempt to provide a comprehensive historical review of dependency grammar, or an in-depth formal description of the various approaches and algorithms to dependency based parsing. Our main objective here is to present the reader with context to the research and analysis that will be presented in the subsequent chapters as part of our thesis. We start this chapter off with a short historical introduction to dependency grammar.

\section{Dependency Grammar}
The early roots of dependency grammar can possibly be traced back to P\={a}\d{n}ini's grammar of Sanskrit written in approximately 350/250 BC \cite{Kruijff:02}. However, the modern study of  dependency grammar is first presented in the works of \citeauthor{Tes:15}. In his seminal work, \textit{Elements of Structural Syntax}, \citeauthor{Tes:15} presents a theory of syntax by focusing on what he calls a \textit{connection} and a \textit{dependency}:

\begin{displayquote}
The sentence is an \textit{organized whole}; its constituent parts are the \textit{words}. Every word that functions as part of a sentence is no longer isolated as in the dictionary: the mind perceives \textit{connections} between the word and its neighbors; the totality of these connections forms the scaffolding of the sentence. The structural connections establish relations of \textit{dependency} among the words. Each such connection in principle links a \textit{superior} term and an \textit{inferior} term. The superior term receives the name \textit{governor (r\'{e}gissant)}; the inferior term receives the name \textit{dependent (subordonn\'{e})}. \cite{Tes:15}.
\end{displayquote}

It is these \textit{connections}, according to \citeauthor{Tes:15}, that make a sentence meaningful: \say{[W]ithout them the sentence would not be intelligible} \cite{Tes:15}. The \textit{connections}, more commonly referred to in the research community as \textit{dependencies}, are used to create a hierarchy between the words in a sentence, where one word acts as the superior of another. This stands in contrast to constituency grammar where the relationships between lexical units can be both lexical and grouped together as phrases. In dependency grammar the lexical units are always atomic, and the dependency relations are all bi-lexical. 

From the works of \citeauthor{Tes:15}, the field of dependency grammar has grown into a wide range of traditions that have explored the notion of dependency from a variety of different perspectives. Among these are the Prague School's Functional Generative Description, Meaning-Text Theory, and Hudson's Word Grammar \cite{Sgall:86, Mel:88, Hudson:90}. We will not give a detailed exposition on the differences and similarities between the various approaches to dependency grammar, but rather focus on the aspects that are informative as a precursor to our section on dependency parsing. What follows is a concise and formal definition of dependency grammar and a set of criteria that can be used for determining dependencies in a sentence. Much of the terminology that will be used in our thesis are introduced in this section.

\subsection{Definitions and Criteria for Dependencies}
\label{definitions}
A dependency can be described as a binary asymmetrical relation between the lexical units of a sentence, i.e. as an arrow pointing from one lexical unit to another. Formally, we describe the dependencies in a sentence $\vec{w} = w_1 ... w_n$ as a directed graph on the set of positions $\vec{w}$ that contain an edge $i \rightarrow j$ if and only if $w_j$ depends on $w_i$ \cite{Kuhl:10}. Directed edges between the lexical units are used so as to represent a dependency going from one lexical unit to another.

The common terms used in the research literature for the lexical unit that stands at the beginning of the arrow is \textit{head}, and the one that is pointed to by the arrowhead as \textit{dependent}. The lexical unit that acts as the entry-point of the graph is an exception to this rule. The common strategy in the research community is to add an artificial unit to the sentence, often named \textit{root}, that then acts as the head. The lexical unit that is pointed to by this artificial unit, the entry-point, is often the main verb of the sentence. In addition, we can have \textit{labels} added to the dependencies defining the type of connection that exists between a head and a dependent. There is no unified set of labels for dependency parsing, and a wide variety of different schemes exist in accordance with an underlying theoretical framework. 

Examining the dependency graph in figure \ref{dep1} we can visualise the formal description thus far: a lexical element that is the \textit{root} of the sentence, the verb `brought' that is the entry-point of the graph, and several labeled dependencies between the heads and dependents in the sentence. As an example we can examine the dependency between the verb `brought' and the noun `Bob': the head of this dependency is the verb, the dependent is the noun, and the dependency that connects them has the label `subj'. The label in this example is used to encode syntactic information. `Bob' acts as the nominal subject of the copula verb `brought'.

\begin{figure}
    \begin{dependency}[]
        \begin{deptext}[column sep=1em, row sep=.1ex]
            Bob \& brought \& the \& cake \& to \& Alice \& . \\
        \end{deptext}
        \deproot[edge unit distance=3.4ex]{2}{root}
        \depedge{2}{1}{nsubj}
        \depedge{2}{4}{obj}
        \depedge{4}{3}{det}
        \depedge{2}{5}{prep}
        \depedge{5}{6}{pobj}
        \depedge[edge unit distance=2.3ex]{2}{7}{punct}
    \end{dependency}
    \caption{An example of a dependency graph with labeled edges.}
    \label{dep1}
\end{figure}

The criteria for establishing dependencies, and defining which lexical unit should be head and dependent is of central concern to dependency grammar. In the case where the dependencies have labels, the type of labels are determined by their own set of criteria. \citeA{Niv:05} proposes a set of criteria, taken from \citeauthor{Zwicky:85} and \citeauthor{Hudson:90}, for establishing dependencies and determining the head \textit{H} and dependent \textit{D} in a construct \textit{C} \cite{Zwicky:85, Hudson:90, Niv:05}:

\begin{enumerate}
\item (\textit{H}) determines the syntactic category of (\textit{C}) and can often replace (\textit{C}).
\item (\textit{H}) determines the semantic category of (\textit{C}), whereas (\textit{D}) give semantic specification.
\item (\textit{H}) is mandatory, whereas (\textit{D}) can be optional.
\item (\textit{H}) selects the category of (\textit{D}) and whether it is mandatory or optional.
\item The form of (\textit{D}), whether it is agreement or government, depends on (\textit{H}).
\item The position of (\textit{D}) is specified in relation to (\textit{H}).
\end{enumerate}

Different traditions of dependency grammar diverge in the interpretation and use of a specific set of criteria for identifying dependencies. The list above encompass a set of syntactic and semantic criteria for establishing dependencies, but do not fully cover the range of criteria that can be used to determine all possibilities. In order to get a better understanding of the criteria listed above, we examine some common terminology used in the research community that can give more insight to the set of criteria outlined above.

\paragraph{Endocentric Constructions} This is a term used for constructions where the dependent is optional and not selected by its head, and where the head can replace the whole without affecting the syntactic structure of the sentence. In terms of the categories above, they are all endocentric with the exception of number 4. \cite{KublerEtAl:09}. The term \textit{head-modifier} is often used in the research literature to describe a construct where the head modifies the dependent either syntactically or semantically. This construct falls within the definition of an endocentric construct \cite{Niv:05}.

\paragraph{Exocentric Constructions} These are constructions that fail on the first criteria, where the head can substitute the whole construct, but may satisfy the other constructs. The term \textit{head-complement} is often used in recent syntactic theories to describe an endocentric construct \cite{Niv:05}.

\paragraph{Valency} The term \textit{valency} is used to determine the distinction between complements and modifiers. In most theoretical frameworks the term valency is used in relation to the semantic predicate-argument structure that is associated with verbs, nouns and adjectives (these three word classes for the most part). It is used as a way to describe a construct where the lexeme impose some type of requirement on its dependent that determines how to interpret it as a semantic predicate \cite{Niv:05}.

\paragraph{Projectivity} This is a technical term that sets a boundary on the type of dependencies that are allowed within a graph. This is a restriction that will be important when we discuss dependency parsing in section \ref{parsing}. A dependency graph is projective if and only if for all its edges $w_i \rightarrow w_j$ in a sentence $\vec{w} = w_1 ... w_n$, they adhere to the restriction that if $w_i \rightarrow w_k$ then $i < k < j$ when $i < j$, or $j < k < i$ when $j < i$ \cite{KublerEtAl:09}. \\

There are numerous different traditions and variations in the research community that have their own theoretical frameworks of dependency grammar. What we have presented here are some aspects of these theories as an informative entry to dependency parsing. We do not delve into the finer details of dependency grammar as the theoretical frameworks are only indirectly linked to the techniques used in dependency parsing. As \citeauthor{Niv:05} notes: \say{... [t]his may be due to the relatively lower degree of formalization of dependency grammar theories in general} \cite{Niv:05}. The work presented in our thesis is based on practical considerations such as the effectiveness and accuracy of different parsing techniques, and not grammatical aspects that relate to the specific details of dependency grammar.

Now that we have given an outline of dependency grammar, we turn our attention to dependency parsing. In doing so we follow \citeA{Carrol:00} in distinguishing between two main approaches to dependency parsing, a \textit{grammar-driven approach} and a \textit{data-driven approach}. Common to both approaches is that their aim is to produce a labeled dependency structure (an example of which is seen in table \ref{dep1}). The outcome is a structure where the words of a sentence are connected by dependency relations. In the following two sections we introduce a set of examples of these two approaches to dependency parsing.

\section{Dependency Parsing}
\label{parsing}

Dependency parsing is a natural language processing task that aims at producing dependency structures for a sentence in a given language. The early approaches to dependency parsing were built on techniques that assigned dependencies based on linguistic theories and formal grammars. However, as \citeauthor{Niv:05} points out, even though some dependency parsers are intimately tied with a particular theory, it is more often the case that a parser is based on a \textit{representation} rather than a formal theory, as opposed to for instance constituency based parsing \cite{Niv:05}. The representation that is used may not have a grounding in a formally strict framework, but rather be the result of some theoretical underpinning coupled with the intuition of the researchers that create the representation. As mentioned previously, the grammar-driven approach can be supplemented with a statistical and/or data-driven part, either for disambiguation or for parts of the predication.

Data-driven approaches tend to be favoured in the more recent research literature. This is an approach that is based on \textit{machine learning} algorithms that are trained on linguistically annotated data that is used to train models that attempts to make a prediction of the most plausible dependency structure for a given sentence.

\subsection{Grammar-Driven}
\label{grammar-driven}

The grammar-driven approach to dependency parsing relies on an explicitly defined grammars to produce a dependency graph. The parsing of a sentence is defined as an analysis in relation to a grammar, and if successful the sentence is then said to belong to the language defined by that grammar. The earliest works on dependency parsing were closely related to context-free grammars \cite{KublerEtAl:09}. These methods use production rules in a context-free grammar to produce dependencies. Standard chart parsing methods can be used for implementation such as the CKY and Earley's algorithm.

\subsection{Data-Driven}

\section{Syntactic Dependency Parsing}

\section{Semantic Dependency Parsing}