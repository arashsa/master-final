\chapter{Background}
\label{chap:background}

This chapter provides an introduction to dependency grammar and reviews the state-of-the-art in dependency-based parsing. We start by exploring dependency grammar in order to provide the reader with some background, and to formally define a set of properties that will be used as basis for our thesis. We then examine two main approaches to dependency-based parsing: the \textit{grammar-driven} and the \textit{data-driven} approach. These two approaches are not mutually exclusive, and we will also examine models that are based on a combination of both.

Our focus in this chapter will be on data-driven approaches to dependency parsing. This is due to our observation that most of the recent research on dependency-based parsing use this approach. In addition, the contrastive errors analysis that we present in chapter \ref{chap:analysis} use data-driven parsers and their results as its empirical foundation. The experiments that we present in chapter \ref{chap:experiments} also rely on data-driven models for classifying semantic frames.

We will focus on two main approaches to data-driven parsing: \textit{transition-based} and \textit{graph-based}. The transition-based approach, also commonly referred to as \textit{shift-reduce dependency parsing}, is based on \textit{finite state machines} for mapping a sentence to a dependency graph. The learning aspect of this approach is to parametrize over local transition states: a model for predicting the next state given previous states. The parsing use this model to construct the most optimal sequence of transitions for a given sentence in order to reach a dependency structure. The graph-based approach to parsing, also known as \textit{maximum spanning tree} parsing, create a space of candidate dependency graphs for a given sentence. This approach to learning is global, and the parsing aspect is to search through a set of dependency graphs and do a weighted selection in order to reach the most probable dependency structure.

% \cite{KublerEtAl:09}

We differentiate dependency-based parsing in two classes: \textit{syntactic dependency parsing} and \textit{semantic dependency parsing}. As a superficial starting point we can state that the dependency relations in the former are represented predominantly as \textit{tree} data structures, while the latter are represented as \textit{graph} data structures. In section \ref{syntactic-semantic} we examine the hypothesis that tree-based data structures are suited for the analysis of grammatical structure, but often lack the expressiveness needed to capture semantics due to the limitations imposed by its structure. We will argue that semantic dependency parsing deserves further investigation in light of these observations.

In order to simplify our discussion, we will use the term graph when we are discussing dependency grammar in section \ref{grammar}. This is based on two assumptions:

\begin{enumerate}
\item A tree can be defined as an acyclic directed graph; all possible trees are a subset of all possible graphs.
\item The difference between tree-based and graph-based dependency structures are only of interest to our discussion when dealing with parsing techniques and algorithms, and not when discussing dependency grammar.
\end{enumerate}

We do not attempt at providing a comprehensive review of dependency grammar, nor an in-depth formal description of the various approaches and algorithms to dependency-based parsing. Our main objective in this chapter is to present the reader with the necessary background and context for our research and analysis in subsequent chapters. We start this chapter off with a short review of dependency grammar.

\section{Dependency Grammar}
\label{grammar}

The early roots of dependency grammar can possibly be traced back to P\={a}\d{n}ini's grammar of Sanskrit written in approximately 350-250 BC \cite{Kruijff:02}. However, the modern study of  dependency grammar is first presented in the works of \citeauthor{Tes:15}. In his seminal work, \textit{Elements of Structural Syntax}, \citeauthor{Tes:15} presents a theory of syntax by focusing on what he calls a \textit{connection} and a \textit{dependency}:

\begin{displayquote}
The sentence is an \textit{organized whole}; its constituent parts are the \textit{words}. Every word that functions as part of a sentence is no longer isolated as in the dictionary: the mind perceives \textit{connections} between the word and its neighbors; the totality of these connections forms the scaffolding of the sentence. The structural connections establish relations of \textit{dependency} among the words. Each such connection in principle links a \textit{superior} term and an \textit{inferior} term. The superior term receives the name \textit{governor (r\'{e}gissant)}; the inferior term receives the name \textit{dependent (subordonn\'{e})} \cite{Tes:15}.
\end{displayquote}

It is these \textit{connections}, according to \citeauthor{Tes:15}, that make a sentence meaningful: \say{[W]ithout them the sentence would not be intelligible} \cite{Tes:15}. The \textit{connections} are used to create a hierarchy between the words in a sentence, where one word is dependent on another, hence the term \textit{dependency}. This is a different approach than for instance \textit{constituency grammar}, where the relationships between lexical units are formed under grammatical constituents. In constituency grammar there are also no hierarchical relationship between the lexical units, instead the hierarchy is formed as grammatical groups such as the \textit{sentence}, \textit{noun-phrase}, \textit{verb-phrase}, etc. In dependency grammar the lexical units are always atomic, and the dependency relations are all bi-lexical.

From the works of \citeauthor{Tes:15}, the field of dependency grammar has grown into a wide range of traditions that have explored the notion of dependency from a variety of different perspectives. Among these are the Prague School's Functional Generative Description, Meaning-Text Theory, and Hudson's Word Grammar \cite{Sgall:86, Mel:88, Hudson:90}. We will not give a detailed exposition on the differences and similarities between the various approaches to dependency grammar, but rather focus on the aspects that are informative as a precursor to our section on dependency parsing. What follows is a concise and formal definition of dependency grammar, and a set of criteria that can be used for determining dependencies in a sentence. We also introduce important terminology that will be used throughout our thesis insofar as they are related to dependency grammar.

\subsection{Defining Dependencies}
\label{definitions}

A dependency can be described as a binary asymmetrical relation between the lexical units of a sentence, i.e. as arrows pointing from one lexical unit to another. Formally, we describe the dependencies in a sentence $\vec{w} = w_1 ... w_n$ as a directed graph on the set of positions $\vec{w}$ that contain an edge $i \rightarrow j$ if and only if $w_j$ depends on $w_i$ \cite{Kuhl:10}. Directed edges between the lexical units are used in order to represent a dependency going from one lexical unit to another.

The common term used in the research literature for the lexical unit that stands at the beginning of the arrow is \textit{head}. For the lexical unit that is pointed to by the arrow-head the term \textit{dependent} is most common. An exception to the description thus far is the lexical unit that acts as the entry-point of the graph, usually the main verb of the sentence. In order for this lexical unit to have a head, a common strategy is to add an artificial unit to the sentence, often named \textit{root}. The edges in the graph can have \textit{labels} added, which signify the type of relation that exists between heads and dependents. 

If we examine the dependency graph in figure \ref{dep1}, we can visualise the description above: the verb `brought' that is the entry-point of the graph, the artificially added unit \textit{root} that acts as the verbs head, and several labeled dependencies between the heads and dependents in the sentence. If we examine the dependency between the verb `brought' and the noun `Bob': we see that the head of this dependency is the verb `brought', the dependent is the noun `Bob', and the edge that connects them has the label `subj'. The label in this example is used to encode syntactic information: `Bob' acts as the nominal subject of the copula verb `brought'. Now that we have briefly defined the dependency graph, we turn our attention to a set of criteria that has been proposed for determining heads and dependents in a sentence.

\begin{figure}
    \centering
    \begin{dependency}[]
        \begin{deptext}[column sep=1em, row sep=.1ex]
            Bob \& brought \& the \& cake \& to \& Alice \& . \\
        \end{deptext}
        \deproot[edge unit distance=3.4ex]{2}{root}
        \depedge{2}{1}{nsubj}
        \depedge{2}{4}{obj}
        \depedge{4}{3}{det}
        \depedge{2}{5}{prep}
        \depedge{5}{6}{pobj}
        \depedge[edge unit distance=2.3ex]{2}{7}{punct}
    \end{dependency}
    \caption{Example dependency graph with labeled edges.}
    \label{dep1}
\end{figure}

\subsection{Criteria for Dependencies}
\label{criteria}

The criteria for establishing the dependencies, i.e. determining which lexical units should be head and dependents in a sentence, are of central concern to dependency grammar. \citeauthor{Niv:05} proposes a set of criteria, with reference to \citeauthor{Zwicky:85} and \citeauthor{Hudson:90}, for establishing dependencies and determining the head \textit{H} and dependent \textit{D} in a construct \textit{C} \cite{Zwicky:85, Hudson:90, Niv:05}:

\begin{enumerate}
\item (\textit{H}) determines the syntactic category of (\textit{C}) and can often replace (\textit{C}).
\item (\textit{H}) determines the semantic category of (\textit{C}), whereas (\textit{D}) gives semantic specification.
\item (\textit{H}) is mandatory, whereas (\textit{D}) can be optional.
\item (\textit{H}) selects the category of (\textit{D}) and whether it is mandatory or optional.
\item The form of (\textit{D}), whether it is agreement or government, depends on (\textit{H}).
\item The position of (\textit{D}) is specified in relation to (\textit{H}).
\end{enumerate}

Different traditions of dependency grammar diverge in their interpretation and use of a specific set of criteria for identifying dependencies. The list above encompasses a set of syntactic and semantic criteria for establishing dependencies, and there have been attempts at providing a single coherent notion of dependency that include all of the criteria above. 

\citeauthor{Hudson:90} has proposed the usage of the concept of a prototype structure that satisfy all or most of the criteria above, and then using special cases for dependencies that only satisfy one or few criteria \cite{Hudson:90}. \citeauthor{Mel:88} has proposed a set of three dependency types: \textit{morphological}, \textit{syntactic}, and \textit{semantic} \cite{Mel:88}. \citeauthor{Nikula:86} has proposed two categories of constructions, \textit{endocentric} and \textit{exocentric}, for determining dependencies \cite{Nikula:86}. \textit{Valency} is another term that is used as criteria for determining dependencies. We will now define the terminology thus far, and add additional terms that are used in the subsequent chapters insofar as they relate to dependency grammar.

\paragraph{Endocentric Construction} This is a term used for constructions where the dependent is optional and not selected by its head, and where the head can replace the whole without affecting the syntactic structure of the sentence. In terms of the categories above, they are all endocentric with the exception of number 4. \cite{KublerEtAl:09}. The term \textit{head-modifier} is often used in the research literature to describe a construct where the head modifies the dependent either syntactically or semantically. Head-modifer constructs usually fall within the definition of an endocentric construction \cite{Niv:05}.

\paragraph{Exocentric Construction} These are constructions that fail on the first criteria, where the head can substitute the whole construct, but may satisfy the others. The term \textit{head-complement} is often used in recent syntactic theories to describe an endocentric construct \cite{Niv:05}.

\paragraph{Valency} The term \textit{valency} is used to determine the distinction between complements and modifiers. In most theoretical frameworks the term valency is used in relation to the semantic predicate-argument structure that is associated with verbs, nouns and adjectives (for the most part). It is used as a way to describe a construct where the lexeme impose some type of requirement on its dependent that determines how to interpret it as a semantic predicate \cite{Niv:05}.

\paragraph{Projectivity} This is a technical term that sets a boundary on the type of dependencies that are permissible in a graph. A dependency graph is projective if and only if for all its edges $w_i \rightarrow w_j$ in a sentence $\vec{w} = w_1 ... w_n$, they adhere to the restriction that if $w_i \rightarrow w_k$ then $i < k < j$ when $i < j$, or $j < k < i$ when $j < i$ \cite{KublerEtAl:09}. We can see the difference by examining the projective graph in figure \ref{projective}, and the non-projective graph in figure \ref{non-projective}.

\begin{figure}
    \centering
    \begin{dependency}[hide label]
        \begin{deptext}[column sep=1em, row sep=.1ex]
        $w_0$ \& $w_1$ \& $w_2$ \& $w_3$ \\
        \end{deptext}
        \depedge{1}{3}{}
        \depedge{3}{2}{}
        \depedge{3}{4}{}
    \end{dependency}
    \caption{A projective dependency graph.}
    \label{projective}
\end{figure}

\begin{figure}
    \centering
    \begin{dependency}[hide label]
        \begin{deptext}[column sep=1em, row sep=.1ex]
        $w_0$ \& $w_1$ \& $w_2$ \& $w_3$ \\    
        \end{deptext}
        \depedge[edge unit distance=2.1ex]{1}{3}{}
        \depedge{3}{4}{}
        \depedge{4}{2}{}
    \end{dependency}
    \caption{A non-projective dependency graph.}
    \label{non-projective}
\end{figure}

\paragraph{Single-head constraint} This term is used to define a constraint where the dependents in a sentence are prohibited from having more than one head. This limitation reduces the flexibility of a dependency graph, and as we shall see in section \ref{syntactic-semantic} on syntactic and semantic dependency parsing, this constraint reduces the possibilities of capturing certain semantic information.\\

There exists a large variety of different traditions and theoretical frameworks of dependency grammar. What we have presented here is a short introduction to some aspects of these theories as a foundation for section \ref{parsing} on dependency-based parsing, and section \ref{syntactic-semantic} where we examine the difference between syntactic and semantic parsing. We do not dive into the finer details of dependency grammar as the these are not seen as informative for our thesis. As \citeauthor{Niv:05} points out, the theories of dependency grammar are only indirectly linked to the techniques used in dependency parsing, and the connection between dependency grammar and dependency-based parsing is largely indirect. \citeauthor{Niv:05} states that one should think of dependency-based parsing as parsing with dependency \textit{representations} rather than with a dependency grammar \cite{Niv:05}. In addition, the work presented in our thesis is based on practical considerations such as the effectiveness and accuracy of different parsing techniques. The grammatical aspects are of interest insofar as they can add to our discussion regarding these aspects of dependency-based parsing, but not of central concern to our thesis.

Now that we have given an outline of dependency grammar, we turn our attention to dependency-based parsing. In doing so we follow \citeA{Carrol:00} in distinguishing between two main approaches to dependency parsing, a \textit{grammar-driven} approach and a \textit{data-driven} approach. They share in common the goal of algorithmically producing a dependency structure for a given sentence (an example of which is seen in table \ref{dep1}). What differentiates them is the way in which they reach this goal.

\section{Dependency Parsing}
\label{parsing}

The early approaches to dependency parsing were based on formal grammars in order to automatically assign a dependency structure for a given sentence. However, as \citeauthor{Niv:05} points out, even though some dependency parsers are intimately tied with a particular theory of dependency grammar, it is more often the case that a parser is based on a \textit{representation} rather than a formal theory. Constituency based parsing, in contrast, is often more tied to a particular theoretical approach \cite{Niv:05}.

% The set of rules used in a grammar-driven approach may either be implemented manually, or learned using a \textit{machine learning} approach by way of an \textit{annotated} \textit{corpora}. The grammar-driven approach may also have a supplementary part to its parsing where a data-driven part is responsible for either disambiguation or for predicting certain aspects of the dependency structure.

In the more recent literature on dependency parsing there has been a shift towards data-driven approaches. This is due to the fact that these approaches have consistently shown progress in both accuracy, speed and robustness. The data-driven approaches are based on statistical modeling or machine learning algorithms for inducing probabilistic or predictive models. We start this section by giving a short review of grammar-driven dependency parsing, before we move on to examining the data-driven approach.

\subsection{Grammar-Driven Approaches}
\label{grammar-driven}

Grammar-driven dependency parsing relies on explicitly defined grammars for producing a dependency graph. Given a sentence, a strategy is deployed in order to find a dependency structure that belongs to the language defined by a specific grammar. This grammar can be made manually, by way of statistical modelling or machine learning, or a fusion of both.

% some strategy can be used in an attempt to find the most probable dependency structure as a post-processing step. These approaches mix grammar- and data-driven approaches, and as we shall see in section \ref{data-driven}, they were precursors to the completely data-driven approaches that emerged later.

The earliest works on dependency parsing were closely related to context-free grammars \cite{KublerEtAl:09}. These methods use production rules in a context-free grammar in order to produce dependencies. Standard chart parsing methods are used for implementation, examples of which are the Cocke-Kasami-Younger (CKY) \cite{Younger:67} and Earley's algorithm \cite{Early:70}. The rules themselves can take the form of production or constraint rules, see \citeA{KublerEtAl:09} for details.

\citeauthor{Gaifman:65} proposes a set of three rules for a \textit{dependency system}. The rules are similar to context-free grammars in that they map a sentence $\vec{w} = w_1 ... w_n$ to a sequence of categories $X_1, ..., X_n$, and they add a relation of dependency $d$ between two lexical units $w_i \rightarrow w_j$ as long as a set of conditions or constraints are upheld. The rules that \citeauthor{Gaifman:65} propose lead to a dependency structure that is a projective directed tree which upholds the single-head constraint \cite{Gaifman:65}. These approaches produce unlabeled dependency structures.

\citeauthor{Niv:05} points out that the results from these early approaches, and the attempts of \citeauthor{Gaifman:65} to show that dependency grammar is only a restricted variant of context-free grammars, led to a period of approximately twenty-five years with a relative lack of interest towards dependency parsing among researchers working in the field of NLP \cite{Niv:05}.

Another common approach to grammar-driven dependency parsing is based on what is commonly referred to as \textit{eliminative} parsing. This approach, as opposed to the systems based on context-free grammars, produce dependency structures by continuously eliminating dependencies that violate a set of constraints. The elimination process is repeated until there are no violations. As \citeauthor{Niv:05} notes, the eliminative approach is a constraint satisfaction problem, where all dependency structures not in violation of the constraints would be considered. This approach poses two problems: the first problem is that the result might be no dependency structure at all, i.e. all suggestions break some constraint. The second problem arises when more than one dependency structure remains \cite{Niv:05}. The latter problem can be solved using a disambiguation step. We will examine disambiguation in section \ref{data-driven} below.

If the recent research on dependency-based parsing is an indication of where we are headed, there seems to be a move away from grammar-driven driven approaches. Data-driven parsing have shown continuous improvements in both accuracy, speed and robustness. In light of this observation, and due to the fact that the analysis in chapter \ref{chap:analysis}, and our own experiments in chapter \ref{chap:experiments}, are based on data-driven approaches, we do not examine the grammar-driven approach in more detail. Instead, we turn our attention to the data-driven models of dependency parsing.

\subsection{Data-Driven Approaches}
\label{data-driven}

Early attempts at data-driven dependency parsing used a grammar-driven part to produce multiple dependency graphs, and added a data-driven model trained on corpora to select the most probably structure from a set of possibilities. \citeauthor{Eisner:96b} presented one of the first successful approaches using this methodology. This approach use three models for probabilistic parsing, trained on manually annotate data, where both part-of-speech tags and unlabeled dependency structures are assigned to a sentence \cite{Eisner:96b}. The algorithm used, similar to the CKY method, is a bottom-up method that predicts the most probable parse from the bottom up. It ensures that there are no cycles in the graph, and adheres to the single-head constraint defined in section \ref{definitions}. Even though this parser is  data-driven, insofar as no hand-written grammar is required, the bottom-up strategy is based on a learned grammar that in combination with a generative probabilistic model attempts to predict the most likely parse for a given sentence.

As \citeauthor{Niv:05} points out, the work of \citeauthor{Eisner:96b} has been influential in two ways. It proved that statistical modeling and machine learning could be used for dependency parsing with an accuracy comparable to the best performing constituency-based parsers of the time. It also showed that efficient parsing techniques could be developed that exploited the special properties of dependency structure \cite{Niv:05}. More recent approaches have moved towards data-driven models where the dependency graph is induced without any explicitly defined grammar. We will now examine two main classes of data-driven dependency parsing.

\subsubsection{Transition-Based Parsing}
\label{ssc:background:transition-based}

Purely deterministic discriminative data-driven models with no need for a grammar were first proposed by \citeA{Kudo:00} and \citeA{Yamada:03}. These models use \textit{Support Vector Machines} (SVMs) for learning, and rely on these machine learning algorithms' ability to cope with large scale feature spaces. The parsers presented by \citeauthor{Kudo:00} and \citeauthor{Yamada:03} construct dependency trees in a left-to-right fashion by way of three transitions: \textit{Shift}, \textit{Right}, and \textit{Left}. \citeauthor{Yamada:03} use three binary classifiers in order to solve these transitions as a multi-class classification problem. One model is made for handling each possible action given a state: \textit{Left} vs. \textit{Right}, \textit{Left} vs. \textit{Shift} and \textit{Right} vs. \textit{Shift} \cite{Yamada:03}. This method managed to produce substantially higher accuracy than the models proposed by \citeA{Eisner:96b}.

The models developed by \citeauthor{Kudo:00} and \citeauthor{Yamada:03} have been further developed in many directions under the umbrella of transition-based parsing. A \textit{transition-based} parser consists of a set of \textit{configurations} (or \textit{states}) that include a set of \textit{transitions} for producing a dependency structure.

We follow \citeA{KublerEtAl:09} in describing transition-based parsing as a \textit{configuration} of triples, consisting of a stack, an input buffer, and a set of dependency arcs. Given a set $R$ of dependency types and a vocabulary $V$, a \textit{configuration} for sentence $S = w_0w_1, ..., w_n$ is a triple $c = (\alpha, \beta, A)$, where:

\begin{enumerate}
\item $\alpha$ is a stack of words $w_i \in V_S$,
\item $\beta$ is a buffer of words $w_i \in V_S$,
\item $A$ is a set of dependency arcs $(w_i, r, w_j) \in V_S \times R \times V_S$.
\end{enumerate}

The \textit{configuration} represents a partial analysis. The words on the stack $\alpha$ are partially processed words from the input, and the words in the buffer $\beta$ are the remaining words from the input. For any input sentence there is an \textit{initial} state, and a \textit{termination} state. The initial state starts with the artificially added unit \textit{root} $(w_0)$ on the stack $\alpha$, the input sentence $S$ in the buffer, and an empty set of dependency arcs in the last place of the triple: $([w_0]_\alpha, [w_1, w_2, ..., w_n]_\beta, \emptyset)$. The process ends in the \textit{termination} state: $(\alpha, []_\beta, A)$. There are three types of transitions from the initial to the termination state:

\begin{enumerate}
\item $Left-Arc_r$ \hspace{10mm} $(\alpha|w_i, w_j|\beta, A) \Rightarrow (\alpha, w_j|\beta, A \cup \{(w_j, r, w_i)\}$
\item $Right-Arc_r$ \hspace{8mm} $(\alpha|w_i, w_j|\beta, A) \Rightarrow (\alpha, w_i|\beta, A \cup \{(w_i, r, w_j)\}$
\item $Shift$ \hspace{22mm} $(\alpha, w_i|\beta, A) \Rightarrow (\alpha|w_i,\beta, A)$
\end{enumerate}

Each of these transitions can be described informally as:

\begin{enumerate}
\item Left-Arc: add a dependency arc $(w_i, r, w_j)$ to the set $A$, where $w_i$ is a lexical unit on top of stack $\alpha$ and $w_j$ is the first lexical unit in buffer $\beta$. Then pop the top lexical unit from stack $\alpha$.
\item Right-Arc :add a dependency arc $(w_j, r, w_i)$ to the set $A$, where $w_i$ is a lexical unit on top of stack $\alpha$ and $w_j$ is the first lexical unit in buffer $\beta$. Then pop the top lexical unit from stack $\alpha$. Then replace $w_j$ by $w_i$ at the head of buffer. 
\item Shift: remove the first lexical unit $w_i$ in the buffer $\beta$ and push it on top of the stack $\alpha$. 
\end{enumerate}

The transitions are performed according to a set of permissible transitions in a sequence $T$. This includes a sequence of configurations $C_{0,m} = (c_0, c_1, ..., c_m)$ for sentence $S$ where:

\begin{enumerate}
\item $c_0$ is the initial configuration $c_0(S)$ for $S$,
\item $c_m$ is a terminal configuration,
\item for every $i$ such that $1 \leq i \leq m$, there is a transition $t \in T$ such that $c_i = t(c_{i-1})$
\end{enumerate}

An example of a transition-based dependency parser is the openly available MaltParser \cite{Niv:Hal:Nil:07}. This parser is an implementation of the \textit{inductive dependency parsing} techniques developed by \citeA{Nivre:Inductive:05}. Given a \textit{treebank} that follows the specific dependency format of the MaltParser, it can induce a parser for the language of that treebank. MaltParser itself includes two basic parsing algorithms, but it also provides an interface so that a variety of shift-reduce based algorithms can be used for parsing. For more details on the algorithms and the interface see \citeA{Niv:Hal:Nil:07}.

\subsubsection{Graph-Based Parsing}
\label{ssc:background:graph-based-parsing}
Graph-based parsing employ already established and extensively studied graph processing algorithms to generate a dependency graph for a sentence. In contrast to the transition-based approach where the learning locally trained, graph-based approaches parametrize models globally on substructures of a dependency structure. The main aspect of the parsing is then to give a \textit{score} to each substructure, and return the structure with the highest score. 

The scoring function is at the core of the graph-based approach. Scores can be calculated using linear classifiers, or use conditional or joint probabilities. The \textit{arc-factored model} is the simplest of the graph-based dependency parsing approaches. It is often referred to as a first-order model, due to its scoring function where possible graphs are evaluated one edge at a time. There are also second- and third-order models where the scoring function decompose the graph in larger fragments, and calculate the value of each graph in more complex manners. The scoring also has a set of restriction that prohibits the parser from producing invalid outputs \cite{KublerEtAl:09}.

The Mate parser \cite{Boh:10} is an openly available second-order graph-based dependency parser. The parser employs a second order maximum spanning tree algorithm, a modification of the algorithm found in \citeA{Carreras:07}, and combine this with a passive-aggressive perceptron algorithm. The Mate parser, as we will see in chapter \ref{chap:semantic}, has proven to consistently show very high accuracy for semantic dependency parsing of the English language.\\

\noindent There exists a variety of parsers, and descriptions of systems, that follow a method based on the transition- or graph-based approaches described above. Different statistical modeling or machine learning algorithms are used for the learning, and the same applies for the parsing actions. We will look closer at these in chapter \ref{chap:semantic} when we examine the state-of-the art in semantic dependency parsing. Before we dive into this discussion, we will define semantic parsing, and show how it differs from syntactic dependency parsing.

\section{From Syntactic to Semantic Parsing}
\label{syntactic-semantic}

The most common representations in dependency parsing in the research community have been based on tree data structures. Such representations restrict the type of dependency structures that are possible, and do not fully capture what can be expressed in natural language. \citeauthor{Hudson:90} recognize this by pointing to representations of such phenomena as relative clauses, control relations, and other long-distance dependencies, and show that these can only be represented properly by using more general graphs \cite{Hudson:90}. \citeauthor{Sagae:Tsuji:08} argue that tree data structures cannot fully capture linguistic phenomena beyond so-called shallow syntactic structures \cite{Sagae:Tsuji:08}.

This is further expanded upon by \citeA{Oepen:14}, arguing that tree-oriented parsers are ill-suited for producing meaning representations due to their inability to capture aspects of semantic analysis. Examples of such semantic aspects of a sentence are cases where a lexical unit is the argument of multiple predicates, or when dealing with semantically vacuous word classes that should be left out of the dependency structure. They argue for the move towards semantic dependency parsing using graph data structures so that a more direct analysis of \textit{Who did What to Whom?} can be made possible.

The approaches that are employed for semantic dependency parsing are very similar to that of syntactic dependency parsing, and many successful approaches have used a variation on the models described in this chapter. An early such approach is the modified shift-reduce algorithm proposed by \citeA{Sagae:Tsuji:08}. They introduce a data-driven approach to dependency parsing where directed acyclic graphs (DAGs) are produced directly from an input string by introducing a modified version of the transition-based model described in section \ref{data-driven}. Two new transitions are introduced to the three transitions that we have already described above: \textit{left-attach} and \textit{right-attach}. These transitions can be described informally as:

\begin{enumerate}
\item Left-Attach: add a dependency arc $(w_i, r, w_j)$ to the set $A$ between the top two items on the stack $\alpha$, making the top item $w_i$ the head and the item below it $w_j$ the dependent, if and only if there is no arc between them already.
\item Right-Attach: add a dependency arc $(w_i, r, w_j)$ to the set $A$ between the top two items on the stack $\alpha$, making the top item $w_i$ the dependent and the item below it $w_j$ the head, if and only if there is no arc between them already. Remove the top item on the stack $\alpha$ and place it back on the buffer $\beta$.
\end{enumerate}

This novel approach makes it possible to represent certain semantic aspects of a sentence that is not possible with syntactic dependency parsing, where such restrictions such as the single-head constraint is put in place. As \citeA{Sagae:Tsuji:08} further note, there are also other semantic aspects of a sentence that semantic dependency parsing manages to capture, such as anaphoric reference and semantically motivated predicate-argument relations. Until these approaches transition-based parsing was mainly restricted to tree based parsing. In Chapter \ref{chap:semantic} we will examine modifications to transition-based systems that make it possible to handle graphs.

\section{Conclusions}

In this chapter we have shortly outlined the history of dependency grammar and parsing. We have examined grammar-driven and data-driven dependency parsing, and given an outline of different approaches within these two parsing traditions. We have provided definitions for certain technical aspects of dependency parsing that are deemed as central to our thesis. We have laid the foundations for the next chapters.

In the following chapter we will examine the state-of-the art in semantic dependency parsing. We will focus on a group of parsers that have are state-of-the-art in terms of their accuracy. We will also review annotated data that these parsers have used for training, development and testing.