\chapter{Background}
\label{chap:background}

This chapter provides an introduction to dependency grammar and reviews the state-of-the-art in dependency-based parsing. We start by exploring dependency grammar in order to provide the reader with some background, and to formally define a set of properties that will be used as basis for our thesis. We then examine two main approaches to dependency-based parsing: the \textit{grammar-driven} and the \textit{data-driven} approach. However, grammar-driven and data-driven approaches to dependency parsing are not mutually exclusive, and as we shall see, there are approaches that rely on both. Our focus in this chapter will be on data-driven approaches to dependency parsing. This is due to our observation that most of the recent research on dependency-based parsing use a data-driven approach. In addition, the analysis that we present in chapter \ref{chap:analysis} are based on data-driven parsers, and our experiments in chapter \ref{chap:experiments}, rely on data-driven methods.

We differentiate dependency-based parsing into two classes: \textit{syntactic dependency parsing} and \textit{semantic dependency parsing}. As a superficial starting point we can state that the dependency relations in the former are represented predominantly as \textit{tree} data structures in the research community, while the latter are represented as \textit{graph} data structures. In section \ref{syntactic-semantic} we examine the hypothesis that tree-based data structures are best suited for the analysis of grammatical structure, but often lack the expressiveness needed to capture sentence semantics. We will argue for the need to move towards more general graph-based dependency parsing due to the limitations that tree-based parsing imposes on meaning representation.

In order to simplify our discussion, we will use the term dependency graph in section \ref{grammar} on dependency grammar, even though what we are discussing may be defined, strictly speaking, as a tree. We do this based on two assumptions:

\begin{enumerate}
\item A tree can be defined as an acyclic directed graph; all possible trees are a subset of all possible graphs.
\item The difference between tree-based and graph-based dependency structures are only of interest to our discussion when dealing with parsing techniques and algorithms, and not when discussing dependency grammar.
\end{enumerate}

We do not attempt at providing a comprehensive historical review of dependency grammar, nor an in-depth formal description of the various approaches and algorithms to dependency-based parsing. Our main objective in this chapter is to present the reader with the necessary background and context for our research and analysis in subsequent chapters. We start this chapter off with a short review of dependency grammar.

\section{Dependency Grammar}
\label{grammar}

The early roots of dependency grammar can possibly be traced back to P\={a}\d{n}ini's grammar of Sanskrit written in approximately 350-250 BC \cite{Kruijff:02}. However, the modern study of  dependency grammar is first presented in the works of \citeauthor{Tes:15}. In his seminal work, \textit{Elements of Structural Syntax}, \citeauthor{Tes:15} presents a theory of syntax by focusing on what he calls a \textit{connection} and a \textit{dependency}:

\begin{displayquote}
The sentence is an \textit{organized whole}; its constituent parts are the \textit{words}. Every word that functions as part of a sentence is no longer isolated as in the dictionary: the mind perceives \textit{connections} between the word and its neighbors; the totality of these connections forms the scaffolding of the sentence. The structural connections establish relations of \textit{dependency} among the words. Each such connection in principle links a \textit{superior} term and an \textit{inferior} term. The superior term receives the name \textit{governor (r\'{e}gissant)}; the inferior term receives the name \textit{dependent (subordonn\'{e})} \cite{Tes:15}.
\end{displayquote}

It is these \textit{connections}, according to \citeauthor{Tes:15}, that make a sentence meaningful: \say{[W]ithout them the sentence would not be intelligible} \cite{Tes:15}. The \textit{connections} are used to create a hierarchy between the words in a sentence, where one word is dependent on another, hence the term \textit{dependency}. This is a different approach than for instance \textit{constituency grammar}, where the relationships between lexical units are formed under grammatical constituents. In constituency grammar there is also no hierarchical relationship between the lexical units, instead the hierarchy is grouped by grammatical groups such as the \textit{sentence}, \textit{noun-phrase}, \textit{verb-phrase}, etc. In dependency grammar the lexical units are always atomic, and the dependency relations are all bi-lexical.

From the works of \citeauthor{Tes:15}, the field of dependency grammar has grown into a wide range of traditions that have explored the notion of dependency from a variety of different perspectives. Among these are the Prague School's Functional Generative Description, Meaning-Text Theory, and Hudson's Word Grammar \cite{Sgall:86, Mel:88, Hudson:90}. We will not give a detailed exposition on the differences and similarities between the various approaches to dependency grammar, but rather focus on the aspects that are informative as a precursor to our section on dependency parsing. What follows is a concise and formal definition of dependency grammar, and a set of criteria that can be used for determining dependencies in a sentence. We also introduce important terminology that will be used throughout our thesis insofar as they are related to dependency grammar.

\subsection{Defining Dependencies}
\label{definitions}

A dependency can be described as a binary asymmetrical relation between the lexical units of a sentence, i.e. as arrows pointing from one lexical unit to another. Formally, we describe the dependencies in a sentence $\vec{w} = w_1 ... w_n$ as a directed graph on the set of positions $\vec{w}$ that contain an edge $i \rightarrow j$ if and only if $w_j$ depends on $w_i$ \cite{Kuhl:10}. Directed edges between the lexical units are used in order to represent a dependency going from one lexical unit to another.

The common term used in the research literature for the lexical unit that stands at the beginning of the arrow is \textit{head}. For the lexical unit that is pointed to by the arrow-head the term \textit{dependent} is most common. An exception to the description thus far is the lexical unit that acts as the entry-point of the graph, usually the main verb of the sentence. In order for this lexical unit to have a head, a common strategy is to add an artificial unit to the sentence, often named \textit{root}. The edges in the graph can have \textit{labels} added, which signify the type of relation that exists between heads and dependents.If we examine the dependency graph in figure \ref{dep1}, we can visualise the description above: the verb `brought' that is the entry-point of the graph, artificially added unit \textit{root} that acts as the verbs head, and several labeled dependencies between the heads and dependents in the sentence. 

If we examine the dependency between the verb `brought' and the noun `Bob': we see that the head of this dependency is the verb `brought', the dependent is the noun `Bob', and the edge that connects them has the label `subj'. The label in this example is used to encode syntactic information: `Bob' acts as the nominal subject of the copula verb `brought'. Now that we have briefly defined the dependency graph, we turn our attention to a set of criteria that has been proposed for determining heads and dependents in a sentence.

\begin{figure}
    \begin{dependency}[]
        \begin{deptext}[column sep=1em, row sep=.1ex]
            Bob \& brought \& the \& cake \& to \& Alice \& . \\
        \end{deptext}
        \deproot[edge unit distance=3.4ex]{2}{root}
        \depedge{2}{1}{nsubj}
        \depedge{2}{4}{obj}
        \depedge{4}{3}{det}
        \depedge{2}{5}{prep}
        \depedge{5}{6}{pobj}
        \depedge[edge unit distance=2.3ex]{2}{7}{punct}
    \end{dependency}
    \caption{A dependency graph with labeled edges.}
    \label{dep1}
\end{figure}

\subsection{Criteria for Dependencies}
\label{criteria}

The criteria for establishing the dependencies in a sentence, i.e. determining which lexical units should be head and dependents in a sentence, are of central concern to dependency grammar. \citeauthor{Niv:05} proposes a set of criteria, with reference to \citeauthor{Zwicky:85} and \citeauthor{Hudson:90}, for establishing dependencies and determining the head \textit{H} and dependent \textit{D} in a construct \textit{C} \cite{Zwicky:85, Hudson:90, Niv:05}:

\begin{enumerate}
\item (\textit{H}) determines the syntactic category of (\textit{C}) and can often replace (\textit{C}).
\item (\textit{H}) determines the semantic category of (\textit{C}), whereas (\textit{D}) gives semantic specification.
\item (\textit{H}) is mandatory, whereas (\textit{D}) can be optional.
\item (\textit{H}) selects the category of (\textit{D}) and whether it is mandatory or optional.
\item The form of (\textit{D}), whether it is agreement or government, depends on (\textit{H}).
\item The position of (\textit{D}) is specified in relation to (\textit{H}).
\end{enumerate}

Different traditions of dependency grammar diverge in the interpretation and use of a specific set of criteria for identifying dependencies. The list above encompasses a set of syntactic and semantic criteria for establishing dependencies, and there have been attempts at providing a single coherent notion of dependency that include all of the criteria above. \citeauthor{Hudson:90} has proposed the usage of the concept of a prototype structure that satisfy all or most of the criteria above, and then using special cases for dependencies that only satisfy one or few criteria \cite{Hudson:90}. \citeauthor{Mel:88} has proposed a set of three dependency types: \textit{morphological}, \textit{syntactic}, and \textit{semantic} \cite{Mel:88}. \citeauthor{Nikula:86} has proposed two categories of constructions, \textit{endocentric} and \textit{exocentric}, for determining dependencies \cite{Nikula:86}. \textit{Valency} is another term that is used as criteria for determining dependencies. We will now define the terminology thus far, and add additional terms that are used in the subsequent chapters insofar as they relate to dependency grammar.

\paragraph{Endocentric Construction} This is a term used for constructions where the dependent is optional and not selected by its head, and where the head can replace the whole without affecting the syntactic structure of the sentence. In terms of the categories above, they are all endocentric with the exception of number 4. \cite{KublerEtAl:09}. The term \textit{head-modifier} is often used in the research literature to describe a construct where the head modifies the dependent either syntactically or semantically. Head-modifer constructs usually fall within the definition of an endocentric construction \cite{Niv:05}.

\paragraph{Exocentric Construction} These are constructions that fail on the first criteria, where the head can substitute the whole construct, but may satisfy the others. The term \textit{head-complement} is often used in recent syntactic theories to describe an endocentric construct \cite{Niv:05}.

\paragraph{Valency} The term \textit{valency} is used to determine the distinction between complements and modifiers. In most theoretical frameworks the term valency is used in relation to the semantic predicate-argument structure that is associated with verbs, nouns and adjectives (for the most part). It is used as a way to describe a construct where the lexeme impose some type of requirement on its dependent that determines how to interpret it as a semantic predicate \cite{Niv:05}.

\paragraph{Projectivity} This is a technical term that sets a boundary on the type of dependencies that are permissible in a graph. This restriction will be important when discussing dependency parsing in section \ref{parsing}. A dependency graph is projective if and only if for all its edges $w_i \rightarrow w_j$ in a sentence $\vec{w} = w_1 ... w_n$, they adhere to the restriction that if $w_i \rightarrow w_k$ then $i < k < j$ when $i < j$, or $j < k < i$ when $j < i$ \cite{KublerEtAl:09}.

\paragraph{Single-head constraint} This term is used to define a constraint where the dependents in a sentence are prohibited from having more than one head. This limitation reduces the flexibility of a dependency graph, and as we shall see in section \ref{syntactic-semantic} on syntactic and semantic dependency parsing, the constraint reduces the possibilities of capturing certain semantic information.\\

There exists a large variety of different traditions and theoretical frameworks of dependency grammar. What we have presented here is a short introduction to some aspects of these theories as a foundation for section \ref{parsing} on dependency-based parsing, and section \ref{syntactic-semantic} where we examine the difference between syntactic and semantic parsing. We do not dive into the finer details of dependency grammar as the these are not seen as informative for our thesis. As \citeauthor{Niv:05} points out, the theories of dependency grammar are only indirectly linked to the techniques used in dependency parsing, and the connection between dependency grammar and dependency-based parsing is largely indirect. \citeauthor{Niv:05} states that one should think of dependency-based parsing as parsing with dependency \textit{representations} rather than with a dependency grammar \cite{Niv:05}. In addition, the work presented in our thesis is based on practical considerations such as the effectiveness and accuracy of different parsing techniques. The grammatical aspects are of interested insofar as they can add to our discussion regarding these aspects of dependency-based parsing, but not of central concern to our thesis.

Now that we have given an outline of dependency grammar, we turn our attention to dependency-based parsing. In doing so we follow \citeA{Carrol:00} in distinguishing between two main approaches to dependency parsing, a \textit{grammar-driven} approach and a \textit{data-driven} approach. They share in common the goal of algorithmically producing a dependency structure for a given sentence (an example of which is seen in table \ref{dep1}). The outcome is a structure where the lexical units of a sentence are connected by dependencies that may have labels attached. What differentiates them is the algorithms, techniques and data used to reach this goal.

\section{Dependency Parsing}
\label{parsing}

The early approaches to dependency parsing used linguistic theories and formal grammars in order to automatically assign a dependency structure for a given sentence. These are usually referred to as grammar-driven approaches to dependency parsing. However, as \citeauthor{Niv:05} points out, even though some dependency parsers are intimately tied with a particular theory, it is more often the case that a parser is based on a \textit{representation} rather than a formal theory. Constituency based parsing, as a contrast, is often more tied to a particular theoretical approach \cite{Niv:05}. The representations used for dependency parsing do not necessarily have a grounding in a formally well-defined theoretical framework either. 

% The set of rules used in a grammar-driven approach may either be implemented manually, or learned using a \textit{machine learning} approach by way of an \textit{annotated} \textit{corpora}. The grammar-driven approach may also have a supplementary part to its parsing where a data-driven part is responsible for either disambiguation or for predicting certain aspects of the dependency structure.

In the more recent literature on dependency parsing there has been a shift towards data-driven approaches. This is due to the fact that these approaches have consistently shown progress in both accuracy, speed and robustness. The data-driven approach is based on different types of machine learning algorithms that are trained on linguistically annotated corpora. A model is built that can, in principle, automatically predict plausible dependency structures for a given sentence. Due to this chronology we start this section with an overview of the grammar-driven approach to dependency parsing.

\subsection{Grammar-Driven Approaches}
\label{grammar-driven}

The grammar-driven approach to dependency parsing relies on explicitly defined grammars for producing a dependency graph. Given a sentence, a strategy is deployed in order to find a dependency structure that belongs to the language defined by a specific grammar. This grammar can be made manually, by way of statistical modelling or machine learning, or a fusion of both. When more than one dependency structure for a given sentence exists in accordance with the grammar, a statistical disambiguation part can be added in order to choose the most probable structure.

% some strategy can be used in an attempt to find the most probable dependency structure as a post-processing step. These approaches mix grammar- and data-driven approaches, and as we shall see in section \ref{data-driven}, they were precursors to the completely data-driven approaches that emerged later.

The earliest works on dependency parsing were closely related to context-free grammars \cite{KublerEtAl:09}. These methods use production rules in a context-free grammar in order to produce dependencies. Standard chart parsing methods are used for implementation, examples of which are the CKY \cite{Younger:67} and Earley's algorithm \cite{Early:70}. The rules themselves can take the form of production or constraint rules, see \citeA{KublerEtAl:09} for details.

\citeauthor{Gaifman:65} proposes a set of three rules for a \textit{dependency system}. The rules are similar to context-free grammars in that they map a sentence $\vec{w} = w_1 ... w_n$ to a sequence of categories $X_1, ..., X_n$, and a relation of dependency $d$ between two lexical units $w_i \rightarrow w_j$ as long as a set of conditions are upheld. The rules that \citeauthor{Gaifman:65} proposes include a set of conditions with restrictions that lead to a dependency structure that is a projective directed tree and upholds the single-head constraint \cite{Gaifman:65}. The approaches mentioned so far produce unlabeled dependency structures.

\citeauthor{Niv:05} points out the results from these early approaches, and the attempts of \citeauthor{Gaifman:65} to show that dependency grammar is only a restricted variant of context-free grammars, led to a to a period of approximately twenty-five years of relative lack of interest towards dependency grammar among researchers working in the field of NLP \cite{Niv:05}. 

The second main approach to grammar-driven dependency parsing is based on what is commonly referred to as \textit{eliminative} parsing. This approach, as opposed to the systems based on context-free grammar, produce dependency structures by continuously eliminating dependencies that violate a set of constraints. The elimination process is repeated until there are no violations. As \citeauthor{Niv:05} notes, the eliminative approach is a constraint satisfaction problem, where all dependency structures not in violation of the constraints would be considered. This approach poses two problems: the first problem being the case where no dependency structure remains, i.e. all suggestions break some constraint, the second problem arises when more than one dependency structure remains \cite{Niv:05}. The latter can be solved using disambigation as a post-processing step.

If the recent research on dependency-based parsing is an indication of where we are headed, there seems to be a move away from grammar-driven driven approaches. Data-driven parsing have shown continuous improvements in both accuracy, speed and robustness.Iin light of this observation, and due to the fact that the parsers examined in chapter \ref{chap:analysis}, and our own experiments in chapter \ref{chap:experiments} are based on data-driven approaches, we do not examine the grammar-driven approach in more detail. Instead, we turn our attention to the data-driven approaches to dependency parsing.

\subsection{Data-Driven Approaches}
\label{data-driven}

Early attempts at data-driven dependency parsing were post-processing steps for disambiguation: taking the multiple outputs of a grammar-driven approach and reaching a final dependency structure by choosing the most likely structure. The disambiguation part consisted of a model trained using a statistical model, or a machine learning algorithm, trained on manually annotated corpora. It is due to this usage a corpora the name data-driven stems from.

% Niv:05, 19

\citeauthor{Eisner:96a} was one of the early developers of an approach that did not rely on a grammar-driven part to reach a dependency structure. He presents three models for flexible probabilistic parsing that assigns both \textit{part-of-speech} tags, and a bare-bones dependency structure without labels, to a sentence. The parser is trained on sentences from the \textit{Wall Street Journal}. Examining the first model that \citeauthor{Eisner:96a} proposes, it is based a Markov process where each pair of words in a sentence is examined, and the probability that a dependency is made between word $i$ and $j$ depends on the (tag, word) pairs at both $i$ and $j$. The other two models are variations on this Markov learning approach. The algorithm used for the parsing is one that is similar to the CKY method mentioned above \cite{Eisner:96a}.


\section{Syntax and Semantics}
\label{syntactic-semantic}