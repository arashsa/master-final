\chapter{Semantic Frame Classification}
\label{chap:experiments}

% random baseline
% most frequent class vs most frequent per lemma
% the latter is a strong baseline
% PSD, multiclass classification is good for PSD, it is noteworthy. 
% For PSD Praha uses a classificator for each lemma
% Sanity test for lemma test
% why is lemma and form higher for PSD. Tense may play a greater role in PSD. 
% surface instead of lexical
% n-gram to window
% form-window
% could have done things in a more greedy fashion, but we used a method where we build upon previous results
% syntactic - label + pos, just labels
% semantic - label + pos, just labels
% SVM undemanding in terms of parameter tuning


We start this chapter with a brief overview of previous work on semantic frame classification, or other work where there is a certain methodological overlap, such as \textit{Word Sense Disambiguation} and \textit{Semantic Role Labeling}. We then take a closer look at the data sets we will use for building a classifier for predicting semantic frames. The experiments and results of the classification will be presented in Chapter \ref{chap:results}, whereas in this chapter we will focus on the methods and machine learning algorithms which serves as the foundation of our experiments. In a similar fashion as our analysis of the results of the three parsing systems in Chapter \ref{chap:analysis}, we use the DM and PSD target representations for our experiments. This is also due to the fact that the PAS target representation does not have annotations for semantic frames.

The main part of this chapter will be dedicated to presenting the \textit{features} that we use as parameters for training our classifier. Features are individual properties that are either derived directly from the data sets, or by way of some transformation. The set of features we use for training our machine learning algorithms will determine their accuracy. We will run our experiments and feature selection on three sets of training data:

\begin{enumerate}
    \item Classifying semantic frames based on \textit{lexical}, \textit{morphological} and \textit{syntactic} features.
    \item Classifying semantic frames based on \textit{lexical}, \textit{morphological} and \textit{semantic} features.
    \item Classifying semantic frames by combining the features of 1. and 2, i.e. using both syntactic and semantic features.
\end{enumerate}

The first set of experiments rely on information that would be available before semantic parsing. Our end result is a classifier where the predictions can be used as part of the feature space for training a semantic dependency parser. The second set of experiments leads to a classifier that relies on the results of a semantic dependency parser for its classification. We leave out syntactic information in this set of experiments in order to compare our results to the SemEval-2015 submissions within the closed track. We then use all of the available data for training and examine if we can get an increase in accuracy by combining syntactic and semantic features.

In this chapter we will also present our experimental setup, i.e. how we prepare the data for training, development and testing. We then provide a description of the machine learning methods used as basis for our experiments. We have decided to run our experiments on \textit{Decision Trees}, \textit{Support Vector Machines}, \textit{Logistic Regression} and \textit{K-nearest neighbors}. These machine learning algorithms ensure that we have a sound comparative basis in order to find an approach which we can obtain good results.

\section{Previous work}

In terms of the specific type of semantic frame detection that we are working on, previous work can be encapsulated to the SemEval-2015 submissions. However, there are problems that are similar to our specific problem, such as \textit{frame-semantic parsing} and \textit{word sense disambiguation}. In terms of the SemEval-2015 submissions, we have presented their performance in Chapter \ref{chap:analysis}.

\section{Experimental setup}
\label{setup}

\begin{table}
    \centering
    \smaller[0.2]
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type} & \textbf{\# Sentences} & \textbf{Frames} & \textbf{\# Frames} \\
        \midrule
        DM training & 28525 & 466 & 494122 \\
        DM development & 7131 & 378 & 123128 \\
        DM test id & 1410 & 290 & 24229 \\
        DM test ood & 1849 & 299 & 23486 \\
        \midrule
        PSD training & 28525 & 5074 & 72006 \\
        PSD development & 7131 & 2705 & 17387 \\
        PSD test id & 1410 & 1174 & 3673 \\
        PSD test ood & 1849 & 1265 & 3882 \\
        \bottomrule
    \end{tabular}
    \caption{The data sets used for training, development and testing. The test set consists of in domain (id) and out of domain (ood) data}
    \label{table:split}
\end{table}

\begin{table}
    \centering
    \smaller[0.2]
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type} & \textbf{\# Sentences} & \textbf{Frames} & \textbf{\# Frames} \\
        \midrule
        DM training & 28525 & 294 & 67493 \\
        DM development & 7131 & 231 & 16295 \\
        DM test id & 1410 & 162 & 3459 \\
        DM test ood & 1849 & 173 & 3750 \\
        \midrule
        PSD training & 28525 & 4951 & 69669 \\
        PSD development & 7131 & 2634 & 16761 \\
        PSD test id & 1410 & 1141 & 3584 \\
        PSD test ood & 1849  & 1209 & 3919 \\
        \bottomrule
    \end{tabular}
    \caption{The data sets used for training, development and testing, but excluding frames on the basis of the rules of the SemEval-2015 evaluation criteria.}
    \label{table:split:VandPred}
\end{table}


We have already explored the details of our data sets in Chapter \ref{chap:semantic}. In this section we focus on how we use the data to train, develop and test our machine learning models. As part of SemEval-2015, an official test data set was made available, and as such we do not need to set aside a test set for our final evaluation. In order to have a data set for tuning while we are experimenting with feature selection, we prepare a development set for our experimentation.

In Table \ref{table:split} we can observe some higher order statistics on the training, development and test data sets. We have split the original training set using a 80-20 split, where we extract 20\% of the training data for tuning and development purposes. This is to ensure that we do not run our experiments on the test data set, and thus avoid over-fitting our machine learning models by selecting features that increase the accuracy of our models directly on the test data. Once we are ready to run a final set of tests, we will train our classifiers on the whole training set, including the held out development set used for tuning purposes, and observe the accuracy of our models on the official test set.

In Table \ref{table:split} we observe that the test data consists of in-domain and out-of-domain data. We will test our machine learning models on both these data sets in the final rounds of testing. Our hypothesis, based on the results of the SemEval-2015 submissions, and general intuition about machine learning, is that the accuracy of our classifier will be lower on the out-of-domain data sets.

Examining Table \ref{table:split} further, we see that the training data consists of 28525 sentences, the development data set of 7131 sentences, the in-domain test set of 1410 sentences and out-of-domain test set of 1849 sentences. For the DM target representation we observe a total of 494122 occurrences of frames in the training set, consisting of 466 unique frames. For the PSD training set, we observe 72006 occurrences of frames and a total number of 5074 unique frames. It is therefore important to note that due to the higher number of unique frames in the PSD data set, we can hypothesize that we will observe a relative drop in the accuracy of our classifier in comparison to the DM target representation.

In evaluating the performance of our models, we follow the SemEval-2015 scoring scheme. The scoring scheme has been presented in Chapter \ref{chap:analysis}. In terms of only scoring frames that are not singletons, we do not use this information during training and classification. After the predictions have been made on all tokens that have a part of speech tag that start with a `V', we then have a post-processing step where we exclude tokens that are singletons.

Now that we have some further insight into the data, we move onto an explanation of the type of feature selections we will examine in our experiments. 

\section{Feature Selection}
\label{features}

\begin{figure}
    \centering
    \smaller[]
    \smaller[]
    \smaller[]
    % \tiny
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
            
            g\_p\_n \& say \& it \& expect \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& \_ \\
            
            NNP \& VBD \& PRP \& VBZ \& TO \& VB \& DT \& NN \& IN \& DT \& NN \& IN \& DT \& NN \& . \\
            
            named:x-c \& v\_to:e-i-h-i \& pron:x \& v:e-i-h \& \_ \& v:e-i-p \& q:i-h-h \& n:x \& p:e-u-i \& q:i-h-h \& n\_of:x-i \& \_ \& q:i-h-h \& n:x \& \_ \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{ARG1}
        \depedge{6}{3}{ARG1}
        \depedge{4}{3}{ARG1}
        \depedge{2}{4}{ARG2}
        \depedge{9}{6}{ARG1}
        \depedge{4}{6}{ARG2}
        \depedge{6}{8}{ARG2}
        \depedge{7}{8}{BV}
        \depedge{9}{11}{ARG2}
        \depedge{10}{11}{BV}
        \depedge{11}{14}{ARG1}
        \depedge{13}{14}{BV}
    \end{dependency}
    \caption{DM target representation with tokens, lemma, part of speech tags, semantic frames and labeled semantic dependencies.}
    \label{DM:all}
\end{figure}

Feature selection is a process where we select the data that will be given to our machine learning algorithms as basis for its learning. For each word, we extract a set of features that are fed to the machine learning algorithm as parameters for learning, and the frame as the class that the set of features belong to. Once we have finished training our model, we can start predicting classes for new data by extracting the same set of features, feeding it to our machine learning model, and getting the most probable class as it is defined by the model.

There are no well defined methods by which we can empirically select the features that will result in the most accurate model. This is due to the fact that the feature space that is possible to construct is not finite, and we therefore have no way of testing all possible feature sets in order to find the set that will produce the most accurate classifier. We must therefore heuristically select our features by examining their impact on the accuracy of our classifiers.

When choosing features for our classifier we consider 3 factors:

\begin{enumerate}
    \item Improving accuracy: We aim for features that increase overall accuracy of our model.
    \item Reducing over-fitting: We create a development set so that we do not over-fit our features to the data.
    \item Reducing training and testing time: We employ a range of statistical measures for feature reduction so that we can increase the speed of training in order to run as many experiments as needed during our feature selection.
\end{enumerate}

We have chosen to base our feature selection on 4 sets of \textit{feature types}: lexical, morphological, syntactic and semantic. Examining Figure \ref{DM:all}, we have an example sentence taken from the training data which includes most of the information we will use as features. This example has been taken from the DM target representation. We start from the top of this figure. The first layer of information is the labeled semantic dependencies. We will use labeled dependencies as part of our feature space in the second and third classifier. The next layer of information we have the \textit{form tokens}, i.e. the words in their original form. 

In the next layer we have \textit{lemmatised tokens}. The \textit{lemma} of a word is its base representation. An example of this is the word forms `run', `runs', `ran' and `running', which are different word forms that share the same lemma `run'. Lemmatisation is a \textit{lossy} processing step whereby we lose some information. What we gain from processing tokens to find their base representation is a reduction of vocabulary, which gives us more coarse representations of a word and its context. As such the usage of lemma as features can increase classification accuracy for a set of tasks. However, some information is also lost in this process, and for more fine-grained classification this might pose some issues. 

The layer below the lemma tokens, we have part of speech tags. These are grammatical categories, such as Nouns, Verbs, Adverbs, that have been assigned to the tokens of a sentence.

In addition to the data presented in Figure \ref{DM:all}, we also use data from a syntactic parser. This data has been provided by the SemEval-2015 organizers as additional resources for semantic dependency parsing. Labeled syntactic dependencies will be used in our first classifier.

\subsection{Lexical and morphological features}

The lexical features that we will examine in our experiments are \textit{word forms}, \textit{lemmas}, \textit{prefixes} and \textit{suffixes}. The information contained in word forms are the most fine grained information that resides in a sentence. The form tokens are informative parts of a sentence that all the other information layers are in some ways abstractions of, or based on. It is therefore a good starting point for our experiments to extract the word form as the sole feature for our classification. However, word forms suffer from the fact that in most corpora beyond a certain size, there will be a large set of words that are very infrequent. Using word form as the basis for semantic frame classification will therefore be too sparse, and there will be many cases where additional information is needed in order to correctly classify a token with the right frame.

The lemma of a token will not fully resolve the sparsity connected to infrequent word forms. However, it will reduce the vocabulary of a corpora, and provide learning examples whereby a machine learning algorithm can connect an infrequent word form with its family of word forms, i.e. its lemma. However, for more frequent verbs, the disambiguation needed to distinguish between many classes of semantic frames, might lose accuracy if we use lemma as a substitute for word form. A combination of form and lemma, which we will be confirmed through our experimentation, is more likely to produce greater accuracy for certain highly frequent tokens. 

Prefixes and suffixes of word forms may introduce patterns in the data that may be useful for semantic frame classification. Our hypothesis is that, particularly suffixes, with such patterns as `ing', `ed', and `ang' may provide some additional information on the usage and context of a verb. However, these may also be too general too actually provide useful features for distinguishing between the exact frame of a token.

\subsection{Syntactic}

The syntactic features we will experiment with as part of our feature selection are the labeled dependencies in a syntactic dependency tree. Each sentence in the data set has a corresponding syntactic dependency tree. These trees are highly interesting in relation to our classification task. The semantic frame of a token is in many cases connected to the number of arguments a verb can take, and the type of these dependencies. Using the dependents of a token as a set of features may help distinguish between the various frames for ambiguous verbs that can be assigned a number of frames. 

\subsection{Semantic}

For semantic features we are using the gold standard semantic graphs provided by the SemEval-2015 organizers in the DM and PSD target representations. In the same way as syntactic features, the semantic graphs can be useful in distinguishing frames by the number of semantic dependents a verb can have.

 
\section{Classification Algorithms}

We have chosen a set of four classifiers for predicting semantic frames in order to have a variety of approaches where we can empirically find a machine learning approach that yields satisfactory results. We start this chapter by providing a short description of the four algorithms that we have chosen for our classification task. These four machine learning algorithms are all implemented and openly available as part of the scikit-learn toolkit.

Scikit-learn is a high-level machine learning toolkit written in the Python programming language. The authors of scikit-learn describe the toolkit as a set of state-of-the art machine learning algorithms that have been designed for usage on medium-scale supervised and unsupervised problems \cite{scikit-learn}.

It is worth noting that we ran the experiments on all four algorithms to a certain limit. Once we had established the classifier that consistently performed with the highest accuracy, we limited further explorations to that classifier. Since running the experiments on each classifier is time consuming, this decision was necessary once we started performing more in-depth feature selections. Our aim is to examine the possibilities of increasing the accuracy of frame detection to rival that of the current state-of-the-art.

\subsection{Decision Trees}

\textit{Decision Tree Classifiers} learn rules by creating a tree structure with simple rules that are easy to analyse and understand. This can be contrasted to more complex machine learning algorithms such as neural nets. In the latter cases the learning and predication is more like a `black-box' in terms of comprehensibility.

The decision tree algorithm learns based on a set of labeled instances by inductively setting up a set of rules which will form a tree. Classification is done by starting at the root node, and base on the rules classify new data by traversing the tree and reaching a node that is designated a specific class from the previous learning. As Kotsiantis note, the process by which a decision tree classifier makes predictions is similar to a greedy search \cite{Kotsiantis:13}.

The decision tree classifier that is part of the scikit-learn toolkit has a small set of options for creating different types of decision trees. The options include setting the split criteria, where the options are \textit{gini} and \textit{entropy}. As Kotsiantis note, gini and entropy are two different measure used as the splitting measure that the learning algorithm uses in order to create the decision trees \cite{Kotsiantis:13}.

Our experiments showed that decision trees can be a fast and accurate machine learning algorithm for the multi-class classification task of frame detection. However, it did not score high enough to be considered for further exploration beyond the morphological features described in Section \ref{features}.

\subsection{Support Vector Machines}

\textit{Support vector machines} are well suited for multi-class classification tasks, particularly when dealing with high dimensional feature sets used for training and predication. However, as Hsu and Lin note, at the time when support vector machines where created it was initially made for binary classification, but a number of methods have been constructed where support vector machines have been extended to handle multiple classes \cite{Hsu:02}.

The scikit-learn toolkit has 3 implementations of support vector machines. After a few experiments, we ended up using the `LinearSVC' class, which is an wrapper around the `LIBLINEAR' library developed by  \citeA{liblinear}.

The `LIBLINEAR' library is an open source library for large-scale linear classification. It is well suited for large data and feature sets, and it is particularly recommended for text classification by its developers. In certain cases it is also known to be faster than many support vector implementations, including the often used `LIBSVM' library \cite{liblinear}. The other scikit-learn support vector machine algorithms are based on `LIBSVM', which proved to be relatively slower once our feature sets grew to include syntactic and semantic features. We found that by using the `LinearSVC' class in the scikit-learn toolkit, we could reduce training time, and where thus capable of running a much larger set of feature sets for experimentation.

Our experiments showed that support vector machines had the highest overall scores among the four machine learning algorithms on the first set of morphological features. It is a versatile algorithm with a wide range of options for tuning, and has proven as a go-to machine learning algorithm for many NLP tasks. We therefore opted to use this machine learning approach once we started examining syntactic and semantic features.

\subsection{Logistic Regression}

Logistic regression is a machine learning model based on regression analysis. The implementation used in scikit-learn, like the support vector machine implementation described above, is also based on the `LIBLINEAR' library. The implementation is of a multivariate logistic regression model with the loss function being $log(1+e^{y_iw^Txi})$, which has been derived from a probabilistic model \cite{liblinear}. 

We achieved relatively high accuracy using the logistic regression model at a training time that allowed for experimentation with different feature sets. However, as we saw that the support vector implementation scored relatively higher for all lexical and morphological features, we did not further test this algorithm once we started experimenting with syntactic and semantic features.

\subsection{K-nearest neighbors}

Nearest neighbors classification is a type of machine learning method where training data is stored directly in a model, and a computation is performed by way of a simple majority vote of the nearest neighbors of each point in the model. It is a non-parametric method used for both classification and regression, where in the classification we the output is a class, and in regression the output is the average of the values of its \textit{k} neighbors \cite{Altman:92}.

We did not achieve comparably high accuracy using the k-nearest neighbors classifier implemented in the scikit-learn toolkit. We therefore opted to leave out this algorithm as well once we started our experiments with syntactic and semantic features.

\section{Conclusions}

In this chapter we have reviewed our experimental setup, the features that we will use as basis for our experiments, and the machine learning algorithms that we will examine. In the next chapter we will present the results of our feature selection and experiments with different machine learning techniques.