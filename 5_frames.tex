\chapter{Semantic Frame Classification}
\label{chap:experiments}


We start this chapter by taking a closer look at the data sets that we use for training, developing and testing, through a set of experiments, a classifier for predicting semantic frames. The experiments and results of the classification will be presented in Chapter \ref{chap:results}. In this chapter we will focus on the feature selection and machine learning algorithms which serve as the foundation of our experiments. 

We use the DM and PSD target representations as basis for our experiments, and leave out PAS as it does not have semantic frames as part of its annotation scheme. This aligns with the analysis of the SemEval-2015 submissions from the previous chapter. We will also rely on the analysis of semantic frames from the previous chapter as basis for our feature selection.

The main part of this chapter will be dedicated to presenting the \textit{features} that we use as parameters for training our classifier. Features are individual properties that are either derived directly from the data sets, or by way of some transformation which can also include additional resources. The set of features we use for training our machine learning algorithms will determine their accuracy. We split our semantic frame classification into three different classifiers where the data used for training differentiates their outcome and usage:

\begin{enumerate}
    \item Classifying semantic frames based on \textit{lexical}, \textit{morphological} and \textit{syntactic} features.
    \item Classifying semantic frames based on \textit{lexical}, \textit{morphological} and \textit{semantic} features.
    \item Classifying semantic frames by combining the features of 1. and 2, i.e. using both syntactic and semantic features.
\end{enumerate}

The first set of experiments rely on information that does not rely on semantic dependency graphs. Our end result is a classifier where the predictions can be used as part of the feature space for training a semantic dependency parser. This could possibly increase the accuracy of state-of-the-art semantic dependency parsers. The second set of experiments leads to a classifier that relies on the results of a semantic dependency parser for its classification. We leave out syntactic information in this set of experiments in order to compare our results to the SemEval-2015 submissions that participated in the closed track. The last set of experiments would be a semantic frame classifier that relies on both a syntactic and semantic parser, and would be comparable to the parsing systems that participated in the SemEval-2015 open track.

In this chapter we will also present our experimental setup, i.e. how we prepare the data for training, development and testing. We will also provide overviews of the machine learning algorithms used as basis for our experiments. We have decided to run our experiments on \textit{Decision Trees}, \textit{Support Vector Machines}, \textit{Logistic Regression} and \textit{K-nearest neighbors}. This ensures a comparative basis for the choice of a machine learning algorithm as basis for our classifier.


\section{Experimental setup}
\label{setup}

\begin{table}
    \centering
    \smaller[0.2]
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type} & \textbf{\# Sentences} & \textbf{Frames} & \textbf{\# Frames} \\
        \midrule
        DM training & 28525 & 466 & 494122 \\
        DM development & 7131 & 378 & 123128 \\
        DM test id & 1410 & 290 & 24229 \\
        DM test ood & 1849 & 299 & 23486 \\
        \midrule
        PSD training & 28525 & 5074 & 72006 \\
        PSD development & 7131 & 2705 & 17387 \\
        PSD test id & 1410 & 1174 & 3673 \\
        PSD test ood & 1849 & 1265 & 3882 \\
        \bottomrule
    \end{tabular}
    \caption{The data sets used for training, development and testing. The test set consists of in domain (id) and out of domain (ood) data}
    \label{table:split}
\end{table}

\begin{table}
    \centering
    \smaller[0.2]
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type} & \textbf{\# Sentences} & \textbf{Frames} & \textbf{\# Frames} \\
        \midrule
        DM training & 28525 & 294 & 67493 \\
        DM development & 7131 & 231 & 16295 \\
        DM test id & 1410 & 162 & 3459 \\
        DM test ood & 1849 & 173 & 3750 \\
        \midrule
        PSD training & 28525 & 4951 & 69669 \\
        PSD development & 7131 & 2634 & 16761 \\
        PSD test id & 1410 & 1141 & 3584 \\
        PSD test ood & 1849  & 1209 & 3919 \\
        \bottomrule
    \end{tabular}
    \caption{The data sets used for training, development and testing, but excluding frames on the basis of the rules of the SemEval-2015 evaluation criteria.}
    \label{table:split:VandPred}
\end{table}


We have already explored the details of our data sets in Chapter \ref{chap:semantic}. In this section we focus on how we use the data to train, develop and test our machine learning models. As part of SemEval-2015, an official test data set was made available, and as such we do not need to set aside a test set for our final evaluation. In order to have a data set for tuning while we are experimenting with our feature selection, we need to set aside part of the training data for development purposes.

In Table \ref{table:split} we can observe some higher order statistics on the training, development and test data sets. We have split the original training set using an often used 80-20 split, where we extract 20\% of the training data for tuning and development purposes during our experimentation. This is to ensure that we do not run our experiments on the test data set, and thus avoid over-fitting our machine learning models by selecting features that increase the accuracy of our models directly on the test data.

Once we are ready to run a final set of tests, we will train our classifiers on the whole training set, including the held out development set used for tuning purposes, and observe the accuracy of our models on the official test set. This will then be presented as the final results of our classification task.

In Table \ref{table:split} we observe that the test data consists of in-domain and out-of-domain data. We will test our machine learning models on both these data sets in the final rounds of testing. Our hypothesis, based on the results of the SemEval-2015 submissions, and general intuition about machine learning, is that the accuracy of our classifier will be lower on the out-of-domain data sets.

Examining Table \ref{table:split} further, we see that the training data consists of 28525 sentences, the development data set of 7131 sentences, the in-domain test set of 1410 sentences and out-of-domain test set of 1849 sentences. For the DM target representation we observe a total of 494122 occurrences of frames in the training set, consisting of 466 unique frames. For the PSD training set, we observe 72006 occurrences of frames and a total number of 5074 unique frames. It is therefore important to note that due to the higher number of unique frames in the PSD data set, we can hypothesize that we will observe a relative drop in the accuracy of our classifier in comparison to the DM target representation.

In evaluating the performance of our models, we follow the SemEval-2015 scoring scheme, which was presented in Chapter \ref{chap:analysis}. In terms of only scoring frames that are not singletons, we do not use this information during training and classification. We train our model on all tokens that have a part of speech tag that starts with the character `V'. During classification we also predict semantic frames for all tokens that have a part of speech tag that starts with `V', but exclude tokens that are singletons as a post-processing step when we evaluate our models.

Now that we have some further insight into the data, and the experimental setup for our classification task, we present the feature selection that will function as basis for our experiments.

\section{Feature Selection}
\label{features}

\begin{figure}
    \centering
    \smaller[]
    \smaller[]
    \smaller[]
    % \tiny
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
            
            g\_p\_n \& say \& it \& expect \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& \_ \\
            
            NNP \& VBD \& PRP \& VBZ \& TO \& VB \& DT \& NN \& IN \& DT \& NN \& IN \& DT \& NN \& . \\
            
            named:x-c \& v\_to:e-i-h-i \& pron:x \& v:e-i-h \& \_ \& v:e-i-p \& q:i-h-h \& n:x \& p:e-u-i \& q:i-h-h \& n\_of:x-i \& \_ \& q:i-h-h \& n:x \& \_ \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{ARG1}
        \depedge{6}{3}{ARG1}
        \depedge{4}{3}{ARG1}
        \depedge{2}{4}{ARG2}
        \depedge{9}{6}{ARG1}
        \depedge{4}{6}{ARG2}
        \depedge{6}{8}{ARG2}
        \depedge{7}{8}{BV}
        \depedge{9}{11}{ARG2}
        \depedge{10}{11}{BV}
        \depedge{11}{14}{ARG1}
        \depedge{13}{14}{BV}
    \end{dependency}
    \caption{DM target representation with tokens, lemma, part of speech tags, semantic frames and labeled semantic dependencies.}
    \label{DM:all}
\end{figure}

Feature selection is a process where we select the data that will be given to our machine learning algorithms as basis for its learning. For each token in the data, we extract a set of features that are used as input data for the machine learning algorithms as parameters for learning. During the training, each feature set has a corresponding semantic frame as its correct class. The machine learning will use this as basis for learning a mapping from a set of features to a correct semantic frame. Once a model has been trained, we can evaluate its performance by predicting classes for unseen tokens in the left out data used for development purposes. The score from this evaluation will be the basis for tuning, i.e. changing the set of features used for training.

There are no well defined methods by which we can empirically select the features that will result in the most accurate machine learning model. This is due to the fact that the feature space that is possible to construct is not finite, and we therefore have no way of testing all possible feature sets in order to find the set that will produce the most accurate classifier. We must therefore heuristically select our features by examining their impact on the accuracy of our classifiers.

When choosing features for our classifier we consider 3 factors:

\begin{enumerate}
    \item Improving accuracy: We aim for features that increase overall accuracy of our model.
    \item Reducing over-fitting: We create a development set so that we do not over-fit our features to the data.
    \item Reducing training and testing time: We employ a range of statistical measures for feature reduction so that we can increase the speed of training in order to run as many experiments as needed during our feature selection.
\end{enumerate}

We have chosen to base our feature selection on 4 sets of \textit{feature types}: lexical, morphological, syntactic and semantic. Examining Figure \ref{DM:all}, we have an example sentence taken from the training data which includes most of the information we will use as features. This example has been taken from the DM target representation. We start from the top of this figure. 

The first layer of information is the labeled semantic dependencies. We will use labeled dependencies as part of our feature space in the second and third classifier. For the second classifier we will use syntactic dependencies. For the syntactic dependencies we use the companion files distributed as part of SemEval-2015, where syntactic dependencies on the training and test data have been produced using the so-called Stanford Basic scheme derived from the Penn Treebank \cite{PennTreebank}. The parser of \citeA{Boh:Niv:12} have been used for the parsing.

In the next layer of information we have the \textit{token forms}, i.e. the words in their original form. The word forms are the units of information that are closest to the actual data (corpora), and the processing that has been performed in this layer is a tokenization step. For frame classification, we will use the token form as basis for our first set of experiments. However, as we will show in our experiments, using token form as our sole feature has its limitations, and with the size of our data and choice of machine learning algorithms, additional features are necessary in order to achieve state-of-the-art accuracy.

In the next layer of Figure \ref{DM:all}, we have the \textit{token lemma}. The \textit{lemma} of a word is its base representation. An example of this are the verbs `run', `runs', `ran' and `running', which are different word forms that share the same lemma `run'. Lemmatisation is a \textit{lossy} processing step whereby we lose some information by transforming words from one form to another. What we gain from processing tokens to find their base representation is a reduced vocabulary size, which in turn presents us with more coarse representations of a token and its context. The usage of lemma as features can thus increase classification accuracy for a set of tasks. However, some information is lost in this process, which for the case of semantic frame classification can lead to lower precision for tokens that are possibly mapped to a large set of semantic frames.

The layer below the token lemma, we have part of speech tags. These are grammatical categories, such as Nouns, Verbs, Adverbs, that have been assigned to the tokens of a sentence. Part of speech tags can help in disambiguation tasks when there is a possible link to a semantic frame for a token that has a set of possible frames where token form, or token lemma, fall short.

We will now examine how we will use these possible features in our experiments, and give definitions for these feature sets.

\subsection{Lexical and Morphological Features}

The lexical features that we will examine in our experiments are the \textit{token forms}, \textit{token lemmas} themselves, and \textit{prefixes} and \textit{suffixes} that we can derive form these. The information contained in word forms are the most fine grained information that resides in a sentence. The token forms are the information that acts as the basis for all the other features, which have been derive either directly, or indirectly, from these ground units. However, word forms suffer from the fact that in most corpora beyond a certain size, there will be a large set of infrequent word forms. It is therefore likely that using token forms will run into an issue of sparsity for infrequent ambiguous verbs that can be labeled with a range of semantic frames.

The lemma of a token will not fully resolve the sparsity connected to infrequent word forms. However, it will reduce the vocabulary of a corpora, and provide learning examples whereby a machine learning algorithm can connect an infrequent word form with its family of word forms, i.e. its lemma. However, for more frequent verbs, the disambiguation needed to distinguish between many classes of semantic frames, we might lose accuracy if we use lemma as a substitute for word form. A combination of form and lemma, which will be confirmed through our experimentation, is more likely to produce greater accuracy.

Prefixes and suffixes of word forms may introduce patterns in the data that may be useful for semantic frame classification. Our hypothesis is that, particularly suffixes, with such patterns as `ing', `ed', and `ang', may provide some additional information on the usage and context of a verb. However, these may also be too general too actually provide useful features for distinguishing between the correct semantic frame.

\subsection{Syntactic Features}

The syntactic features we will experiment with as part of our feature selection are the labeled dependencies in a syntactic dependency tree. Each sentence in the data set has a corresponding syntactic dependency tree. These trees are highly interesting in relation to our classification task. The semantic frame of a token is in many cases connected to the number of arguments a verb can take, and the type of dependency labels they have. Using the dependents of a token as a set of features may help distinguish between the various frames for ambiguous verbs where other features may be less relevant. Our experiments will include using the dependency labels in connection with the token form, token lemma, and the part of speech tag of the dependent.

We can also use the head of a token as its feature. Our hypothesis is that the heads of verbs do not add the necessary information that could possibly help in distinguishing their specific frame.

\subsection{Semantic}

For semantic features we are using the gold standard semantic graphs provided by the SemEval-2015 organizers in the DM and PSD target representations. In the same way as syntactic features, the semantic graphs can be useful in distinguishing frames by using the dependents in a semantic dependency graph. Examining Figure \ref{DM:all}, we can illustrate this by examining the verb `complete', which has two dependents: `it' and `issue', and where both have the labels `ARG1'. This indicates that this particular verb can have two dependents that are of \textit{type} `ARG1', which might distinguish it from other verbs used in the same context. When using dependents as features, we are faced with the possibility of having to sparse feature representations. Using the part of speech tags of dependents, instead of their form or lemma, can possibly help mitigate this, and lead to representations that are more informative for our classification.

Now that we have presented the possible feature space that we will use for our classification task, we will now examine the machine learning algorithms used as basis for our classification.

 
\section{Classification Algorithms}

We have chosen a set of four classifiers for predicting semantic frames in order to have a variety of approaches where we can empirically find a machine learning approach that yields satisfactory results. We start this chapter by providing a short description of the four algorithms that we have chosen for our classification task. These four machine learning algorithms are all implemented and openly available as part of the \textit{scikit-learn toolkit}.

Scikit-learn is a high-level machine learning toolkit written in the Python programming language. The authors of scikit-learn describe the toolkit as a set of state-of-the art machine learning algorithms that have been designed for usage on medium-scale supervised and unsupervised problems \cite{scikit-learn}. It well established machine learning toolkit that is both used in research and business.

It is worth noting that we ran the experiments on all four algorithms to a certain limit. Once we established the classifier that consistently performed with the highest accuracy on our chosen set of initial features, we limited further explorations to the most accurate algorithm. Since running the experiments on each classifier is time consuming, this decision was necessary once we started performing more in-depth feature selections so that we could proceed in a timely fashion. Our aim is to examine the possibilities of increasing the accuracy of frame detection to rival that of the current state-of-the-art.

\subsection{Decision Trees}

\textit{Decision Tree Classifiers} learn rules by creating a tree structure with simple rules that are easy to analyze and understand. This can be contrasted to more complex machine learning algorithms such as \textit{neural networks}. In the latter cases the learning and predictions made by the algorithm acts like a `black-box' in terms of comprehensibility.

The decision tree algorithm learns based on a set of labeled instances by inductively setting up a set of rules which form a tree structure. The tree structure can be visualized by thinking of each node in the tree as a question. Classification is done by starting at the root node, and based on the rules that have been learned, classify new data by traversing the tree and reaching a node that is designated by a specific class. For each node, a check is made on the unseen data that is to be labeled as the traversing step. As Kotsiantis note, the process by which a decision tree classifier makes predictions is similar to a greedy search \cite{Kotsiantis:13}.

The decision tree classifier that is part of the scikit-learn toolkit has a small set of options for creating different types of decision trees. The options include setting the split criteria, where the options are \textit{gini} and \textit{entropy}. As Kotsiantis note, gini and entropy are two different measure used as the splitting measure that the learning algorithm uses in order to create the decision trees \cite{Kotsiantis:13}.

Our experiments showed that decision trees can be a fast and accurate machine learning algorithm for frame classification. However, it did not score high enough to be considered for further exploration beyond the morphological features described in Section \ref{features}. We also observed that the high dimensionality of features used in our classifier was not suited for a decision tree classifier.

\subsection{Support Vector Machines}

\textit{Support vector machines} are well suited for multi-class classification tasks, particularly when dealing with high dimensional feature sets as in our case. As Hsu and Lin note, at the time when support vector machines where created it was initially made for binary classification. However, a number of methods have been constructed whereby a support vector machine can be extended to handle multiple classes \cite{Hsu:02}.

The scikit-learn toolkit has 3 implementations of support vector machines. After a few experiments, we ended up using the `LinearSVC' class, which is a wrapper around the `LIBLINEAR' library developed by  \citeA{liblinear}. Without parameter tuning, this implementation proved the most accurate on our initial set of experiments. We not include the experiments run on the other implementations that are part of the scikit-learn tookit.

The `LIBLINEAR' library is an open source library for large-scale linear classification. It is well suited for large data and feature sets, and it is particularly recommended for text classification by its developers. In certain cases it is also known to be faster than many support vector implementations, including the often used `LIBSVM' library \cite{liblinear}. The other scikit-learn support vector machine algorithms are based on `LIBSVM', which proved to be relatively slower once our feature sets grew to include syntactic and semantic features. We found that by using the `LinearSVC' class in the scikit-learn toolkit we reduce training time, which made it possible to run more experiments in comparison with the other implementations. 

Our experiments showed that support vector machines had the highest overall scores among the four machine learning algorithms on the first set of morphological features. It is a versatile algorithm that performed well without any parameter tuning. We therefore opted for this algorithm once we reached a threshold of experimentation on the basis of its comparatively higher accuracy. It is not definite that the other machine learning algorithms could not produce comparable, or even higher accuracy. However, this would have demanded a greater effort in examining the possible range of parameter tuning for all four algorithms, which unfortunately was deemed outside the scope of our thesis.

\subsection{Logistic Regression}

Logistic regression is a machine learning model based on regression analysis. The implementation used in scikit-learn, like the support vector machine implementation described above, is based on the `LIBLINEAR' library. The implementation is of a multivariate logistic regression model with the loss function being $log(1+e^{y_iw^Txi})$, which has been derived from a probabilistic model \cite{liblinear}. 

We achieved relatively high accuracy using the logistic regression model at a training time that allowed for experimentation with different feature sets. However, as we saw that the support vector implementation scored relatively higher for all lexical and morphological features, we did not further test this algorithm once we started experimenting with syntactic and semantic features. It is our hypothesis that with devoted analysis to the effects of parameter tuning, logistic regression could potentially be an interesting rival to support vector machines for our case. This is based on the fact that logistic regression and support vector machine implementations showed relatively similar accuracy levels on most of the experiments we ran on both algorithms.

\subsection{K-nearest neighbors}

Nearest neighbors classification is a type of machine learning method where training data is stored directly in a model, and a computation is performed by way of a simple majority vote of the nearest neighbors of each point in the model. It is a non-parametric method used for both classification and regression, where in the classification the output of our model is a class, and in regression the output is the average of the values of its \textit{k} neighbors \cite{Altman:92}. The k-nearest neighbors algorithm can also be used as an unsupervised learning algorithm, and is often used as basis for tasks that involve some form of clustering.

We did not achieve comparably high accuracy using the k-nearest neighbors classifier implemented in the scikit-learn toolkit. We therefore opted to leave out this algorithm as well once we started our experiments with syntactic and semantic features. In fact, for high feature dimensions, the accuracy of this algorithm performed well below the other three algorithms we examined.

\section{Conclusions}

In this chapter we have reviewed our experimental setup. We have presented the training, development, and test data, and given an overview of the distribution of semantic frames across these sets. We have also presented the feature types that we will use as basis for our experiments, and the machine learning algorithms that we will examine as basis for creating our classifiers. In the next chapter we will present the results of our feature selection and experiments with different machine learning techniques. We show that with in-depth feature selection, it is possible to rival the accuracy of state-of-the-art semantic frame classification, and that our results can be used as basis for enhancing the best submissions in the closed track of SemEval-2015.