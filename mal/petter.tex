\documentclass[a4paper,12pt,english]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tikz-dependency}
\usepackage{times}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{booktabs}
\usepackage{covington}
\usepackage{relsize}
\usepackage{hyperref}
\usepackage{appendix}
\usepackage{ifimasterforside}
\usepackage{emptypage}
\usepackage{tocbibind}
\usepackage{url}
\usepackage{tikz}
\usepackage{apacite}
\usepackage{amsmath}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\renewcommand{\chaptermark}[1]{\markboth{\textsc{\thechapter.\ #1}}{}}
\lhead[\leftmark]{}
\rhead[]{\rightmark}
\lfoot[\thepage]{}
\rfoot[]{\thepage}

\title{Optimizing a PoS Tag Set for Norwegian Dependency Parsing}
\author{Petter Hohle}

\begin{document}
\ififorside{}

\chapter*{Abstract}
\thispagestyle{empty}
This thesis presents a systematic, empirical investigation of how an existing
PoS tag set can be modified and optimized for the task of syntactic dependency
parsing of Norwegian. The tag set of the Norwegian Dependency Treebank is
modified and optimized through experiments with the morphological features in
the treebank. The experiments are complemented by evaluation of a range of
state-of-the-art PoS taggers and syntactic parsers applied to Norwegian. The
results of our work are concrete contributions to the Norwegian NLP community:
(i) a data set split (training/development/test) of the Norwegian Dependency
Treebank; (ii) a PoS tag set optimized for syntactic dependency parsing of
Norwegian; (iii) a PoS tagger model trained on the treebank; and (iv) a
syntactic parser model trained on the treebank.
%These resources are made publicly available\footnote{See
%\url{https://www.github.com/petterhh/ndt-tools}} with the hope that they can
%be found useful by researchers and others interested in applying and advancing
%language technology for Norwegian.

\chapter*{Acknowledgements}
\thispagestyle{empty}
First of all, I want to express my sincere gratitude to my supervisors, Erik
Velldal and Lilja Øvrelid, for their exceptional guidance and for going out of
their way to assist me in overcoming obstacles, large or small. The importance
of their meticulous editing and feedback cannot be overstated, and their
encouragements and confidence in my research inspired me greatly.
\\

I want to thank my fellow students for the sorely needed coffee and lunch
breaks and the staff at the Language Technology Group for including me and
making me feel like a peer.
\\

I am very grateful to the National Library of Norway for granting me a
scholarship for my thesis work. Furthermore, I want to thank them for their
development of the Norwegian Dependency Treebank, which made this thesis
possible.
\\

I would also like to thank the developers of the taggers and parsers I
utilized, for making their tools and data open-source and publicly available.
\\

Last, but certainly not least, I want to extend my gratitude to my family, for
always believing in me and providing encouraging words and their continuous
support.

\frontmatter{}
\fancyhead[LE]{\nouppercase\leftmark}
\fancyhead[RO]{\nouppercase\rightmark}
\tableofcontents{}
\listoftables{}
\listoffigures{}

\mainmatter{}
\chapter{Introduction}
\label{sec:introduction}
%The size and granularity of PoS tag sets may greatly
%affect the performance of PoS taggers, as while large tag sets with
%fine-grained tags provide more linguistic information, this generally comes
%at the expense of tagger accuracy.

%%PoS tagging is an important pre-processing
%%step for syntactic parsing, which is the process of assigning syntactic
%%structure to sentences. It is therefore interesting to investigate how the PoS
%%tagging, and specifically the tag set granularity, may affect the performance
%%of parsers. The additional linguistic information provided by the more
%%fine-grained tags may be syntactically informative and rectify the drop in
%%parser accuracy scores.

Part-of-speech (PoS) tagging is a central component in many Natural Language
Processing (NLP) tasks, as it provides morphosyntactic analysis by assigning
each word in a particular sentence a part-of-speech tag taken from a predefined
set of potential tags, called the tag set. In particular, the resulting PoS
tags allow for syntactic parsing, which involves analyzing the structure of the
sentence. PoS tag sets are generally developed with the linguistic
considerations and preferences of their developers in mind
%and not necessarily
rather than being based on empirical evaluation of what tags provide the most
useful linguistic information.
%assisting the PoS taggers and syntactic parsers, or other NLP applications, in
%determining the correct analysis.
Furthermore, PoS tag sets are generally taken for granted and used in an
unaltered form
%under a `one size fits all' assumption
instead of being optimized for the task at hand, syntactic parsing.

There are few off-the-shelf NLP tools for Norwegian today. Until now, the most
widely used PoS tagger for Norwegian has been the rule-based Oslo-Bergen
Tagger, which has served the Norwegian NLP community well over the years.
However, numerous aspects of the tagger call for the release of and transition
to new tools closer to the state-of-the-art. It is not open-source, nor is it
actively maintained, which severely impair its usability. Additionally, due to
its rule-based nature, it cannot learn from or be trained on annotated data. In
the pre-processing before PoS tagging, it performs tokenization and
lemmatization, both of which are useful for several NLP applications. However,
as it is not modular,
%it is not modular and in principle a `black box', i.e., we can only see its
%input and output without knowledge of its internal structure and workings.
we cannot isolate any of the individual components and run them separately on
running text without having to run the entire pipeline.

%Additionally, its tokenization differs from that used in
%the Norwegian Dependency Treebank, hence the Oslo-Bergen Tagger cannot be
%trained on nor reproduce the annotations of the treebank. As it is rule-based,
%and hence does not have any concept of training, it cannot be trained on the
%Norwegian Dependency Treebank; the rules are already predefined.

The newly developed Norwegian Dependency Treebank is the first freely available
treebank for Norwegian and contains manually annotated morphosyntactic analyses
of sentences in both official written standards of Norwegian, i.e., Bokmål and
Nynorsk. The treebank contains information about parts-of-speech and
morphological properties such as definiteness, gender and number, and may
therefore be used to train and evaluate PoS taggers for Norwegian. Furthermore,
the treebank is annotated with syntactic analyses represented in the dependency
grammar scheme, which allows for the training and evaluation of syntactic
dependency parsers for Norwegian. Due to its very recent development and the
lack of a standardized data set split of the treebank, it has not been
extensively used to train and evaluate PoS taggers and dependency parsers,
until now.

This thesis seeks to identify an optimized part-of-speech tag set for syntactic
parsing of Norwegian based on the Norwegian Dependency Treebank as well as
present a comparative analysis of the performance of a range of
state-of-the-art PoS taggers and syntactic dependency parsers when applied to
Norwegian. Through experiments with linguistically motivated tag set
modifications, we will examine how different granularities of categories, e.g.,
different sized tag sets, affect the results of both PoS tagging, and, more
importantly for this thesis, syntactic dependency parsing. This allows us to
assess the effects of various linguistic features on the tagging and parsing of
Norwegian and identify the most morphosyntactically informative linguistic
features in the treebank. As there is currently no readily available PoS tagger
model or syntactic parser model trained on the treebank, we will train and
evaluate a variety of state-of-the-art PoS taggers and dependency parsers on
the treebank to identify tools and provide accompanying models that can be
applied to running text in Norwegian.
%and result in a nearly full off-the-shelf pipeline, only remaining is
%tokenizer
The results of our work are concrete contributions to the Norwegian NLP
community: a data set split (training/development/test) of the Norwegian
Dependency Treebank, a tailored PoS tag set optimized for syntactic dependency
parsing, and clear recommendations regarding tools for PoS tagging and syntactic
parsing of Norwegian.


%It is therefore interesting to see whether we can modify the tag set using
%linguistic motivation in ways that improve the parser performance for
%Norwegian, also investigating what linguistic information serves as most
%useful by taking advantage of the morphological
%features assigned to tokens in the treebank, which provide information about
%properties such as definiteness, gender and number.

\section{Overview}
\paragraph{Chapter 2} provides a brief introduction to the Norwegian Dependency
Treebank and the tasks of PoS tagging and syntactic parsing together with their
respective approaches, exemplified by concrete tools.  It also provides an
overview of previous work on PoS tagging for Norwegian and Swedish as well as
the effects of tag set granularity on PoS tagging.

\paragraph{Chapter 3} describes the various parts-of-speech found in Norwegian
and their respective morphological properties followed by how these are
represented in the PoS tags and morphological features of the Norwegian
Dependency Treebank.  This is complemented by linguistic considerations for
designing and modifying PoS tag sets as well as a qualitative comparison of the
tag set of NDT and those of other comparable treebanks.

\paragraph{Chapter 4} outlines the experimental setup used in our experiments
with linguistically motivated tag set modifications, which includes a proposed
data set split of NDT, our implemented tag set mapping and the choice of PoS
tagger and syntactic parser for the development phase, and how the sum of these
components are conjoined in a pipeline under which the tag set experiments are
run.

\paragraph{Chapter 5} details our experiments with modifying the PoS tag set of
NDT using linguistic insights
%linguistically motivated tag set modifications
and assesses the results of these experiments. For the most
promising tag set modification for each category, we perform in-depth error
analysis of tagging and parsing performance. Finally, we present our optimized
tag set, obtained by combining the most promising tag set modification for each
category.

\paragraph{Chapter 6} presents a comparative analysis of a wide array of
state-of-the-art PoS taggers and dependency parses on the task of tagging and
parsing the Norwegian Dependency Treebank using our optimized tag set. The best
tagger and parser will then be employed on the held-out test data set using our
optimized tag set, the results of which will be compared to the results
obtained using the original tag set to contrast the tag sets and demonstrate
the effects of our optimization.

\paragraph{Chapter 7} serves as a summary and conclusion of this thesis, also
discussing future work related to this thesis.

\chapter{Background}
\label{chap:background}
To start things out, we briefly outline the newly developed Norwegian
Dependency Treebank and provide an introduction to PoS tagging and syntactic
parsing together with their respective approaches, exemplified through
state-of-the-art tools. With this established, we will look at previous work
related to this thesis, which includes PoS tagging for Norwegian and
investigating the effects of tag set granularity on the performance of PoS
taggers for English.

\section{The Norwegian Dependency Treebank}
The Norwegian Dependency Treebank (NDT) \cite{Sol:Skj:Ovr:14} is the first
publicly available treebank for Norwegian and serves as the basis for this
thesis. It was developed at the National Library of Norway in collaboration
with the University of Oslo, and contains manual syntactic and morphological
annotation. The treebank contains 311 000 tokens of Bokmål and 303 000 tokens
of Nynorsk, the morphological annotation of which follows that of the
Oslo-Bergen Tagger \cite{Hag:Joh:Nok:00, Sol:13}, which in turn is largely
based on the work of \citeA{Faa:Lie:Van:97}. The annotated texts are mostly
newspaper text, but also include government reports, parliament transcripts and
excerpts from blogs.  The annotation process of the treebank was supported by
the Oslo-Bergen Tagger, described in Section \ref{sssec:rulebased}, and then
manually corrected by annotators.

\section{Part-of-Speech Tagging}
Part-of-speech tagging is concerned with assigning a single part-of-speech to
each word in a sentence. Parts-of-speech, also known as PoS or word classes,
are used to group words with similar grammatical properties, e.g. verbs, nouns
and adjectives. The words in a particular PoS often function similarly in terms
of what they can occur nearby (i.e., syntactic properties) or the affixes they
can take (i.e., morphological properties). Furthermore, parts-of-speech can be
divided into two broad classes: closed class types, which have a relatively
fixed membership and generally do not get assigned new words (e.g., pronouns
and prepositions), and open class types, which have an open membership,
continuously being assigned new words (e.g., nouns and verbs).

Different granularities are used in different tag sets, and while one can use
as few as eight parts-of-speech for a very high-level approach, some systems
operate with several hundred tags. The more is not necessarily the merrier, as
while fine-grained tags provide more information, it becomes increasingly
difficult to determine the correct tag with each of them being so fine-grained
\cite[pp.~157--158]{Jur:Mar:09}.

To evaluate the performance of a PoS tagger, we compare its predictions on a
held-out test set with a so-called `gold standard', which is a manually
annotated version of the test set. If the tagger agrees with the human
annotator on the tag of a given word, the tagger has made a correct prediction.
We iterate the entire data set and compare the predictions of the tagger with
the human annotation in order to accumulate the number of correct predictions.
The typical evaluation metric for measuring tagger performance is
\emph{accuracy}, which is a simple fraction of the number of correctly tagged
tokens divided by the total number of tagged tokens.

Part-of-speech tagging plays an important role in many Natural Language
Processing (NLP) applications as a pre-processing step, i.e., it lays the
ground for further processing of the text on which the tagging was performed.
Many parsers, dependency parsers in particular, require the input text to be
PoS tagged. This is by no means a trivial task, even though it may seem fairly
straightforward; simply looking the word up in the dictionary is not
sufficient. The main challenge here, like in so much of computational
linguistics, is ambiguity, which is a pervasive phenomenon in languages. One
word can potentially belong to several parts-of-speech, and while most words in
English are unambiguous, many of the most common words are in fact ambiguous
\cite[p.~167]{Jur:Mar:09}.  When faced with ambiguity, a tagger has to be able
to disambiguate, i.e., resolve the ambiguity. This can be done in several ways,
e.g., by looking at the surrounding words in the sentence forming the context,
which can actually be very informative of the word and provide useful
information as to what tag should be assigned to the word in question. For
instance, if the preceding word is a determiner, chances are that the word in
question is a common noun.
%Next, I will look at different approaches employed to perform the tagging and
%resolve the ambiguities.

\subsection{Approaches to PoS Tagging}
\label{ssec:taggingapproaches}
PoS tagging is performed by PoS taggers, which can generally be divided into
two groups: rule-based and data-driven taggers. Where rule-based taggers rely
on handwritten rules in order to assign each word the correct tag, the
data-driven (also known as stochastic) taggers determine the correct tag based
on gathered statistics.  While rule-based taggers can be directly applied to
new text, the data-driven taggers need to be trained first in order to gather
the statistics used in the tagging. One could argue that rule-based taggers are
(indirectly) data-driven, as it is highly likely that existing rules are
altered and/or new rules are added as more data is processed. However, these
changes are carried out by trained linguists, as opposed to the machines used
in data-driven approaches, which is based on the method of machine learning. In
the following sections, I will go into detail about these two approaches,
discussing their respective merits and how they differ as well as provide
examples of PoS taggers employing them.

\subsubsection{Data-driven PoS Taggers}
\label{sssec:datadriven}
Data-driven taggers are taggers that are trained on annotated data, with this
training consisting of processing and analyzing the input data to recognize and
generalize patterns that are used to find the most likely tag for a given word
in a given context. This is done by applying machine learning techniques, which
can learn from and subsequently make predictions on data. The trained models
and the gathered statistics are then used in the tagging of new data. The
training data sets are typically manually annotated corpora, i.e., large text
collections in which the words are labeled with part-of-speech tags by human
annotators. The most widely used corpus for English is the Penn Treebank
\cite{Mar:San:Mar:93}, which consists of 4.5 million words gathered from
sources including the Wall Street Journal, IBM computer manuals and texts from
the Library of America.

\paragraph{TnT}
One of the most well-known data-driven PoS taggers is
TnT\footnote{\url{http://www.coli.uni-saarland.de/~thorsten/tnt/}}
\cite{Bra:00}, short for Trigram'n'Tags, which uses Hidden Markov Models (HMMs)
combined with a smoothing technique and handling of unknown words. HMM-based
taggers employ two kinds of probabilities to determine the correct tag:
emission probabilities, which express the probability of a tag emitting a
certain word, i.e., the probability of seeing a particular word given a
particular tag, and transition probabilities, which express the probability of
a particular tag given the preceding tag(s). As is standard in HMMs, the
emission probabilities and transition probabilities are estimated using Maximum
Likelihood Estimation (MLE), which is based on relative frequencies encountered
in the training.  After this initial probability estimation, TnT smooths
contextual and lexical frequencies and implements support for handling unknown
words, which I will return to shortly.

Smoothing is a technique used to try to the solve the problem of sparse data,
which is caused by the fact that the data in any corpus will be sparse in the
sense that it can not include all acceptable, grammatical sentences or
sequences of tags.  The probability of a sequence containing an unknown word
will be 0 if smoothing is not applied, as the probabilities are estimated using
maximum likelihood estimation, and the maximum likelihood of a word that has
not been seen will be 0, even if it is a perfectly fine word in a perfectly
fine sentence \cite[pp.~131--132]{Jur:Mar:09}. Smoothing resolves this by
reserving some of the probability mass to unseen words. A number of smoothing
techniques can be employed to achieve this, but \citeA{Bra:00} argues that the
smoothing paradigm that yields the best results in TnT is linear interpolation
of unigrams, bigrams and trigrams.

The handling of unknown words in TnT is carried out by using suffix analysis.
Tag probabilities are set according to the word's ending, as \citeA{Bra:00}
argues that the suffix is a strong predictor for PoS classes. The probability
distribution for a particular suffix is obtained by looking at all words in the
training set that share the same suffix. Furthermore, \citeA{Bra:00} argues
that using suffixes of infrequent words is a better approximation for unknown
words than using suffixes of frequent words, and the suffix handling procedure
is thus restricted to words appearing less than 10 times.

\paragraph{HunPos}
HunPos\footnote{\url{https://code.google.com/archive/p/hunpos/}}
\cite{Hal:Kor:Ora:07} is an open-source PoS tagger based on TnT. Its main
feature, departing slightly from TnT, is augmenting the parameters of the
traditional emission probability estimation, additionally taking into account
the preceding tag, thus incorporating more context into the standard HMM model.
The authors of HunPos argue that this can lead to as much as 10\% reduction in
the error rate. HunPos reaches an accuracy of 96.58\% on the Wall Street
Journal data of the Penn Treebank \cite{Mar:San:Mar:93}, while the original TnT
obtains an accuracy of 96.46\%. \citeA{Hal:Kor:Ora:07} argue that the increase
in the emission order clearly improves the performance of the tagger for
English.

\paragraph{Stanford}
The Stanford
tagger\footnote{\url{http://nlp.stanford.edu/software/tagger.shtml}}
\cite{Tou:Kle:Man:03} is based on maximum entropy, also known as multiclass
logistic regression, which generalizes logistic regression to multiclass
problems. Their novel approach combines this with cyclic (bidirectional)
dependency networks, where the probability of a given node (word or tag) is
conditioned on both the preceding and the following word(s) or tag(s), yielding
bidirectional inference. \citeA{Tou:Kle:Man:03} make extended use of lexical
features focusing on surrounding words, which they found to be very useful, and
not incorporated into previous models. They report an accuracy of 97.24\% on
the Wall Street Journal data of the Penn Treebank, which at the time of
publication was the best automatically learned PoS tagging result.

\paragraph{SVMTool}
SVMTool\footnote{\url{http://www.cs.upc.edu/~nlp/SVMTool/}} \cite{Gim:Mar:04}
is a data-driven PoS tagger that employs Support
Vector Machines (SVMs) to perform the tagging. An SVM model is geometric rather
than probabilistic, and represents the objects (corresponding to words in the
case of PoS tagging) as points in a vector space. It is a max-margin
classifier, meaning that it aims to make the decision boundary separating the
classes as wide as possible. SVMTool is highly flexible and tunable, and offers
a rich feature set which can be modified to fit different needs. These features
include word features and PoS features for the three preceding and following
words and tags, respectively, as well as both preceding and following bigrams
and trigrams. They also take into account affixes and orthographic features,
such as capitalization, hyphenation and similar information related to the word
form. The tagger reached an accuracy of 97.16\% on the Wall Street Journal data
of the Penn Treebank.

\subsubsection{Rule-based PoS Taggers}
\label{sssec:rulebased}
In contrast to data-driven taggers, rule-based taggers do not use
probabilities to determine the correct tag, but instead have a large set of
handwritten disambiguation rules for all the possible ambiguities they might
encounter. If a rule-based tagger encounters an ambiguity to which there is no
applicable rule, it is simply unable to perform the disambiguation. The tagging
is performed in two stages: first, the word in question is looked up in the
dictionary in order to obtain a list of potential parts-of-speech. This list is
then (hopefully) narrowed down to a single part-of-speech by applying the
relevant disambiguation rule(s) \cite[pp.~170--172]{Jur:Mar:09}.

\paragraph{Oslo-Bergen Tagger}
\label{OBT}
The Oslo-Bergen
Tagger\footnote{\url{http://www.tekstlab.uio.no/obt-ny/english/}}
\cite{Hag:Joh:Nok:00}, henceforth OBT, is an example of a rule-based tagger. It
was developed for Norwegian and uses a formalism called Constraint Grammar
(CG). Taggers employing the CG approach applies a large set of linguist-written
constraints to the input sentence to rule out incorrect parts-of-speech, with
each rule either adding, removing, selecting or replacing a tag or a set of a
tags.

The authors behind OBT chose to develop a large number of detailed
constraints for each possible correct reading. When facing ambiguous readings,
it finds all possible readings for each word, and then applies the appropriate
constraint in order to disambiguate and find the correct reading. However, the
constraints do not necessarily produce one single reading per token. As the OBT
only concentrates on grammatical ambiguity, and does not look at ambiguity
between lemmas, it may output several readings, thus not fully
disambiguating. This, combined with the fact that continued rule writing gave
diminishing returns, motivated the addition of a statistical module.

This resulted in OBT+Stat \cite{Joh:Hag:Nok:11}, a statistical module that
resolves all ambiguities left in the OBT output, including the grammatical
ambiguities and the lemma ambiguities originally left on purpose as well as the
unfortunate ambiguities. This statistical module uses the aforementioned
HunPos tagger to perform the disambiguation, which is run independently of the
CG-based disambiguation when the CG tagger leaves more than one reading per
token, in which case the results from the two taggers are unified. If the two
taggers agree on a reading, that reading is selected.  Otherwise, an attempt is
made to disambiguate using the lemma. This lemma disambiguation scheme works by
using a word frequency list derived from NoWaC \cite{Gue:10}, a corpus of
Norwegian documents published on the Internet. It selects the lemma with the
highest frequency in the corpus, based on the assumption that most lemmas will
appear as words in a large corpus as Norwegian lemmas correspond to uninflected
word forms. However, if the lemma disambiguation fails, an arbitrary reading
out of the possible readings is selected.

After the CG-driven disambiguation had been performed, 8.6\% ambiguous tokens
were still present, which were all resolved by  the statistical module. The
OBT+Stat tagger achieved an overall accuracy of 96.56\%, which
\citeA{Joh:Hag:Nok:11} consider to be a good result, considering that they
removed all remaining ambiguity and at the same time kept a large, detailed tag
set and disambiguated lemmas with identical tags.

\section{Syntactic Parsing}
Syntactic parsing is concerned with analyzing sentence structure, one aspect of
which is assigning syntactic functions to words in a sentence, e.g., finding
the subject and object in a sentence.  Syntactic parsers typically require the
input sentence to be labeled with parts-of-speech, as these provide important
linguistic information about the words in a sentence and are often used as
features in the parser. Just as for PoS tagging, ambiguity is the main obstacle
in parsing. Most sentences have more than one possible structure, hence the
parser must be able to determine the correct analysis out of a potentially very
large set of possible analyses. Syntactic parsing serves as an important step
in a range of NLP tasks where the structural analysis of sentences is crucial.

There exists a wealth of linguistic theories to represent syntactic structures,
but as Norwegian Dependency Treebank is based on dependency grammar, we will
focus on dependency grammar.

\subsection{Dependency Grammar}
Dependency grammar assumes a binary, asymmetrical relation between words in a
sentence, so-called \emph{dependencies}, where each node has a single head, of
which it is the dependent. These dependencies are represented as arcs from a
head to its dependent, where the label on the arc denotes the name of the
dependency relation, e.g., \textsc{subj} (subject) or \textsc{adv} (adverbial).
The head is regarded as the superior node in the relation in that it is more
prominent and in some way governs its dependent, `allowing' for its presence
\cite{Niv:05}.

The syntactic annotation choices in NDT are largely based on the
Norwegian Reference Grammar \cite{Faa:Lie:Van:97}. The annotation choices are
outlined in Table \ref{ndtannotation}, taken from \citeA{Sol:Skj:Ovr:14},
providing overview of the analyses of syntactic constructions that often
distinguish dependency treebanks, such as coordination and the treatment of
auxiliary and main verbs. See an example of a dependency graph taken from NDT
in Figure \ref{exndt}.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Head} & \textbf{Dependent} \\
        \midrule
        Preposition & Prepositional complement \\
        Finite verb & Complementizer \\
        First conjunct & Subsequent conjuncts \\
        Finite auxiliary & Lexical/main verb \\
        Noun & Determiner \\
        \bottomrule
    \end{tabular}
    \caption{Annotation choices in NDT, taken from
        \protect\citeA{Sol:Skj:Ovr:14}.}
    \label{ndtannotation}
\end{table}

\begin{figure}
    \smaller[0.5]
    \begin{dependency}[theme = simple]
        \begin{deptext}[column sep=1em]
            Det \& er \& et \& resultat \& av \& President \& Bushs \& kamp \&
            mot\& terrorisme \& . \\
        \end{deptext}
        \deproot{2}{\larger{FINV}}
        \depedge{2}{1}{\larger{SUBJ}}
        \depedge[arc angle=40]{4}{3}{\larger{DET}}
        \depedge{2}{4}{\larger{SPRED}}
        \depedge{4}{5}{\larger{ATR}}
        \depedge{7}{6}{\larger{APP}}
        \depedge{8}{7}{\larger{DET}}
        \depedge{5}{8}{\larger{PUTFYLL}}
        \depedge{8}{9}{\larger{ATR}}
        \depedge{9}{10}{\larger{PUTFYLL}}
        \depedge{2}{11}{\larger{IP}}
    \end{dependency}
    \caption{Example of a dependency graph representation of a sentence taken
        from NDT.}
    \label{exndt}
\end{figure}

\subsection{Data-Driven Dependency Parsing}
\label{ssec:depparsers}
Dependency parsing is the process of parsing a sentence and assigning it
syntactic structure represented in terms of dependency grammar. Similarly to
the approaches to PoS tagging, the most successful dependency parsers today
employ statistical modeling and machine learning, resulting in data-driven
dependency parsers. Given an input sentence, we want to derive the correct,
i.e., the most probable, dependency graph out of a set of candidate graphs.

Data-driven dependency parsing can be broadly divided into two main approaches:
graph-based models and transition-based models \cite{McD:Niv:11}. Parsers
employing the \emph{graph-based} approach parameterize models over dependency
subgraphs and learn these parameters to score an entire dependency graph for a
given sentence in order to find and locate the highest-scoring, optimal parse
from a set of candidate parses. They are hence globally trained and often
employ exhaustive search to find and locate the most probable parse for a given
sentence. \emph{Transition-based} approaches, on the other hand, parameterize
models over transitions between states in an abstract state machine and use
local training and greedy search to find the best transition (the locally
optimal choice) from any given state.

Data-driven dependency parsing has seen a surge in recent years, with several
shared tasks dedicated to advance the state of the art of dependency parsers.
We will now turn to four dependency parsers which will be trained and evaluated
on NDT in later experiments in Chapter \ref{chap:optpipeline}.

\paragraph{Malt}
MaltParser\footnote{\url{http://maltparser.org/}} \cite{Niv:Hal:Nil:07} is a
transition-based dependency parser based on a deterministic parsing strategy
along with treebank-induced classifiers for predicting parser actions, i.e.,
transitions. It uses history-based feature models, which involves using
previous transitions as a means to guide the parsing. It comes with a variety
of parsing algorithms, feature models and machine learners to choose from, the
optimal of which can be found by employing MaltOptimizer \cite{Bal:Niv:12},
used to optimize the parameters of the parser.

\paragraph{Mate}
The Mate parser\footnote{\url{https://code.google.com/archive/p/mate-tools/}}
\cite{Boh:10} is graph-based and uses maximum spanning trees and third-order
features to find the best dependency parse. The maximum spanning tree of a
sentence is the highest scoring tree spanning the whole sentence, where the
score of a dependency tree is the sum of the scores of its dependencies. Third
order features involve the parser being able to evaluate substructures
containing three dependencies. It has been shown to perform very well for
English \cite{Cho:Tet:Ste:15}, which \citeA{Boh:10} attributes mainly to the
use of the Hash Kernel.

\paragraph{RBG}
The RBG parser\footnote{\url{https://github.com/taolei87/RBGParser}}
\cite{Lei:Xin:Zha:14} is a graph-based dependency parser using tensor
decomposition and a low-rank factorization method to map high-dimensional
feature vectors to low-dimensional representations, as described by
\cite{Lei:Xin:Zha:14}. It can be run with either first-order or third-order
features, where the first-order feature model is the fastest but least
accurate.

\paragraph{Turbo}
The Turbo parser\footnote{\url{http://www.cs.cmu.edu/~ark/TurboParser/}}
\cite{Mar:Alm:Smi:13} is graph-based and employs dual decomposition and
specialized head automata to handle third-order features in producing
non-projective dependency graphs, i.e., graphs with crossing arcs.  Dual
decomposition splits the original problem into local subproblems and seek an
agreement on the overlapping variables.

\section{Previous Work}
\subsection{PoS Tagging for Norwegian}
\citeA{Mar:14} developed an open-source part-of-speech tagger for Norwegian
based on an existing processing library, motivated by the scarcity of PoS
taggers for Norwegian, with the author citing the Oslo-Bergen Tagger
\cite{Hag:Joh:Nok:00} as the only available tool for this purpose. The tagger
is mostly based on statistics and makes use of a simple and standard tag set,
hence clearly departs from the rule-based and tag-comprehensive approach seen
in OBT.

\paragraph{Methodology and Data}
The part-of-speech tagger was created using the FreeLing tool, an open-source
text processing tool offering a number of language analysis services, with its
part-of-speech tagger being the focus of this paper. The tagger uses an
adaptation of \emph{Norsk ordbank} as its dictionary, which is a large database
of lexical units with morphosyntactic and argument structure information. It
was trained and evaluated on a pre-release version of NDT using the
FreeLing-included HMM tagger, which is a trigram tagger based on TnT. The tag
set in both the dictionary and the gold standard corpus was simplified and
standardized, with the simplification consisting of combining certain original
tags into a single tag and omitting some of the arguably less important
grammatical information.
%For instance, the (original) tag \emph{subst nøyt
    %appell ent be normert} indicates that the word is a \emph{substantiv}
%`noun', \emph{nøytrum} `neuter', \emph{appellativ} `common (noun)',
%\emph{entall} `singular' and \emph{bestemt} `definite'. \emph{Normert} is
%Norwegian for `normalized', indicating that the word shows a normalized
%spelling. This  was mapped to 'NCNS000D' in
The adapted tag set is mostly based on the EAGLES standard, where the first
letter of each tag indicates the part-of-speech of the word and the remaining
letters specify more fine-grained morphosyntactic and semantic information.

FreeLing analyses forms not found in the dictionary, i.e., unknown words, by
use of a compound module and an affixation module that checks whether the word
is a compound or a derived form, respectively. The compound module works by
simply concatenating words found in the dictionary to check whether the word in
question is a compound of existing words, and the affixation module detects
whether a word is a derived form, based on a list of affixes applied to words
in the dictionary. Additional modules customized for the purpose of this tagger
include the tokenizer and the multiword expression module.

\paragraph{Error Analysis}
%Briefly reviewing the error analysis of this tagger by \citeA{Mar:14} is
%interesting due to the fact that this tagger is similar to what I set out to do
%with my thesis. By looking at the errors made by this tagger, I can hopefully
%avoid these in my tagger.
Most errors seen in the tagging were due to
ambiguities in the dictionary. These include ambiguities in morphological
features, such as nominative versus accusative case ambiguity in pronouns,
number ambiguity in nouns and gender ambiguity in adjectives. Words not
included in the dictionary, such as  many proper nouns, acronyms and compound
words, also constitute a large portion of the errors.

\paragraph{Evaluation}
The accuracy of the tagger was evaluated using five-fold cross validation over
the gold standard . Three types of morphological information with
increasing degree of detail were evaluated: (i) main part-of-speech (PoS-1);
(ii) main part-of-speech and information about the subtype of the
part-of-speech, e.g., auxiliary versus main verb (PoS-2); and (iii) detailed
morphosyntactic information given by the tag, e.g., gender, number and case
(PoS-3). The tagger's performance was evaluated as quite close to that of
state-of-the-art taggers for English (between 96\% and 98\%) reaching an
accuracy of 97.3\%, 96.2\% and 92.4\% for PoS-1, PoS-2 and PoS-3, respectively.
It yielded a higher accuracy in morphosyntactic tagging than the OBT+Stat
tagger, but as pointed out by \citeA{Mar:14}, this is not necessarily a valid
comparison, considering that the two taggers use different tag sets and data
sets. It is worth noting that the performance of taggers are not necessarily
directly comparable; unless the taggers are trained  and tested on the same
premises and resources, a direct comparison is rather futile. Moreover, even if
one were to evaluate them in a near-identical manner, the results wouldn't
necessarily be all that revealing, as a tagger developed and trained for
tagging a specific type of text performing poorly at tagging a completely
different type of text does not provide useful information about the tagger. We
want to find out how the tagger performs on the data to which we want to apply
it. With this in mind, I will turn to a paper investigating how taggers can be
evaluated and compared in a systematic manner.

\subsection{Comparing PoS Taggers for Swedish}
\citeA{Meg:01} argues that the manner in which PoS taggers are evaluated by
researchers differ greatly, making it difficult to compare the performance of
the systems. Motivated by this, \citeauthor{Meg:01} set out to compare four
data-driven learning algorithms on the task of PoS tagging of
Swedish, and to investigate what types of errors they made as well as the
effects of tag set size and training set size. The four algorithms in question
are Hidden Markov Model (HMM), Maximum Entropy (MaxEnt or ME), Memory-Based
Learning (MBL) and Transformation-Based Learning (TBL). These taggers had all
previously been tested for English with an average accuracy between 95\% and
97\%, and were used with their default settings as the main goal was to
evaluate the systems out-of-the-box. The experiments were run on the second
version of the Stockholm-Umeå Corpus (SUC) \cite{Gus:Har:06}, which was
annotated with a Swedish version of PAROLE tags, with the tag set totaling 139
tags. The corpus was randomly divided into ten roughly equal parts to get
subsets containing different parts.

\paragraph{Taggers}
Memory-based learning is a case-based approach where new items are classified
based on their similarities to the items already stored in memory during
learning \cite{Dae:Zav:Ber:96}. The maximum-entropy framework called MXPOST is
a probabilistic classification-based approach based on a maximum-entropy model
where contextual information is represented as binary features that are used
simultaneously in order to predict the PoS tag. It uses beam search to find the
most probable sequence of tags, and then chooses the tag sequence with the
highest probability \cite{Rat:96}. Transformation-based learning is a
rule-based approach that learns by detecting errors, and generates new rules
based on the observed errors and the subsequent resolution of these errors
\cite{Bri:94}. TnT is used as presented in section \ref{sssec:datadriven}.

The evaluated taggers have in common their approach to representing data, which
is to generate a feature vector for each word in the corpus, consisting of the
words they appear with and their respective tags, i.e., the context. However,
these taggers differ with respect to what features they store in the vectors.
MBL takes into account the current word, the preceding and following word
forms, the two preceding tags and the immediately following tag; ME includes
the current word, the following and preceding two words and the preceding two
tags; TBL uses a context of three preceding and following words and/or tags;
and finally, TnT is trigram-based and works as previously described.

\paragraph{Evaluation}
PoS taggers can be evaluated in a number of ways, and \citeA{Meg:01} decided to
evaluate the taggers from three different aspects to see how they compare: (i)
the accuracy of each classifier was evaluated by using the entire tag set and
10\% of the SUC as training data; (ii) the original tag set was divided into
smaller subsets of varying size to investigate the effects of tag set size (See
Section \ref{ssec:tagsets}); (iii) the size of the training corpus was varied
from one thousand up to one million tokens for each tagger to see how the
various sizes affect the error rates.

To ensure a fair comparison, each tagger was trained on the same portion of SUC
for each of the experiments, and the resulting classifiers were then evaluated
on the same separate test set. The taggers were only allowed to assign a single
tag (i.e., not a set of possible tags) to each token in the test data set.

TnT had the highest overall accuracy of 93.55\%, and also succeeds best in the
annotation of known and unknown words, with the MXPOST tagger coming in second
at 91.20\%. The memory-based learner (89.28\%) performs slightly better than
the transformation-based learner (89.06\%), which performs poorly at tagging
unknown words, but manages to disambiguate known words quite well. The
different taggers make different types of errors; whereas TBL and MB more often
make mistakes in the morphological analysis of categories, TnT and ME more
frequently confuse PoS categories for ambiguous words.

Each tagger was trained ten times on the same data of various sizes, including
1 000, 10 000, 100 000 and 1 millions tokens. The corresponding test set was
subsequently annotated by each of the resulting classifiers. The error rate
decreases as the size of the training data increases, unsurprisingly, as the
ratio of unknown words to known words is much lower in larger data sets. The
impact of the size of the training data on the taggers differ greatly; whereas
ME was highly sensitive to the data set and shows a great decrease in error
rate (88\%), TBL was less sensitive, and the error rate decreased by 50\% when
going from 5 000 to 500 000 tokens. TnT outperforms the other taggers on the
task of annotating unknown words when the training set consists of 20 000 or more
tokens, and when there are less than 20 000 tokens, TBL has the lowest error
rate. TnT consistently outperforms all the other taggers in annotating known
words, showing an error rate of only 3.5\% when the training set consists of
one millions tokens. It is also worth taking into consideration the run times
of the different taggers. Whereas TnT can learn from 100 000 tokens in one
second, and manages to tag a text containing the same amount of data in three
seconds, ME and TBL spends roughly a day training on 100 000 tokens.

\paragraph{HunPos}
In a round of follow-up experiments, \citeA{Meg:09} evaluated the
aforementioned HunPos tagger on the task of PoS tagging for Swedish. The tagger
was trained on SUC, and in a similar fashion to the experiments carried out in
\citeA{Meg:01}, trained on different subsets of the corpus, varying in size
from 1000 tokens to 1 million tokens. The separate test set contained 117 685
tokens in 7 464 sentences. The annotation was done using the PAROLE tag set,
which at the time of writing contained 156 tags. Various feature settings were
used in training the tagger to investigate what settings serve as the most
appropriate for Swedish, among them the order of tag transitions, i.e., the
number of preceding tags included in the estimation of transition
probabilities, using either bigram tagging or the default trigram tagging.
Different estimations of emission probabilities were also tested, where the
probability of a word is conditioned only on the tag of the word itself or both
the current tag and the immediately preceding tag, with the former being used
in TnT and the latter being the default in HunPos. \citeA{Meg:09} found that
bigram models perform better and are more appropriate when the training data
contains less than 50 000 tokens, while trigram models are to be preferred when
the training data contains more than 50 000 tokens. Furthermore, the size of
the suffixes was varied, experimenting with setting the size to 5 and 9 in
addition to the default value of 10 to investigate whether a decrease in suffix
length can increase performance. \citeA{Meg:09} argues that the results prove
this not to be the case, and that the larger suffix window is always to be
preferred for Swedish. She finally experimented with varying the maximum
frequency with which a word can occur in order to be added to the suffix tree
and used in the suffix analysis, either 5, 9 or the default value of 10. For
training data containing less than 100 000 tokens, reducing the frequency
requirement for words to be added to the suffix tree can lead to an improvement
in tagger performance. For larger training data sets, however, the experiments
by \citeA{Meg:09} suggest that the default value of 10 can be used, as there
are no significant improvements in decreasing the frequency requirement.

HunPos was then compared to the taggers evaluated in \citeA{Meg:01}, namely
MBT, MXPOST, TBL and TnT. These were all run with default settings, while
HunPos was tested both with default settings and optimal settings, where the
optimization involved choosing the feature settings that led to the highest
accuracy. Consistent with the results presented in \citeA{Meg:01}, the
HMM-based taggers performed best, with HunPos and TnT performing closely to one
another on all training data sizes. HunPos had the highest accuracy when the
training data contained less than 20 000 tokens, while TnT performed best on
the remaining data sets, except for when the training data set consisted of 1
million tokens, where HunPos was marginally better than its predecessor,
showing an accuracy of 95.90\% compared to TnT's 95.89\%.

\subsection{The Effects of Tag Sets}
\label{ssec:tagsets}
Part-of-speech tag sets are often taken for granted, and many NLP applications
for English use the tag set supplied by the de facto standard Penn Treebank
\cite{Mar:San:Mar:93}, totaling 45 tags, in an unaltered form. It is therefore
interesting to investigate whether more fine-grained tag sets can lead to
improvements in tagger performance, as they certainly increase the linguistic
utility, emitting more linguistic information than their more coarse-grained
counterparts. It is known, and rather apparent, that more complex tag sets may
complicate the tagging and lead to drops in tagger accuracy, as the likelihood
of ambiguity is higher.
%I will therefore next turn to two papers investigating how tag sets of
%different granularities affect the performance of taggers.

\paragraph{Mapping Fine-Grained Tag Sets to Coarse-Grained Tag Sets} In
addition to the complete tag set of 139 tags in the Stockholm-Umeå Corpus
\cite{Gus:Har:06}, \citeA{Meg:01} trained the taggers on four subsets of these
tags, consisting of 26, 39, 44 and 48 tags, respectively. In \citeA{Meg:02},
the tag sets are slightly altered, consisting of 25, 39, 43 and 48 tags,
respectively. These experiments were carried out because a tag set with
complete morphological tags is not necessarily needed for all NLP applications
\cite{Meg:02}. For applications where a complete fine-grained tag set is
excessive, one can instead reduce the size of the tag set, which in turn leads
to a decrease in error rate as the tagger has an easier task determining the
correct tag. A subset of tags with some more general properties, perhaps only
the main PoS tags, may suffice. Due to this, \citeA{Meg:02} chose to map the
original tag set into smaller tag sets in the training data to investigate how
this affects the performance, with the goal being to develop tag sets that can
be useful for various applications. The smaller subsets differ with respect to
the type of morphological features included within each PoS category. The
smallest subset totaling 26 tags consists of the main PoS tags as well as some
subcategorization information, while the larger subsets of 39, 44 and 48,
respectively, are extensions of this. For the tag set containing 39 tags, the
extension involved including distinctions between present and perfect
participle and between verbs in various tenses. This tag set was then used as
basis for further distinctions for the tag sets of 44 and 48 tags, leading to
increasingly fine-grained tag sets.

The number of errors made by each tagger is higher when using a large tag set,
which is not surprising, considering that it is more challenging to determine
the correct tag when the set of possible tags is larger. TnT consistently
performs best, again with ME coming in second, MB third and TBL last. TnT and
MB seem to be less sensitive to the size of the tag set than ME and TBL, as the
changes in error rate are smaller. \citeA{Meg:02} experimented with using tag
sets of different sizes in training and testing, using the complete tag set of
139 tags in the training and the smallest subset of 25 tags in the testing.
This lead to a decrease in error rate for TnT and MB, while causing more errors
for ME and TBL, compared to when both training and testing were done on the
smallest tag set.

\paragraph{Mapping Coarse-Grained Tag Sets to Fine-Grained Tag Sets}
While \citeA{Meg:01} mapped the original tag set into smaller subsets,
\citeA{Mac:05} mapped a coarse-grained tag set into more fine-grained subsets,
thus augmenting the tag set. The approaches do, however, share a common goal,
which is to investigate the effects of tag sets on tagger performance.
\citeauthor{Mac:05} used the Penn Treebank and its accompanying tag set,
totaling 45 tags, as basis for the experiments, from which new, more
fine-grained tag sets were mapped using the Natural Language Toolkit (NLTK) in
Python. The algorithms used for carrying out the experiments were
Transformation-Based Learning (TBL), Support Vector Machines (SVM) and Maximum
Entropy (MaxEnt or ME).

The primary goal of \citeA{Mac:05} was to improve tagger performance using
linguistic insight. The tags were subdivided in a linguistically sensible way
into finer-grained tag sets, which yield additional information that may be
used to assist the tagger in making syntactic generalisations which are not
apparent either from the coarse PoS tags or from the sparsely populated lexical
feature vector. Finer-grained tag sets increase its linguistic utility as more
linguistic information is present. However, complex tag sets may in turn lead
to increased difficulty in determining the correct tag due to a potentially
increased number of possible tags for a word, between which the tagger has to
disambiguate.

\citeA{Mac:05} attempted to resolve the problem of complex tag sets leading to an
increase in error rate by mapping the new tags back to the original tags
before evaluation, hence using the mapped tags only internally as a means to
increase the tagger performance, not to be used in the final evaluation. The
modifications adhered to certain restrictions for this inverse mapping to
be carried out sufficiently: there can be no discrepancies between the original
and the inversely mapped tags, i.e., the original tags had to be unambiguously
recoverable from the new tags.

\subparagraph{Motivation}
It may seem rather pointless to strive for a small performance improvement,
e.g., increasing the accuracy from 97.01\% to 97.05\%. Sentence-level accuracy,
however, increases much more radically even when there is only a small increase
in token-level accuracy. \citeA{Mac:05} argues that a tagger which yields 97\%
accuracy for tokens, achieves 49\% at the sentence level, while a tagger
performing at 98\% yields an accuracy of 62\% for entire sentences. This
indicates that it certainly is worth striving for a slight increase in word
token accuracy, as it leads to a substantially higher performance over
sentences.

\subparagraph{Experimental Evaluation}
The linguistically motivated modifications, i.e., tag mappings, were
conditioned on both lexical features and syntactic features. It is interesting
to investigate modifications of both types as they clearly differ in how they
use linguistic insight.

The syntactically conditioned modifications involved looking at the head of the
phrase or the preceding or following words to determine the correct tag. One of
these modifications involved introducing a new, separate class for subordinate
conjunctions, which are originally grouped with prepositions in the \emph{IN}
class, as well as mapping the prepositional uses of \emph{to} into \emph{IN},
while being left as \emph{TO} when used as an infinitive marker. Other
modifications include attempting to resolve the difficulty of distinguishing
adjectives from verb participles, as they can be hard or even impossible even
for humans to disambiguate when extended knowledge of the context is not
provided. However, \citeA{Mac:05} argues that the word in question can
unambiguously be classified as an adjective when paired with a degree adverb,
as verb participles cannot be modified by an adverb.

The tag set modifications conditioned on lexical features include attempts to
resolve the ambiguity between \emph{IN} and \emph{RP}, as prepositions function
as predicate complement in verb particles. There are words in the \emph{IN}
class that do not have a corresponding homograph in \emph{RP}, meaning that
they cannot be the predicate complement in a verb particle, hence can be
unambiguously tagged as \emph{IN}. They therefore mapped the ambiguous members
of \emph{IN} to a separate class as a pre-processing step to distinguish them
from those which are unambiguous. Other lexically conditioned modifications
include mapping determiners and articles into classes based on how they
indicate the number of the noun phrase they precede, i.e., if the noun is
singular (e.g., \emph{a}) or plural (e.g., \emph{some}), or if the number
simply cannot be inferred from the determiner (e.g., \emph{the}).

In addition to the primary approach, \citeA{Mac:05} pursued an alternative,
more data-driven approach by using machine learning to determine clusters
within the tag sets corresponding to patterns of syntactic regularities. A
range of syntactic and collocational features were introduced to assist in
determining these patterns. The features for each <\emph{word type,tag}> pair are
derived from syntactic and collocational patterns seen in the training. For
instance, one feature was \emph{Par\_is\_VBP}, which simply states the
probability of the parent of the word being \emph{VBP}. The values of these
features are probability distributions indicating their relative frequency. The
features and their values were then used to generate clusters, which assist the
tagger in assigning words the correct tag.

The results of these experiments were not of the promising kind, as the
introduction of new classes and more fine-grained tag sets rarely led to an
improvement in performance, in most cases in fact having a negative impact on
the accuracy. Even in the cases where they saw an improvement in performance,
the improvements were for the most part marginal at best. The most promising
modifications were generally those resulting from the clustering approach
rather than the linguistic modifications. They argue that it seems like the
linguistic modifications are less data-dependent while the data-driven
modifications have a slight tendency to overfit. The modifications which had no
impact on the performance, i.e., that neither led to an increase nor a decrease
in accuracy, may still be used in other NLP applications where the increased
linguistic utility by itself is useful.

\chapter{Parts-of-Speech \& Tag Sets}
\label{chap:posandtagsets}
 This chapter provides an overview of the parts-of-speech in Norwegian and the
tag set of the Norwegian Dependency Treebank, complemented by linguistic and
computational considerations for designing and modifying tag sets.  We will
discuss how the tag set of the Norwegian Dependency Treebank is designed with
regard to parts-of-speech in Norwegian and present a qualitative comparison of
the tag set of NDT and tag sets of other comparable treebanks to see how they
relate.

\section{Considerations for Designing Tag Sets}
We are looking to optimize the existing tag set of NDT for dependency parsing
of Norwegian, thus identify a tag set that yields high parser performance while
also providing us with useful linguistic information. As tagging is an
important preprocessing step for parsing, the performance of the tagger will
likely have great impact on the parser performance.

The granularity of a tag set is directly related to its size; a coarse-grained
tag set consists of a relatively small number of tags, and as tag sets increase
in size, the more fine-grained they become. Smaller, more coarse-grained tag
sets tend to lead to higher tagger accuracy because it is easier to infer the
correct tag when the number of possible choices is smaller. Indeed, simply
opting for a tag set consisting of a single tag would lead to 100\% accuracy.
This would, however, be entirely useless for NLP tasks, as we want to find a
tag set that also models the relevant morphological and syntactic information.

\citeA{Lee:97} noted the conflict between linguistic and computational
considerations one often encounters when designing tag sets. The linguistic
quality of a tag set is determined by the extent to which it represents all
important grammatical information in the language, while the computational
tractability of a tag set is determined by how easy it is to infer the correct
tag for a particular word and how useful a particular tag is in aiding the
disambiguation process \cite{Lee:97}.

We are looking for tag sets that are of high linguistic quality and are highly
computationally tractable. Some more fine-grained distinctions may actually
improve the tagger performance, as the extra information available may assist
the tagger in disambiguating ambiguous and unknown words. However, tag sets of
very high linguistic quality are likely to lead to diminishing returns and
compromise the computational tractability, as the tagger is unable to properly
learn very fine-grained distinctions for which there is not sufficient training
data. Hence, there is often a trade-off between the two aspects, and we want
to find the `golden mean' between coarse-grained and fine-grained tag sets.

The best tagging does not necessarily lead to the best parse, and a tag set
that is optimized in terms of tagger performance alone is not necessarily
optimized for parsing. We will therefore modify the tag set of NDT in various
ways to see how the tag set granularity affects the tagging and, more
importantly in our case, the syntactic parsing in order to a identify a tag set
tailored for dependency parsing of Norwegian.

\section{Existing Tag Sets}
\label{sec:extagsets}
The tag set of a given treebank is the set of tags that can be assigned to
tokens in the treebank. The number of tags and thus the granularity varies
greatly between treebanks, and in order to limit ourselves to a small number of
tag sets to which we will compare the tag set of NDT, we chose Penn Treebank
\cite{Mar:San:Mar:93}, Stockholm-Umeå Corpus \cite{Gus:Har:06} and Universal
Dependencies \cite{Niv:15}.  It is worth discussing what grammatical
information is represented in the tags of the various tag sets and the authors'
rationale when developing the tag set.

Penn Treebank was chosen due to its status as the de facto standard treebank
for English. It is widely used in NLP and has also served as inspiration for
other treebanks. Stockholm-Umeå Corpus was chosen as Swedish is closely related
to Norwegian and thus very similar and comparable in terms of morphology and
syntax. Finally, we compare the tag set of NDT to the tag set of Universal
Dependencies because it is an attempt to define a cross-lingual tag set.

\subsection{Penn Treebank}
Penn Treebank \cite{Mar:San:Mar:93} is the de facto standard treebank for
English. It was developed at the University of Pennsylvania from 1989 to 1996,
and its tag set is based on that of the Brown corpus \cite{Fra:Kuc:79}.  The
Brown corpus tag set comprises 87 simple tags, but additionally allows the
formation of compound tags, giving a total of 187 tags. In order to reduce the
size of the tag set, \citeA{Mar:San:Mar:93} set out to eliminate the redundancy
present in the Brown corpus tag set by taking into account both lexical and
syntactic information in the development of tags. The final morphosyntactic tag
set of Penn Treebank consists of 36 PoS tags, disregarding tags for
punctuation, symbols, etc..

Penn Treebank introduces more fine-grained distinctions mainly for nouns and
verbs, which are inflected and agree with words in their context. The tag set
consists of fairly fine-grained tags for nouns (four tags), verbs (six tags)
and wh-words (four tags), as well as separate tags for adjectives and adverbs
of comparative and/or superlative degree. Apart from these, the categories are
mostly rather broad, such as \texttt{DT} (determiner).

State-of-the-art PoS taggers trained and evaluated on Penn Treebank report an
accuracy between 97\% and
98\%\footnote{\url{http://aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)}}.

\subsection{Stockholm-Umeå Corpus}
Stockholm-Umeå Corpus \cite{Gus:Har:06} is a treebank for Swedish, developed in
collaboration between Stockholm University and Umeå University in the 1990s. It
was the first generally accessible, annotated corpus of the Swedish language.
Its tag set is based on the tag set of SWETWOL \cite{Kar:92} and extensively
discussed in \citeA{Eje:Kal:Wen:Ast:92}. The tag set of SUC comprises 22
morphosyntactic tags and is thus quite coarse-grained.  It consists of mostly
very general categories, such as \texttt{JJ} (adjective), \texttt{NN} (noun)
and \texttt{VB} (verb).

\citeA{Ost:13} presented Stagger, an open source part-of-speech tagger for
Swedish, which reached an accuracy of 96.58\% on the second version of SUC. In
comparison, TnT \cite{Bra:00} obtained an accuracy of 95.9\%.

\subsection{Universal Dependencies}
\label{ssec:ud}
As parts-of-speech vary greatly between languages, properly comparing the
performance of taggers across languages is often impossible, as the tag sets
are highly dissimilar and therefore incomparable. In order to try to alleviate
the issues caused by these differences, \citeA{Pet:Das:McD:12} set out to
develop a universal tag set, universal in the sense that it is cross-lingual
and applicable to a range of languages. This is based on the idea that there
exists a set of coarse syntactic PoS categories in similar forms across
languages, so-called universals. They defined a tag set consisting of twelve
universal PoS categories: \textsc{noun} (noun), \textsc{verb} (verb),
\textsc{adj} (adjective), \textsc{adv} (adverb), \textsc{pron} (pronoun),
\textsc{det} (determiners), \textsc{adp} (adpositions, i.e., prepositions and
postpositions), \textsc{num} (numeral), \textsc{conj} (conjunction),
\textsc{prt} (particle), '.' (punctuation mark) and \textsc{x} (a catch-all for
categories such as abbreviations and foreign words). These categories are based
on what \citeA{Pet:Das:McD:12} consider to be the most frequent parts-of-speech
that exist in most languages. Moreover, these categories are what they expect
to be the most useful for users of PoS taggers.

To evaluate the universal tag set, they trained a supervised tagger based on a
trigram Markov model \cite{Bra:00} on the treebanks of 22 different languages.
For each of these treebanks, they created a mapping from the original treebank
tag set to the universal PoS tags, based on annotation guidelines and PoS tag
definitions. They experimented with three combinations of training and testing:
training and testing on the original tag set, training and testing on the
universal tag set, and finally, training on the original tag set and testing on
the universal tag set by mapping the predictions to the universal tag sets.
Accuracies for each of these experiments were presented, and in all cases,
training on the original tag set and testing on the universal tag set resulted
in the highest accuracy.

\citeA{Niv:15} revised and extended the universal tag set with five additional
tags, namely \textsc{intj} (interjection), \textsc{propn} (proper noun),
\textsc{aux} (auxiliary verb), \textsc{sconj} (subordinate conjunction) and
\textsc{sym} (symbol), resulting in a total of 17 tags. Additionally,
\textsc{prt} was renamed \textsc{part} and '.' was renamed \textsc{punct}. This
was done as part of the Universal Dependencies (UD)
project\footnote{\url{http://universaldependencies.org}}, which is developing
cross-linguistically consistent treebank annotation for a wide array of
languages.

\section{Parts-of-Speech in the Norwegian Dependency Treebank}
\label{sec:posndt}
The Norwegian Dependency Treebank operates with 12 morphosyntactic PoS tags,
7 of which additionally have features for more fine-grained morphological
information, such as  gender, number, definiteness and tense.
We will take advantage of these features in the development of new tag sets,
where we will create new tags which are concatenations of a coarse tag and one
or more of these features.
%For instance, to see the effect of gender of nouns,
%we can create the new tags \texttt{subst|mask} `masculine noun',
%\texttt{subst|fem} `feminine noun' and \texttt{subst|nøyt} `neuter noun'. We
%can then introduce a mapping in which we replace the original coarse tag with
%the more fine-grained tag for the tokens where this is applicable in the
%treebank.
This allows us to assess what morphological information is useful in
tagging and parsing of Norwegian.

As the tag set of the NDT borrows heavily from the Oslo-Bergen Tagger, which in
turn is based on \citeA{Faa:Lie:Van:97}, we mainly rely on
\citeA{Faa:Lie:Van:97} in discussing the parts-of-speech and their respective
properties. All examples are taken from the Norwegian Dependency Treebank,
unless otherwise noted.  Feature values in brackets, e.g., \texttt{<adv>} (used
for adjectives which may function as adverbials), give additional information
about the token that might be relevant, but does not concern the morphological
form directly \cite{Kin:Sol:Eri:13}.

In the following, we will look at the parts-of-speech of Norwegian, how they
are represented and used in the Norwegian Dependency Treebank, and compare the
tag set of NDT to three other arguably comparable tag sets, namely those of the
aforementioned Penn Treebank \cite{Mar:San:Mar:93}, Stockholm-Umeå Corpus
\cite{Gus:Har:06} and Universal Dependencies \cite{Niv:15}, outlined in Section
\ref{sec:extagsets}. See Table \ref{comptagsets} for complete overview of their
tag sets, compared to the tag set of NDT.

%\begin{table}
    %\centering
    %\smaller[1]
    %\begin{tabular}{lllllllll}
        %\toprule
        %\textbf{PoS} & \textbf{Gender} & \textbf{Number} & \textbf{Type} &
        %\textbf{Def.} & \textbf{Tense} & \textbf{Person} & \textbf{Case} & \textbf{Degree} \\ \midrule
        %\multirow{4}{*}{adj}
        %& m/f & ent & <adv> & ub & & & & pos \\
        %& nøyt & fl & <ordenstall> & be & & & & komp \\
        %& & & <perf-part> & & & & & sup \\
        %& & & <pres-part> & & & & & \\
        %& & & fork & & & & & \\
        %\midrule
        %adv & & & & & & & & \\
        %\midrule
        %\multirow{10}{*}{det} & mask & ent & dem & ub & & & & \\
        %& nøyt & fl & <adj> & be & & & & \\
        %& fem & & forst & & & & & \\
        %& & & sp & & & & & \\
        %& & & kvant & & & & & \\
        %& & & poss & & & & & \\
        %& & & res & & & & & \\
        %& & & høflig & & & & & \\
        %\midrule
        %inf-merke & & & & & & & & \\
        %\midrule
        %interj & & & & & & & & \\
        %\midrule
        %\multirow{2}{*}{konj} & & & <adv> & & & & & \\
        %& & & clb & & & & & \\
        %\midrule
        %prep & & & & & & & & \\
        %\midrule
        %\multirow{9}{*}{pron} & fem & ent & hum  & & & 1 & nom & \\
        %& mask & fl & res & & & 2 & akk & \\
        %& nøyt & & pers & & & 3 & & \\
        %& & & refl  & & & & & \\
        %& & & høflig & & & & & \\
        %& & & poss  & & & & & \\
        %& & & refl & & & & & \\
        %& & & sp & & & & & \\
        %\midrule
        %sbu & & & <spørreartikkel> & & & & & \\
        %\midrule
        %\multirow{3}{*}{subst} & mask & ent & appell & ub & & & gen & \\
        %& fem & fl & prop & be & & & & \\
        %& nøyt & & fork & & & & & \\
        %\midrule
        %ukjent & & & & & & & & \\
        %\midrule
        %\multirow{7}{*}{verb} & & & & & imp & & & \\
        %& & & & & inf & & & \\
        %& & & & & perf-part & & & \\
        %& & & & & pres & & & \\
        %& & & & & pret & & & \\
        %& & & & & pass & & & \\
        %\bottomrule
    %\end{tabular}
    %\caption{The tag set and features in NDT.}
    %\label{completetagset}
%\end{table}

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{lllll}
        \toprule
        \textbf{Description} & \textbf{NDT} & \textbf{PTB} & \textbf{SUC} &
        \textbf{UD} \\
        \midrule
        Adjective & \texttt{adj} & \texttt{JJ} & \texttt{JJ} & \texttt{ADJ} \\
        & & \texttt{JJR} & \texttt{PC} & \\
        & & \texttt{JJS} & & \\
        \midrule
        Adverb & \texttt{adv} & \texttt{RB} & \texttt{AB} & \texttt{ADV} \\
        & & \texttt{RBR} & \texttt{HA} & \\
        & & \texttt{RB\$} & & \\
        & & \texttt{WRB} \\
        \midrule
        Determiner & \texttt{det} & \texttt{DT} & \texttt{DT} & \texttt{DET} \\
        & & \texttt{PDT} & \texttt{PS} & \texttt{NUM} \\
        & & \texttt{PRP\$} & & \\
        \midrule
        Infinitive marker & \texttt{inf-merke} & \texttt{TO} & \texttt{IE} &
        \texttt{PART} \\
        \midrule
        Interjection & \texttt{interj} & \texttt{UH} & \texttt{IN} & \texttt{INTJ} \\
        \midrule
        Conjunction & \texttt{konj} & \texttt{CC} & \texttt{KN} & \texttt{CONJ} \\
        \midrule
        Preposition & \texttt{prep} & \texttt{IN} & \texttt{PP} & \texttt{ADP} \\
        & & & & \texttt{ADV} \\
        \midrule
        Pronoun & \texttt{pron} & \texttt{PRP} & \texttt{PN} & \texttt{PRON} \\
        & & \texttt{PRP\$} & \texttt{PS} & \\
        & & \texttt{WP} & \texttt{HP} & \\
        & & \texttt{WP\$} & \texttt{HS} \\
        \midrule
        Subordinate conjunction & \texttt{sbu} & \texttt{IN} & \texttt{SN} & \texttt{SCONJ} \\
        \midrule
        Noun & \texttt{subst} & \texttt{NN} & \texttt{NN} & \texttt{NOUN} \\
        & & \texttt{NNS} & \texttt{PM} & \texttt{PROPN} \\
        & & \texttt{NNP} & & \\
        & & \texttt{NNPS} & & \\
        \midrule
        Unknown & \texttt{ukjent} & \texttt{FW} & \texttt{UO} & \texttt{X} \\
        \midrule
        Verb & \texttt{verb} & \texttt{VB} & \texttt{VB} & \texttt{VERB} \\
        & & \texttt{VBD} & & \texttt{AUX} \\
        & & \texttt{VBG} & & \\
        & & \texttt{VBN} & & \\
        & & \texttt{VBP} & & \\
        & & \texttt{VBZ} & & \\
        & & \texttt{MD} & & \\
        \bottomrule
    \end{tabular}
    \caption{Qualitative comparison of the tag sets of NDT, PTB, SUC and UD.}
    \label{comptagsets}
\end{table}

\subsection{Nouns}
\label{subsec:noun}
Nouns are words that denote entities and objects, either real or abstract. In
Norwegian, nouns have gender (masculine, feminine or neuter) and are inflected
for number (singular or plural) and definiteness (definite or indefinite). For
instance, a masculine noun such as \emph{bil} `car' have four different forms:
indefinite singular \emph{bil}, definite singular \emph{bilen}, indefinite
plural \emph{biler} and definite plural \emph{bilene}.  Introducing rather
fine-grained tags for nouns may prove useful due to the nature of nouns, as
they have modifiers with which they agree (in gender, number and definiteness).
Example (\ref{exnoun}) shows this agreement, where the form of the determiner
and adjective is dependent on the properties of the noun they modify.

\begin{examples}
\item \gll Det var et kraftfullt svar
    It was a.\textsc{neut} powerful.\textsc{neut} answer.\textsc{neut}
    \glt `It was a powerful answer'
    \glend
    \label{exnoun}
\end{examples}

In NDT, nouns can also have three different types: \texttt{appell} `common
noun', \texttt{prop} `proper noun` or \texttt{fork} `abbreviation'. Nouns in
genitive case, e.g., \emph{Karis} `Kari's', are tagged as noun and marked with
the feature \texttt{gen}. This is quite different from the Penn Treebank, where
genitives have the possessive ending split off and tagged separately as a
possessive ending (\texttt{POS}).

Penn Treebank operates with four PoS tags for nouns, e.g., \texttt{NN} for
mass or singular nouns and \texttt{NNP} for singular proper nouns. SUC
distinguishes between common and proper nouns, \texttt{NN} and \texttt{PM},
respectively. Similarly, the UD tag set has \texttt{NOUN} for common nouns and
\texttt{PROPN} for proper nouns. As they all distinguish between proper and
common nouns, investigating how the noun type may affect tagging and parsing
of Norwegian is of interest.

\subsection{Verbs}
\label{subsec:verb}
Verbs are words that convey actions, e.g., \emph{spille} `play' and \emph{lese}
`read'. Verbs are inflected for tense in Norwegian, and can also have the
imperative mood, indicating a command. For instance, the verb \emph{skrive}
`write' has five different forms: imperative \emph{skriv}, infinitive
\emph{skrive}, present \emph{skriver}, preterite \emph{skrev} and past perfect
\emph{(har) skrevet}. We furthermore see passive voice marked either through
inflection (See Example (\ref{expassive})) or periphrastically with an auxiliary
verb (e.g., \emph{ble skrevet}) \cite{Faa:Lie:Van:97}. The usage and
distribution of auxiliary verbs in Norwegian is very similar to that of
English, however, the treebank does not have a separate category, or even
feature, for auxiliary verbs, but instead simply treats them as verbs.

\begin{examples}
\item \gll Fondet bør forvaltes av Norfund
    Fund.\textsc{def} should manage.\textsc{pass} by Norfund
    \glt `The fund should be managed by Norfund'
    \glend
    \label{expassive}
\end{examples}

Perfect participles, e.g., \emph{skrevet} `written', can function either as
verbs, e.g., \emph{Han hadde skrevet boken} `He had written the book', or as
adjectives, e.g., \emph{En skrevet bok} `A written book'. Note that perfect
participles appearing as adjectives can be inflected for definiteness in
Norwegian, e.g., \emph{Den skrevne boken} `The written book'. To determine
whether a perfect participle is an adjective or a verb, the authors of NDT
utilized certain syntactic tests. If the participle is an attribute preceding a
head noun, it should be tagged as adjective, but when the participle is a
complement to the auxiliary verbs \emph{ha} `have' or \emph{få} `get', or a
modal verb, it should be assigned the \texttt{verb} tag. However, if the
participle is complement to the auxiliary verbs \emph{være} `be' or \emph{bli}
`become', it could be either verb or adjective. If a degree adverb such as
\emph{fort} `quickly' can precede the participle, it should be tagged as verb,
and if it can be modified by a manner adverb such as \emph{veldig} `very', it
should be given the \texttt{adj} tag. The treebank uses the bracket tag
\texttt{<perf-part>} as a feature for perfect participles which are adjectives
\cite{Kin:Sol:Eri:13}. As present participles are generally regarded as
adjectives \cite{Faa:Lie:Van:97}, we discuss them in Section \ref{subsec:adj}.

Penn Treebank has a total of six verb tags, among those \texttt{VB} for verb
base form, \texttt{VBD} for verbs in past tense and \texttt{VBZ} for verbs in
third person singular present. In addition to these tags, they have a separate
tag for modal auxiliary verbs, \texttt{MD}. It makes sense to have more
fine-grained verb tags for English, as verbs in English agree with their
arguments (subject and object) in person and number (cf.  \emph{I am},
\emph{you are}, \emph{he is}). The tag set of SUC is far more coarse-grained
with only a single tag for verbs, \texttt{VB}, while the UD tag set includes
\texttt{VERB} and a separate \texttt{AUX} tag for auxiliary verbs.

%, due to the fact that auxiliary
%category helps define a syntactic rule that attaches verbs to an auxiliary
%head, which is beneficial for certain languages \cite{Pet:Das:McD:12}.

\subsection{Adjectives}
\label{subsec:adj}
Adjectives are words that describe or modify other words, mostly nouns, by
assigning them certain attributes. In Norwegian, adjectives agree with the
noun they modify in terms of gender, number and definiteness. A given adjective
generally takes on three different forms: indefinite masculine/feminine
singular (e.g., \emph{stor} `big'), indefinite neuter singular (e.g.,
\emph{stort}) and definite/plural (e.g., \emph{store}). Most, but not all,
adjectives can furthermore be inflected for degree, viz., positive (e.g.,
\emph{stor}), comparative (e.g., \emph{større} `bigger') and superlative (e.g.,
\emph{størst} `biggest').  Comparative and superlative can be expressed either
through inflection or periphrastically with a degree adverb (e.g., \emph{mer
    tilfreds} `more satisfied').

The treebank operates with five different features for adjectives: gender,
number, type, definiteness and degree. Gender, number, definiteness and degree
are as just described. The type feature is use to denote the type of
the adjective, and applies to adjectives which are in some sense `special',
hence most adjectives will not be assigned a type. The five available types are
\texttt{<adv>} `adverbial', \texttt{<ordenstall>} `ordinal number',
\texttt{<perf-part>} `perfect participle', \texttt{<pres-part>} `present
particle' and \texttt{fork} `abbreviation'. Adjectives that can occur as
adverbials include temporal adverbs that can be inflected, such as \emph{ofte}
`often' and \emph{snart} `soon', cf.  \emph{veldig ofte} `very often' and
\emph{ganske snart} `pretty soon', respectively. As previously mentioned,
bracket tags give additional information about the token that might be
relevant, but does not concern the morphological form directly
\cite{Kin:Sol:Eri:13}.

Participles are verb forms that can function as adjectives and hence modify
nouns, e.g., \emph{en patentert oppfinnelse} `a patented invention' (perfect
participle) or \emph{en levende person} `a living person' (present participle).
Generally, if the perfect participle is an attribute preceding a noun, it is
tagged as adjective, otherwise, it is tagged as verb. However, these are just
the general principles, and there are exceptions; see section \ref{subsec:verb}
for the full discussion on the treatment of perfect participles in the
treebank. Present participles are almost exclusively used as adjectives in
Norwegian, where they generally appear in the same syntactic positions as other
adjectives, i.e., as complements to nouns, predicates, etc., and can hence best
be described as an adjective. The exceptions are restricted to very specific
syntactic constructions with continuous aspect in which the present participle
follows the auxiliary verb \emph{bli} `become' \cite{Faa:Lie:Van:97}.

Penn Treebank operates with three adjective tags: \texttt{JJ} for adjectives,
\texttt{JJR} for comparative adjectives and \texttt{JJS} for superlative
adjectives. Furthermore, they have separate tags for participles: \texttt{VBG}
for present participle (also known as gerund) and \texttt{VBN} for past
participle. In SUC, similarly to Penn Treebank, we see the \texttt{JJ} tag for
adjectives and a separate tag for participles, \texttt{PC}. In the UD tag set,
there is a single tag for adjectives, namely \texttt{ADJ}.

\subsection{Determiners}
\label{subsec:det}
Determiners are words that occur with a noun or noun phrase, where they
determine or specify the reference of the noun or noun phrase
\cite{Faa:Lie:Van:97}, e.g., \emph{noen} `some' or \emph{min} `my'. Determiners
in Norwegian can have gender (feminine, masculine or neuter), number (singular
or plural) and definiteness (definite or indefinite), agreeing with the noun or
noun phrase they modify. NDT also has a number of possible types for
determiners: possessives (denoting possession or belonging), demonstratives
(referring to an entity in the context), quantifiers (expressing quantity
of some kind), amplifiers (amplifying degree of ownership, etc.) and
interrogatives (introducing a question).

In some cases, the noun or noun phrase is not explicitly expressed, but instead
implicit, e.g., \emph{Min (bil) er nyere enn din} `My (car) is newer than
yours' \cite{Faa:Lie:Van:97}. Possessives may also follow the noun, cf. Example
\ref{exdet}.

Both Penn Treebank and SUC use the \texttt{DT} tag for determiners and
have a separate category for possessive pronouns (\texttt{PRP\$} and \texttt{PS},
respectively). Determiners in the PTB tag set include articles such as
\emph{every} and \emph{no}, indefinite determiners such as \emph{any},
\emph{each} and \emph{those}, and instances of \emph{all} and \emph{both} when
they do not precede a determiner or possessive pronoun \cite{San:90}.  The tag
set of the Penn Treebank furthermore have a separate category for
predeterminers, \texttt{PDT}. The English language distinguishes between
possessive determiners (e.g., \emph{my}) and possessive pronouns (e.g.,
\emph{mine}). This is not the case for Norwegian, which uses the same form in
both cases, thus simplifying the tagging.

The UD tag set has a single category for determiners, \texttt{DET}. It
does, however, have a separate tag for numerals, \texttt{NUM}, which includes
quantifiers, which function as determiners and are tagged as such in NDT.

\begin{examples}
\item \gll Jeg kjente ikke konen hans (...)
    I knew not wife.\textsc{def} his
    \glt `I did not know his wife'
    \glend
    \label{exdet}
\end{examples}

\subsection{Pronouns}
\label{subsec:pron}
Pronouns are words that can take the place of a noun or noun phrase, e.g.,
\emph{hun} `she' or \emph{vi} `we'. Norwegian pronouns can exhibit gender
(feminine, masculine or neuter), number (singular or plural), person (first,
second or third) and case (nominative or accusative). Note that case-marking
only applies to pronouns in Norwegian, as in English. We furthermore see four
types of pronouns in the NDT: interrogative (\texttt{sp}), personal
(\texttt{pers}), reflexive (\texttt{refl}) and reciprocal (\texttt{res}).
%, as
%well as the secondary types human (\texttt{hum}) and polite (\texttt{høflig}).
One generally also considers possessives as pronouns, but these are tagged as
determiners in the Norwegian Dependency Treebank, for reasons we consider in
\ref{subsec:det}.

Reflexive pronouns are pronouns that refer to a preceding (pro)noun, its
antecedent. In Norwegian, the accusative form of personal pronouns in first and
second person can act as reflexive, e.g., \emph{Jeg så meg om} `I looked
around', \emph{jeg} being first person singular nominative and \emph{meg} being
(normally, in non-reflexive constructions) first person singular accusative.
However, the reflexive pronoun \emph{seg} is exclusively reflexive, used only
in third person, cf. Example (\ref{exrefl2}).

\begin{examples}
\item \gll Israel bryr seg om dette
    Israel cares \textsc{refl.\small{3}} about this
    \glt `Israel cares about this'
    \glend
    \label{exrefl2}
\end{examples}

%As noted, NDT has a variety of features for pronouns, e.g., for \emph{hun}
%`she', we see `feminine singular personal human third person nominative'. This
%is clearly very fine-grained, possibly more so than is desirable for our task.
%For instance, the gender and number of the pronoun should not be of
%significance, cf.  \emph{Hun gjorde det} `She did it' (feminine singular) vs.
%\emph{Vi gjorde det} `We did it' (plural). They appear in the same exact
%syntactic constructions, so these distinctions should not have notable
%syntactic consequences.

The Penn Treebank tag set contains four tags for pronouns: \texttt{PRP} for
personal pronouns, \texttt{PRP\$} for possessive pronouns, \texttt{WP} for
wh-pronouns  and \texttt{WP\$} for possessive wh-pronouns.  SUC operates with
the \texttt{PN} tag for pronouns, additionally separating possessive pronouns
in the \texttt{PS} tag. For interrogative/relative pronouns and
interrogative/relative possessive pronouns, they have the tags \texttt{HP} and
\texttt{HS}, respectively. The UD tag set has a single tag for pronouns,
namely \texttt{PRON}.

\subsection{Adverbs}
Adverbs are words that modify verbs, adjectives, determiners and more, and
typically express manner, time, degree, etc.. It is sometimes also regarded as
a catch-all category for words unfit for other categories. In the NDT, the
\texttt{adv} `adverb' category does not have any features and hence no further
morphological information. Adverbs that are derived from adjectives by
conversion (also known as zero derivation), i.e., adverbs that have the same
form as their corresponding adjective and normally express the manner in which
a verb is performed, are simply tagged as adjective, but given the feature
\texttt{<adv>}. Furthermore, these adverbs are given the exact same
morphological features as the adjective from which they are derived. Other
adverbs, such as locative or temporal adverbs, are simply tagged as adverbs.

Penn Treebank operates with four different tags for adverbs: \texttt{RB} for
adverbs, \texttt{RBR} for comparative adverbs, \texttt{RBS} for superlative
adverbs and lastly \texttt{WRB} for wh-adverbs. SUC has a single tag for
adverbs (\texttt{AB}), as does the UD tag set (\texttt{ADV}).

\subsection{Other Categories}
\label{ssec:othercat}
Several parts-of-speech in the treebank (mostly closed classes) do not have any
available features, hence there are no features to use for tag set
modifications. These categories are, apart from \texttt{adv},
\texttt{inf-merke} `infinitive marker', \texttt{interj} `interjection',
\texttt{konj} `conjunction', \texttt{prep} `preposition', \texttt{sbu}
`subordinate conjunction' and \texttt{ukjent} `unknown' (mostly foreign words).

Penn Treebank uses the \texttt{TO} tag for both uses of the word `to',
including the infinitive marker. SUC tags infinitive markers with the
\texttt{IE} tag. In the UD tag set, infinitive markers are tagged as particles
(\texttt{PART}).  Interjections receive the \texttt{UH} tag in Penn Treebank,
\texttt{IN} in SUC and \texttt{INTJ} in the UD tag set.  Conjunctions are
tagged with \texttt{CC} in Penn Treebank, \texttt{KN} in SUC and \texttt{CONJ}
in the UD tag set.  Penn Treebank groups both prepositions and subordinate
conjunctions under the \texttt{IN} tag. SUC assigns \texttt{PP} to prepositions
and \texttt{SN} to subordinate conjunctions, while the UD tag set uses
\texttt{ADP} and \texttt{SCONJ}, respectively.  For foreign words, Penn
Treebank uses the \texttt{FW} tag, while SUC uses \texttt{UO}. The \texttt{X}
tag in the UD tag set acts as a catch-all for foreign words as well as other
categories, including abbreviations.

%Tags in Penn Treebank with no equivalent in NDT(??): CD (cardinal number), EX
%(existential there), LS (list item marker), POS (possessive ending), RP
%(particle)

%Tags in SUC with no equivalent in NDT(??): HD (interrogative/relative
%determiner), PL (particle), RG (cardinal numbre), RO (ordinal number). Also PC
%(participle)?

%Tags in UD with no equivalent in NDT(??): PRT (particles). What about PUNCT and
%SYM? not actually PoS...

%In NDT, ordinal numbers are tagged as \texttt{adj} and given the
%feature \texttt{<ordenstall>}.

\chapter{Experimental Setup}
\label{chap:expsetup}
In preparation to conducting our experiments with linguistically motivated tag
set modifications, a concrete setup for the experiments needed to be
established, which is presented in the following. This setup includes a
proposed data set split (training/development/test) of the Norwegian Dependency
Treebank, our initial tag sets and how we realize the tag set modifications by
mapping the original tags to new, more fine-grained tags by appending selected
sets of morphological features. We then detail the evaluation of tagging and
parsing in our tag set experiments before discussing various ways of computing
a baseline tagger accuracy to which we can compare the tagger performance.
Finally, we present the tagger and parser used in our tag set experiments and
how the sum of these components are conjoined in a pipeline under which the
experiments are run.

\section{Data Set Split}
When working with data for machine learning, it is common practice to split the
data into separate sets to be used for specific parts of the machine learning
process. This usually involves splitting the data into three data sets:
training data, which is used for training the machine learning algorithm;
development data, used for evaluation of the system during development; and
test data, to be used for the final evaluation of the system.  It is crucial
that we keep the test set separate from the other data and abstain from using
it during the development as the final evaluation needs to be performed on new
data for the most realistic evaluation of how the system will perform on data
outside of the treebank.

A central point of concern in machine learning is that machine learning
algorithms have a tendency to overfit. Overfitting occurs when the learning
algorithm is excessively trained on the training data, leading to the algorithm
being `fitted' to the training data to such an extent that its performance on
the training/development data is a false measure of its predictive performance
on unseen data.  We want the training data to be representative of the data we
encounter elsewhere in the treebank (i.e., in the development and test sets),
but not overly representative, as that would lead to the algorithm being geared
towards the training data and highly sensitive to input data. The algorithm
needs to be able to generalize from training data to unseen data.

\subsection{Data Set Split of the Norwegian Dependency Treebank}
Currently, there is no standard data set split of the Norwegian Dependency
Treebank. We therefore propose a split of the treebank which we hope to
establish as the new standard. A standard split is important for ease of use
and reusability of the treebank, which we facilitate by splitting on files and
listing what files comprise the different data sets (See Appendix
\ref{chap:appsplit}).  Furthermore, we have collected the data for the data
sets in separate files in CoNLL format, which will be distributed with the
treebank. Our data set split was used in the Norwegian contribution to the
Universal Dependencies project \cite{Ovr:Hoh:16}. See Tables
\ref{trainingdataset}, \ref{devdataset}, \ref{testdataset} and
\ref{completedataset} for overview of the split of the treebank.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrr@{}}
        \toprule
        \textbf{Source} & \textbf{Genre} & \textbf{\# Sentences} &
        \textbf{\# Tokens} \\
        \midrule
        Aftenposten & Newspaper & 4629 & 75832 \\
        Bergens Tidende & Newspaper & 2587 & 38762 \\
        Dagbladet & Newspaper & 3573 & 48640 \\
        Klassekampen & Newspaper & 686 & 12446 \\
        Sunnmørsposten & Newspaper & 847 & 13231 \\
        Verdens Gang & Newspaper & 755 & 11091 \\
        Blogs & Blogs & 806 & 12514 \\
        Government reports & Government reports & 843 & 14797 \\
        Parliament transcripts & Parliament transcripts & 970 & 17463 \\
        \midrule
        Total & & 15696 & 244776 \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the training data set of NDT.}
    \label{trainingdataset}
\end{table}

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrr@{}}
        \toprule
        \textbf{Source} & \textbf{Genre} & \textbf{\# Sentences} &
        \textbf{\# Tokens} \\
        \midrule
        Aftenposten & Newspaper & 659 & 11006\\
        Bergens Tidende & Newspaper & 400 & 5141 \\
        Dagbladet & Newspaper & 448 & 7086 \\
        Klassekampen & Newspaper & 119 & 2135 \\
        Sunnmørsposten & Newspaper & 186 & 2923 \\
        Verdens Gang & Newspaper & 98 & 1465 \\
        Blogs & Blogs & 200 & 1444 \\
        Government reports & Government reports & 100 & 2298 \\
        Parliament transcripts & Parliament transcripts & 200 & 2969 \\
        \midrule
        Total & & 2410 & 36467 \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the development data set of NDT.}
    \label{devdataset}
\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrr@{}}
        \toprule
        \textbf{Source} & \textbf{Genre} & \textbf{\# Sentences} &
        \textbf{\# Tokens} \\
        \midrule
        Aftenposten & Newspaper & 578 & 9160 \\
        Bergens Tidende & Newspaper & 348 & 5332 \\
        Dagbladet & Newspaper & 415 & 5048 \\
        Klassekampen & Newspaper & 114 & 1912 \\
        Sunnmørsposten & Newspaper & 168 & 2983 \\
        Verdens Gang & Newspaper & 15 & 277 \\
        Blogs & Blogs & 144 & 2433 \\
        Government reports & Government reports & 47 & 1165 \\
        Parliament transcripts & Parliament transcripts & 110 & 1724 \\
        \midrule
        Total & & 1939 & 30034 \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the test data set of NDT.}
    \label{testdataset}
\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrr@{}}
        \toprule
        \textbf{Source} & \textbf{Genre} & \textbf{\# Sentences} &
        \textbf{\# Tokens} \\
        \midrule
        Aftenposten & Newspaper & 5866 & 95998 \\
        Bergens Tidende & Newspaper & 3335 & 49235 \\
        Dagbladet & Newspaper & 4436 & 60774 \\
        Klassekampen & Newspaper & 919 & 16493 \\
        Sunnmørsposten & Newspaper & 1201 & 19137 \\
        Verdens Gang & Newspaper & 868 & 12833 \\
        Blogs & Blogs & 1150 & 16391 \\
        Government reports & Government reports & 990 & 18260 \\
        Parliament transcripts & Parliament transcripts & 1280 & 22156 \\
        \midrule
        Total & & 20045 & 311277 \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the full data set of NDT.}
    \label{completedataset}
\end{table}

Our split of the Norwegian Dependency Treebank splits the data into three data
sets, viz., training, development and testing. 80\% of the data is used in the
training, 10\% in the development and the final 10\% resides in the held-out
test data set, which is a commonly used data set split for machine learning in
NLP. NDT consists of data from various sources in a range of genres, namely
newspaper articles, blog posts, parliament transcripts and government reports.
The data from these sources is organized in files, where each file contains a
maximum of 100 documents. Here, a document denotes a coherent piece of text
from a given source, i.e., an article, a blog post, a parliament transcript or
a government report.

We want new words and sentences in the development and test data sets, while
balancing the split in terms of genre. This is important because the language
of different genres tends to differ in many ways, and we cannot expect a tagger
that is trained exclusively on fashion blogs to perform well on the sports
section of a newspaper, for instance.

The two approaches for splitting the treebank we considered was either to
maintain contiguous sections within the sources (i.e., splitting on files) or
to divide the corpus into units of ten sentences each, assigning the first
eight sentences of each unit to the training data set and the two remaining
sentences to development and testing, respectively. This is analogous to
assigning every ninth sentence to development and every tenth sentence to
testing, and simply assigning the remaining sentences to the training set.

We opted for contiguous sections within the sources of the treebank, where the
first 80\% of the files from a particular source is used for training, the next
10\% is used for development and the final 10\% is used for testing. The
entirety of a given document is generally contained in one of the data sets,
instead of having document fragments distributed over the sets. Fragments are
problematic because they might lead to overfitting, as we would be testing on
sentences from a document that has already been partially seen in the training.
There are some overlap of documents, i.e., documents that start in one file and
continue and end in the next, but this can occur only with the last document of
a particular file, and is hence rather negligible.

%If we instead were to divide the corpus on documents, i.e., manually separate
%the documents (i.e., articles/blog posts, etc.) within the files, and use these
%instead of the files as main units on which we separate, we would have to
%manually find the article boundaries, which are not directly deducible from the
%files; we would have to infer this from the context. This would be quite
%cumbersome to perform and the results wouldn't necessarily be beneficial for
%our task. Hence, we chose to eschew this approach.

\section{Initial Tag Sets}
\label{sec:inittagsets}
In our experiments, we want to investigate how we can use the morphological
features included in the treebank in the creation of new, more fine-grained
tags. The original tag set of NDT contains 19 tags, 12 of which are
morphosyntactic tags, the remaining 7 being for punctuation, symbols, etc.. In
an initial experiment, we concatenated the tag of each token with its set of
morphological features in order to map the original tag set to a new, more
fine-grained tag set\footnote{Hereafter referred to as the \emph{full} tag set}. The
result of this was a total of 368 tags, which is clearly very fine-grained.
These two tag sets thus represent two extremes in terms of granularity, shown
in Table \ref{ndttagsets}.

As a consequence of the very fine granularity in the full tag set, we see tags
in the development and test data that do not occur in the training data. As
these tags are not part of the training data, the tagger has no way of learning
or successfully assigning them. This is the case only for very infrequent tags,
and is caused by the problem of sparse data. As there is no clear way of
resolving this minor issue, we do not take any action to alleviate this.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Tag set} & \textbf{\# Tags} \\
        \midrule
        Original & 19 \\
        Full & 368 \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the initial tag sets and their respective size.}
    \label{ndttagsets}
\end{table}

\section{Tag Set Mapping}
\label{sec:mapping}
In order to modify the tag set of the treebank, we need to be able to map the
original tag set to a new tag set. This is done by specifying the features to
be concatenated to the relevant existing tag(s) in a separate file and
supplying it to the pipeline as an argument. For instance, to create new tags
for nouns which include the grammatical gender, we would add \emph{subst fem}
`noun feminine', \emph{subst mask} `noun masculine' and \emph{subst nøyt} `noun
neuter` to the tag set file, thus creating the new tags \texttt{subst|fem},
\texttt{subst|mask} and \texttt{subst|nøyt}. We then replace the original tag
with the more fine-grained tag for all applicable tokens in the treebank, here
corresponding to tokens tagged as \texttt{subst}, having either \texttt{fem},
\texttt{mask} or \texttt{nøyt} in the morphological features. See Table
\ref{exmapping} for an example of the tag set mapping. This is one of the
mappings carried out in the experiments with tag set modifications, the results
of which can be seen in Section \ref{sec:expnouns}.

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llll|l@{}}
        \toprule
        \textbf{ID} & \textbf{Token} & \textbf{Tag} & \textbf{Features} &
        \textbf{New Tag} \\
        \midrule
        1 & \emph{Lam} & \texttt{subst} & \texttt{appell|nøyt|ub|ent} &
        \texttt{subst|nøyt} \\
        2 & \emph{og} & \texttt{konj} & \_ & \\
        3 & \emph{piggvar} & \texttt{subst} & \texttt{appell|mask|ub|ent}
        & \texttt{subst|mask} \\
        4 & \emph{på} & \texttt{prep} & \_ & \\
        5 & \emph{bryllupsmenyen} & \texttt{subst} &
        \texttt{appell|mask|be|ent|samset} & \texttt{subst|mask} \\
        6 & \emph{|} & \texttt{clb} & \_ & \\
        \bottomrule
    \end{tabular}
    \caption{Example of tag set mapping, introducing gender for nouns.}
    \label{exmapping}
\end{table}

The tag set mapping needs to be deterministic and injective, i.e., for any
given token, there can be at most one applicable new tag. This imposes
restrictions on what tag set modifications we can carry out, which will be
further discussed where relevant. We need to enforce determinism both locally,
in regard to a specific feature, as well as across features when we combine
features. This involves the various values for a given feature having to be
mutually exclusive, ensuring local determinism. If any token in the treebank
has more than one of these feature values, we would ultimately reach a dilemma
in which we have to choose which feature to use. For instance, when
experimenting with tense and voice for verbs, we found that many relevant
tokens in the treebank had both tense and voice marked, meaning that there were
two possible tags for these verbs, either appending the tense or the voice to
the original tag. As this violates the determinism, we instead combined the
feature values for the relevant tokens, resulting in \texttt{verb|inf|pass}
`verb infinitive passive' and \texttt{verb|pres|pass} `verb present passive',
as all passive verbs in Norwegian are either infinitive or present tense.
Similar workarounds are performed when necessary and presented accordingly.

\section{Evaluation}
To evaluate and compare the performance of PoS taggers and syntactic dependency
parsers, there are certain metrics regarded as `de facto standards' in the
field. We adhere to these standards in order to facilitate a comparative
analysis of the various systems that can be readily compared to previous work.

For a given class (e.g., PoS or dependency relation), the \emph{true positives}
are the instances correctly predicted as belonging to said class, while the
\emph{false positives} are the instances erroneously assigned to said class. The
\emph{false negatives} are the instances that should have been assigned to said
class, but erroneously was not.

\emph{Precision} measures the reliability of the system's predictions, i.e.,
the percentage of instances assigned to a given class by the system that
actually belong in the class per the gold standard.

\begin{equation*}
    \text{Precision} = \frac{\text{true positives}}{\text{true positives +
            false positives}}
\end{equation*}

\vspace{1ex}

\emph{Recall} measures the robustness of the system, i.e., the percentage of
instances in a given class per the gold standard that the system correctly
assigned to said class.

\begin{equation*}
    \text{Recall} = \frac{\text{true positives}}{\text{true positives + false
            negatives}}
\end{equation*}

\vspace{1ex}

\emph{F-score} is a harmonic mean of precision and recall.

\begin{equation*}
    \text{F-score} = 2*\frac{\text{precision * recall}}{\text{precision + recall}}
\end{equation*}

\paragraph{PoS Tagging}
\emph{Accuracy} is defined as the percentage of tokens assigned the correct PoS
tag.

\begin{equation*}
    \text{Accuracy} = \frac{\text{\# correctly tagged tokens}}{\text{\# tagged
            tokens}}
\end{equation*}

\vspace{1ex}

The TnT-included evaluation script \texttt{tnt-diff} is used to evaluate TnT as
well as our MFT baseline tagger in our tag set experiments. Precision, recall
and F-score are calculated by our tailor-made tagger error analysis
script\footnote{See \url{https://github.com/petterhh/ndt-tools}}.

\paragraph{Syntactic Parsing}
\emph{Unlabeled attachment score (UAS)} measures the percentage of tokens that
are assigned the correct head by the system, while \emph{labeled attachment
    score (LAS)} additionally takes the label into account.

\begin{equation*}
    \text{UAS} = \frac{\text{\# correctly attached tokens}}{\text{\# tokens}}
\end{equation*}

\vspace{1ex}

\begin{equation*}
    \text{LAS} = \frac{\text{\# correctly attached and labeled
            tokens}}{\text{\# tokens}}
\end{equation*}

\vspace{1ex}

\emph{Frequency-weighted difference of F-scores} for a given dependency
relation weights the difference in F-score between two samples by the relative
frequency of the dependency relation.

\begin{equation*}
    \text{Weighted F-score diff} = \text{F-score diff}*\frac{\text{\# tokens
            assigned given label}}{\text{\# tokens}}
\end{equation*}

\vspace{1ex}

The parser accuracy scores, including LAS and UAS, were computed by the
\texttt{eval.pl}\footnote{\url{http://ilk.uvt.nl/conll/software.html}} script
used in the CoNLL shared tasks. It also reports the precision and recall of
labeled and unlabeled attachment, which we use in our error analyses.

\section{Baseline}
It is common practice to compare the performance of PoS taggers to a
pre-computed baseline for an initial point of comparison.
%Generally, the baseline acts as a figure we expect the taggers to surpass.
%The baseline accuracy is usually the accuracy reached by a very simple,
%unsophisticated algorithm. We want to establish this baseline to which we can
%compare the results of other taggers in order to assess their performance.
For PoS tagging, a commonly used baseline is the Most Frequent Tag (MFT)
baseline, which we use in our experiments. This involves labeling each word
with the tag it was assigned most frequently in the training. For all unknown
words, i.e., words not seen in the training data, there are two main
approaches: assign it the most frequent tag overall in the training data, or
the tag most frequently assigned to words seen only once in the training data.
We adopted the latter approach for our baseline tagger, as we believe that
unknown words may have much in common with words that occur only once in the
training, more so than simply words of the most frequent part-of-speech. The
reason for this is that unknown and infrequent words have in common that they
rarely occur, and we might therefore expect them to have similar properties.

Norwegian is a synthetic language with a quite productive morphology, paving
the way for many closed compound words, e.g., \emph{trøffelhonningvinaigrette},
'truffle honey vinaigrette'. The possibility for these kinds of derivations
leads to the formation of many new words, and it is very likely that unseen
data contains many closed compound words, as well as proper nouns, that were
not seen in the training data. In the case of the initial tag sets, noun is the
most frequent tag for words occurring only once in the training; \texttt{subst}
`noun' for the coarse-grained tag set and \texttt{subst|prop} `proper noun' for
the fine-grained tag set. If we instead were to use the most frequent tag
overall as our baseline, \texttt{prep} `preposition' would be assigned to
previously unseen words when using the full tag set, mostly due to
\texttt{prep} having no morphological features, hence not being affected by the
inclusion of these.  As prepositions is a closed class, and we expect to see
all prepositions in the training data, this does not seem very promising, and
serves to indicate that our approach for unknown words may be the better
choice.

\section{PoS Tagger}
For our experiments with tag set modifications, we want a PoS tagger that is
both fast and accurate. There is often a trade-off between the two, as the best
taggers tend to suffer in terms of speed due to their complexity. However, a
tagger that achieves both close to state-of-the-art accuracy as well as very
high speed is TnT \cite{Bra:00}, which we introduced in Section
\ref{sssec:datadriven}. The fact that TnT was used for evaluating the universal
tag set \cite{Pet:Das:McD:12}, as described in Section \ref{ssec:ud}, served as
another good indication of TnT being appropriate for our task. The sum of these
factors led to TnT being the tagger of choice for our experiments.

\section{Syntactic Parser}
In choosing a parser for our experiments, we considered previous work on
dependency parsing of Norwegian, specifically that of the Norwegian Dependency
Treebank, as presented in \citeA{Sol:Skj:Ovr:14}. They evaluated a range of
dependency parsers, such as MaltParser \cite{Niv:Hal:Nil:07} and the Mate
parser \cite{Boh:10}, which we briefly described in Section
\ref{ssec:depparsers}. They reported that the Mate parser proved best in
parsing of NDT, where it achieved a labeled attachment score (LAS) of 90.41\%
and unlabeled attachment score (UAS) of 92.84\% for Bokmål. In comparison,
MaltParser with default settings reported 84.57\% and 88.02\% for LAS and UAS,
respectively, while reaching an LAS of 89.61\% and UAS of 91.96\% after a round
of optimization using the optimization tool MaltOptimizer \cite{Bal:Niv:12}.
Mate was consequently chosen as the parser for our experiments with tag set
modifications.

\section{Tags \& Features}
\label{sec:tagsandfeatures}
As we seek to quantify the effects of PoS tagging in a realistic setting, we
want to run the parser on automatically assigned PoS tags. For the training of
the parser, however, we have two options: using either gold standard or
automatically assigned tags. In order to settle on a configuration, we
conducted experiments with gold standard and automatically assigned tags to see
how they differ with respect to performance. The results of these experiments
are shown in Table \ref{initialtaggerparsereval}.  They reveal that the
combination of training and testing on automatic tags is superior to training
on gold standard tags and testing on automatic tags. This is rather surprising,
as one would expect gold standard tags to always be the preferred choice. This
motivates us to use automatically assigned tags both for training and testing
in our tag set experiments.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Training} & \textbf{Testing} & \textbf{LAS} & \textbf{UAS} \\
        \midrule
        Gold & Gold & 90.15\% & 92.51\% \\
        Gold & Auto & 85.68\% & 88.98\% \\
        Auto & Auto & 87.01\% & 90.19\% \\
        \bottomrule
    \end{tabular}
    \caption{Results from initial parsing experiments. \emph{Gold} denotes gold
        standard tags, \emph{Auto} denotes automatically assigned tags from
        TnT.}
    \label{initialtaggerparsereval}
\end{table}

Note that it is absolutely crucial that the morphological features in the
treebank (See column \emph{Features} in Table \ref{exmapping}) are
removed when using automatic tags, as they are still gold standard. For
instance, if a verb token is erroneously tagged as a noun, we could potentially
have a noun token with verbal features such as tense, which markedly obfuscates
the training and parsing. Another important factor is that we want to isolate
the effect of PoS tags, necessitating the exclusion of morphological features.

A similar approach was employed in the dependency parser comparison of
\citeA{Cho:Tet:Ste:15}, where they trained on automatically assigned PoS tags
and excluded any morphological features from the input data. They found Mate to
be the best parser for the English portion of the OntoNotes 5 corpus, beating a
wide range of contemporary state-of-the-art parsers.
%We are looking for relative differences between different tag set
%granularities, which we expect to be constant across different taggers and
%parsers.

\section{Pipeline}
With all the components in place for our experimental setup, the pipeline under
which we run each experiment is presented in Figure \ref{pipeline}. First, we
perform the mapping of the relevant tags in the data sets. We then train TnT on
the mapped training data and use the resulting model to tag the mapped
development data. Subsequently, we tag the training data with TnT and use the
resulting data to train Mate, which is then used to parse the tagged
development data.

\begin{figure}
    % define block styles
    \tikzstyle{block} = [draw, rectangle, fill=gray!25, text width=4em, text
    centered, minimum height=3em]
    \tikzstyle{line} = [draw, -latex']
    \tikzstyle{cyl} = [draw, cylinder, fill=gray!25, shape aspect=.4, text
    width=4em, text centered]

    \footnotesize
    \begin{tikzpicture}[node distance = 3cm, auto, font=\sffamily]
        % place nodes
        \node [cyl] (traindata) {NDT Training Data};
        \node [block, below of=traindata] (mapping) {Tag Set Mapping};
        \node [cyl, below of=mapping] (devdata) {NDT Dev Data};
        \node [block, right of=mapping] (taggertrain) {Tagger
            Training};
        \node [block, right of=taggertrain] (tagger) {Tagger};
        %\node [block, above of=tagger] (taggereval) {Tagger
            %Evaluator};
        \node [cyl, above of=taggertrain] (mappedtraindata) {Mapped Training Data};
        \node [cyl, below of=tagger] (mappeddevdata) {Mapped Dev Data};
        \node [cyl, above of=tagger] (taggedtraindata) {Tagged Training Data};
        \node [block, right of=tagger] (parsertrain) {Parser Training};
        \node [block, right of=parsertrain] (parser) {Parser};
        \node [cyl, below of=parsertrain] (taggeddevdata) {Tagged Dev Data};
        %\node [block, below of=parser] (eval) {Evaluator};
        % draw edges
        %\draw [->] (traindata) -- node[near start, below, fill=white] {label} (mapping);
        \path [line] (traindata) -- (mapping);
        \path [line] (devdata) -- (mapping);
        \path [line] (mapping) -- (mappeddevdata);
        \path [line] (mapping) -- (mappedtraindata);
        \path [line] (mappedtraindata) -- (taggertrain);
        \path [line] (mappeddevdata) -- (tagger);
        \path [line] (taggeddevdata) -- (parser);
        \path [line] (mappedtraindata) -- (tagger);
        \path [line] (taggertrain) -- (tagger);
        \path [line] (taggedtraindata) -- (parsertrain);
        \path [line] (tagger) -- (taggedtraindata);
        \path [line] (tagger) -- (taggeddevdata);
        \path [line] (parsertrain) -- (parser);
        %\path [line] (parser) -- (eval);
    \end{tikzpicture}
    \caption{Architecture of the experimental pipeline.}
    \label{pipeline}
\end{figure}

\chapter{Tag Set Optimization}
\label{chap:experiments}
It is now time to turn to our experiments with tag set modifications. In these
experiments, we will modify the tag set of the Norwegian Dependency Treebank in
various ways by taking advantage of the morphological features in the treebank.
They will be used in the creation of more fine-grained tags to quantify the
effects of PoS tag set granularity on parsing and identify an optimized tag set
for NDT, complemented by in-depth error analysis of tagging and parsing. The
experiments will be carried out for the five parts-of-speech for which there is
a range of morphological features in the treebank, namely determiners
(\texttt{det}), verbs (\texttt{verb}), nouns (\texttt{subst}), pronouns
(\texttt{pron}) and adjectives (\texttt{adj}). In the conclusion of this
chapter, we will present an optimized tag set for NDT along with results from
parsing with said tag set.

\section{Motivation}
As we discussed in Chapter \ref{chap:posandtagsets}, introducing more
fine-grained distinctions in a tag set may improve the tagger performance as
well as the performance of downstream applications such as syntactic parsers,
which is the focus of this work. With more fine-grained linguistically
motivated distinctions, we increase the linguistic information represented in
the tags, which may assist the tagger in disambiguating ambiguous and unknown
words, which in turn may aid the parser in recognizing and generalizing
syntactic patterns. However, the addition of more linguistic information to the
tags and thus a more fine-grained tag set will most likely lead to a drop in
tagger accuracy, due to the increase in complexity. The best tagging does not
necessarily lead to the best parse, and it is therefore interesting to
investigate how the tag set modifications may affect the interplay between
tagging and parsing. We seek to identify the most beneficial and informative
morphological features for syntactic parsing of NDT and append these to the
existing coarse-grained tags to create a new tag set tailored for dependency
parsing of Norwegian.

As we want to investigate how we can increase the linguistic quality of a tag
set, we will not introduce tag set modifications which are not linguistically
motivated. We will only consider distinctions we deem linguistically sensible,
even if we expect them to impair the computational tractability; investigating
the interplay between these two considerations is ultimately our goal.

\section{Baseline Experiments}
\label{sec:baselineexp}
In an initial round of experiments, we trained and evaluated TnT and Mate on
the training and development data, respectively, using the two initial tag sets
described in Section \ref{sec:inittagsets} (repeated in Table
\ref{repndttagsets} for convenience) to see how the tag set granularity would
affect the tagging and parsing performance. The \emph{original} tag set is the
existing tag set in NDT, while the \emph{full} tag set is created by
concatenating the PoS tag of each token with its set of morphological features.
In Table \ref{inittagseteval}, we report the results of these experiments. We
see that the tagger accuracy drastically drops when going from the original to
the full tag set. The MFT baseline for the original tag set is 94.14\%, while
it drops by almost 9 percentage points to 85.15\% for the full tag set. TnT
reports an accuracy of 97.47\% on the original tag set, which is reduced to
93.48\% for the full tag set. These results confirm our hypothesis that the
very high linguistic quality in the full tag set comes at the expense of
computational tractability in terms of tagger performance.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lr@{}}
        \toprule
        \textbf{Tag Set} & \textbf{\# Tags} \\
        \midrule
        Original & 19 \\
        Full & 368 \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the initial tag sets and their respective size.}
    \label{repndttagsets}
\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Tag Set} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        Original & \textbf{94.14\%} & \textbf{97.47\%} & 87.01\% & 90.19\% \\
        Full & 85.15\% & 93.48\% & \textbf{87.15\%} & \textbf{90.39\%} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation of tagging and parsing the development data with the two
        initial tag sets.}
    \label{inittagseteval}
\end{table}

However, the additional linguistic information provided by the full tag set
improves the parser performance. With the original tag set, Mate reports an LAS
of 87.01\% and a UAS of 90.19\%, which increases to 87.15\% and 90.39\%,
respectively, when using the full tag set. As we are looking for fine-grained
distinctions that improve the syntactic parsing, these results are promising
and serve to indicate that additional morphological information assists the
syntactic parsers, which will be further explored later in this chapter.

To assess what parts-of-speech are challenging for the tagger, we performed
error analysis of tagging with the original coarse tag set, presented in Table
\ref{baselinetagerror}. The frequency of the various PoS tags and dependency
relations in the following is reported per their frequency in the gold standard
development data.  We find that \texttt{ukjent} `unknown' is the most
challenging tag for the tagger, with a reported F-score of 72.73\% on the
development data. As this constitutes a highly disparate class containing
mostly foreign words, and is the second least frequent tag in the data, this is
not surprising. We see a similar trend for interjections, the least frequent
tag. On the other hand, the tagger obtains an F-score of more than 97\% for
infinitive markers, conjunctions, prepositions, nouns and verbs. Infinitive
markers and conjunctions are the two categories with an F-score exceeding 99\%.
Both these classes contain almost exclusively unambiguous tokens, which makes
it easy for the tagger to recognize them and successfully assign them the
correct tag.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lrrrr@{}}
        %\toprule
        %\textbf{Tag} & \textbf{Gold} & \textbf{Correct} & \textbf{System} &
        %\textbf{F-score} \\
        %\midrule
        %\texttt{adj} & 3144 & 2986 & 3114 & 95.43\% \\
        %\texttt{adv} & 1337 & 1296 & 1350 & 96.46\% \\
        %\texttt{det} & 2408 & 2291 & 2385 & 95.60\% \\
        %\texttt{inf-merke} & 531 & 530 & 532 & 99.72\% \\
        %\texttt{interj} & 35 & 23 & 25 & 76.67\% \\
        %\texttt{konj} & 1307 & 1296 & 1301 & 99.39\% \\
        %\texttt{prep} & 4878 & 4762 & 4841 & 97.99\% \\
        %\texttt{pron} & 2369 & 2306 & 2400 & 96.71\% \\
        %\texttt{sbu} & 1074 & 1009 & 1115 & 92.19\% \\
        %\texttt{subst} & 8944 & 8791 & 8999 & 97.99\% \\
        %\texttt{ukjent} & 51 & 40 & 59 & 72.73\% \\
        %\texttt{verb} & 5932 & 5759 & 5889 & 97.44\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tag error rates with the original tag set.}
    %\label{baselinetagerror}
%\end{table}

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lrrrr@{}}
        \toprule
        \textbf{Tag} &  \textbf{Freq} & \textbf{Precision} & \textbf{Recall} &
        \textbf{F-score} \\
        \midrule
        \texttt{adj} & 3144 & 95.89\% & 94.97\% & 95.43\% \\
        \texttt{adv} & 1337 & 96.00\% & 96.93\% & 96.46\% \\
        \texttt{det} & 2408 & 96.06\% & 95.14\% & 95.60\% \\
        \texttt{inf-merke} & 531 & 99.62\% & 99.81\% & 99.72\% \\
        \texttt{interj} & 35 & 92.00\% & 65.71\% & 76.67\% \\
        \texttt{konj} & 1307 & 99.62\% & 99.16\% & 99.39\% \\
        \texttt{prep} & 4878 & 98.37\% & 97.62\% & 97.99\% \\
        \texttt{pron} & 2369 & 96.08\% & 97.34\% & 96.71\% \\
        \texttt{sbu} & 1074 & 90.49\% & 93.95\% & 92.19\% \\
        \texttt{subst} & 8944 & 97.69\% & 98.29\% & 97.99\% \\
        \texttt{ukjent} & 51 & 67.80\% & 78.43\% & 72.73\% \\
        \texttt{verb} & 5932 & 97.79\% & 97.08\% & 97.44\% \\
        \bottomrule
    \end{tabular}
    \caption{Tagger performance with the original tag set.}
    \label{baselinetagerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lrrrrrr@{}}
        %\toprule
        %\textbf{Tag} &  \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{Precision} & \textbf{Recall} &
        %\textbf{F-score} \\
        %\midrule
        %\texttt{adj} & 3144 & 2986 & 3114 & 95.89\% & 94.97\% & 95.43\% \\
        %\texttt{adv} & 1337 & 1296 & 1350 & 96.00\% & 96.93\% & 96.46\% \\
        %\texttt{det} & 2408 & 2291 & 2385 & 96.06\% & 95.14\% & 95.60\% \\
        %\texttt{inf-merke} & 531 & 530 & 532 & 99.62\% & 99.81\% & 99.72\% \\
        %\texttt{interj} & 35 & 23 & 25 & 92.00\% & 65.71\% & 76.67\% \\
        %\texttt{konj} & 1307 & 1296 & 1301 & 99.62\% & 99.16\% & 99.39\% \\
        %\texttt{prep} & 4878 & 4762 & 4841 & 98.37\% & 97.62\% & 97.99\% \\
        %\texttt{pron} & 2369 & 2306 & 2400 & 96.08\% & 97.34\% & 96.71\% \\
        %\texttt{sbu} & 1074 & 1009 & 1115 & 90.49\% & 93.95\% & 92.19\% \\
        %\texttt{subst} & 8944 & 8791 & 8999 & 97.69\% & 98.29\% & 97.99\% \\
        %\texttt{ukjent} & 51 & 40 & 59 & 67.80\% & 78.43\% & 72.73\% \\
        %\texttt{verb} & 5932 & 5759 & 5889 & 97.79\% & 97.08\% & 97.44\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tag error rates with the original tag set.}
    %\label{baselinetagerror}
%\end{table}

In Table \ref{baselineparseerror}, we present the error analysis of parsing
with the original tag set. We observe similar patterns as for tagging, where
the most frequent dependency relations generally achieve the highest F-scores.
\textsc{det} (determiner), \textsc{finv} (finite verb), \textsc{infv}
(nonfinite verb), \textsc{ip} (sentence-separating punctuation), \textsc{konj}
(conjunction), \textsc{putfyll} (prepositional complement) and \textsc{sbu}
(complementizer) obtain an F-score exceeding 90\%.  Coordination
(\textsc{koord}) and adverbial (\textsc{adv}), albeit frequent, constitute
challenging constructions, with reported F-scores of 76.54\% and 80.06\%,
respectively. They are known to be notoriously difficult to parse due to the
structural ambiguity they often exhibit, which we will see more clearly in our
later experiments. An even more difficult type of coordination is found in
coordination with verbal ellipsis (\textsc{koord-ell}), which obtains an
F-score of mere 12.90\%. In coordination with verbal ellipsis, the verb in the
first conjunct is elided in the second conjunct, exemplified in Figure
\ref{exkoordell}, where \emph{er} `am' is implicitly the verb for the second
conjunct.  Other challenging constructions include apposition (\textsc{app}),
flat structure (\textsc{flat}), indirect object (\textsc{iobj}) and superfluous
word (\textsc{ukjent}). \textsc{flat} is assigned to constructions to which it
is not appropriate or possible to give a hierarchical, syntactic structure,
such as foreign quotes, proper nouns and other multi-words units; see Example
in Figure \ref{exflat}.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Deprel} & \textbf{Description} & \textbf{Freq} &
        \textbf{Precision} & \textbf{Recall} & \textbf{F-score} \\
        \midrule
        \textsc{adv} & Adverbial & 5101 & 79.40\% & 80.73\% & 80.06\% \\
        \textsc{app} & Apposition & 285 & 46.32\% & 49.07\% & 47.66\% \\
        \textsc{atr} & Attribute & 4010 & 83.82\% & 82.86\% & 83.34\% \\
        \textsc{det} & Determiner & 2435 & 93.14\% & 88.94\% & 90.99\% \\
        \textsc{dobj} & Direct object & 1982 & 87.64\% & 85.44\% & 86.53\% \\
        \textsc{finv} & Finite verb & 2097 & 96.90\% & 96.76\% & 96.83\% \\
        \textsc{flat} & Flat structure & 691 & 76.85\% & 76.40\% & 76.62\%\\
        \textsc{fobj} & Formal object & 10 & 40.00\% & 66.67\% & 50.00\% \\
        \textsc{fopred} & Free object predicative & 7 & 28.57\% & 50.00\% & 36.36\% \\
        \textsc{frag} & Fragment & 301 & 84.05\% & 84.05\% & 84.05\% \\
        \textsc{fspred} & Free subject predicative & 49 & 32.65\% & 40.00\% & 35.95\%\\
        \textsc{fsubj} & Formal subject & 360 & 85.56\% & 75.86\% & 80.42\% \\
        \textsc{ik} & Sentence-internal punctuation & 5 & 20.00\% & 100.00\% & 33.33\% \\
        \textsc{infv} & Nonfinite verb & 1761 & 95.29\% & 95.50\% & 95.39\% \\
        \textsc{interj} & Interjection & 34 & 55.88\% & 70.37\% & 62.29\% \\
        \textsc{iobj} & Indirect object & 74 & 63.51\% & 83.93\% & 72.31\% \\
        \textsc{ip} & Sentence-separating punctuation & 98 & 95.92\% & 95.92\% & 95.92\% \\
        \textsc{konj} & Conjunction & 1297 & 93.29\% & 93.36\% & 93.32\% \\
        \textsc{koord} & Coordination & 1344 & 76.71\% & 76.37\% & 76.54\% \\
        \textsc{koord-ell} & Coordination with verbal ellipsis & 33 & 12.12\% &
        13.79\% & 12.90\% \\
        \textsc{opred} & Object predicative & 87 & 44.83\% & 58.21\% & 50.65\% \\
        \textsc{par} & Parenthetical expression & 188 & 72.34\% & 86.62\% & 78.84\% \\
        \textsc{pobj} & Potential object & 7 & 42.86\% & 100.00\% & 60.00\% \\
        \textsc{psubj} & Potential subject & 205 & 65.85\% & 82.32\% & 73.17\% \\
        \textsc{putfull} & Prepositional complement & 4433 & 94.90\% & 94.39\% & 94.64\% \\
        \textsc{sbu} & Complementizer & 1071 & 95.99\% & 95.19\% & 95.59\% \\
        \textsc{spred} & Subject predicative & 1025 & 84.88\% & 84.06\% & 84.47\% \\
        \textsc{subj} & Subject & 3102 & 88.36\% & 89.94\% & 89.09\% \\
        \textsc{ukjent} & Superfluous words  & 15 & 0.00\% & 0.00\% & 0.00\% \\
        \bottomrule
    \end{tabular}
    \caption{Parser performance in terms of labeled attachment with the original tag
        set.}
    \label{baselineparseerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Deprel} & \textbf{Description} & \textbf{Gold} & \textbf{Correct}
        %& \textbf{System} & \textbf{F-score} \\
        %\midrule
        %\textsc{adv} & Adverbial & 5101 & 4050 & 5017 & 80.06\% \\
        %\textsc{app} & Apposition & 285 & 132 & 269 & 47.66\% \\
        %\textsc{atr} & Attribute & 4010 & 3361 & 4056 & 83.34\% \\
        %\textsc{det} & Determiner & 2435 & 2268 & 2550 & 90.99\% \\
        %\textsc{dobj} & Direct object & 1982 & 1737 & 2033 & 86.53\% \\
        %\textsc{finv} & Finite verb & 2097 & 2032 & 2100 & 96.83\% \\
        %\textsc{flat} & Flat structure & 691 & 531 & 695 & 76.62\%\\
        %\textsc{fobj} & Formal object & 10 & 4 & 6 & 50.00\% \\
        %\textsc{fopred} & Free object predicative & 7 & 2 & 4 & 36.36\% \\
        %\textsc{frag} & Fragment & 301 & 253 & 301 & 84.05\% \\
        %\textsc{fspred} & Free subject predicative & 49 & 16 & 40 & 35.95\%\\
        %\textsc{fsubj} & Formal subject & 49 & 16 & 40 & 35.95\% \\
        %\textsc{ik} & Sentence-internal punctuation & 5 & 1 & 1 & 33.33\% \\
        %\textsc{infv} & Nonfinite verb & 1761 & 1678 & 1757 & 95.39\% \\
        %\textsc{interj} & Interjection & 34 & 19 & 27 & 62.29\% \\
        %\textsc{iobj} & Indirect object & 74 & 47 & 56 & 72.31\% \\
        %\textsc{ip} & Sentence-separating punctuation & 98 & 94 & 98 & 95.92\% \\
        %\textsc{konj} & Conjunction & 1297 & 1210 & 1296 & 93.32\% \\
        %\textsc{koord} & Coordination & 1344 & 1031 & 1350 & 76.54\% \\
        %\textsc{koord-ell} & Coordination with verbal ellipsis & 33 & 4 & 29 & 12.90\% \\
        %\textsc{opred} & Object predicative & 87 & 39 & 67 & 50.65\% \\
        %\textsc{par} & Parenthetical expression & 188 & 136 & 157 & 78.84\% \\
        %\textsc{pobj} & Potential object & 7 & 3 & 3 & 60.00\% \\
        %\textsc{psubj} & Potential subject & 205 & 135 & 164 & 73.17\% \\
        %\textsc{putfull} & Prepositional complement & 4433 & 4207 & 4457 & 94.64\% \\
        %\textsc{sbu} & Complementizer & 1071 & 1028 & 1080 & 95.59\% \\
        %\textsc{spred} & Subject predicative & 1025 & 870 & 1035 & 84.47\% \\
        %\textsc{subj} & Subject & 3102 & 2741 & 3051 & 89.09\% \\
        %\textsc{ukjent} & Superfluous words  & 15 & 0 & 2 & 0.00\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Parser performance in terms of labeled attachment with the original tag
        %set.}
    %\label{baselineparseerror}
%\end{table}

\begin{figure}
    \begin{dependency}[theme = simple]
        \begin{deptext}[column sep=1em]
            Jeg \& er \& norsk \& i \& Norge \& , \& og \& fransk \& i \& Frankrike
            \& . \\
        \end{deptext}
        \deproot{2}{\larger{FINV}}
        \depedge{2}{1}{\larger{SUBJ}}
        \depedge[arc angle=10]{2}{3}{\larger{SPRED}}
        \depedge{2}{4}{\larger{ADV}}
        \depedge{4}{5}{\larger{PUTFYLL}}
        \depedge{2}{6}{\larger{IK}}
        \depedge{8}{7}{\larger{KONJ}}
        \depedge{2}{8}{\larger{\textbf{KOORD-ELL}}}
        \depedge{8}{9}{\larger{ADV}}
        \depedge{9}{10}{\larger{PUTFYLL}}
        \depedge{2}{11}{\larger{IP}}
    \end{dependency}
    \caption{Example of sentence with dependency relation \textsc{koord-ell}
        (coordination with verbal ellipsis).}
    \label{exkoordell}
\end{figure}

\begin{figure}
    \begin{dependency}[theme = simple]
        \begin{deptext}[column sep=1em]
            Pål \& Anders \& Ullevålseter \& deltar \& i \& årets \& Rally \&
            Dakar \& . \\
        \end{deptext}
        \deproot{4}{\larger{FINV}}
        \depedge{4}{1}{\larger{SUBJ}}
        \depedge[arc angle=50]{1}{2}{\larger{\textbf{FLAT}}}
        \depedge{1}{3}{\larger{\textbf{FLAT}}}
        \depedge{4}{5}{\larger{ADV}}
        \depedge[arc angle=50]{7}{6}{\larger{DET}}
        \depedge{5}{7}{\larger{PUTFYLL}}
        \depedge{7}{8}{\larger{\textbf{FLAT}}}
        \depedge{4}{9}{\larger{IP}}
    \end{dependency}
    \caption{Example of sentence with dependency relation \textsc{flat}
        (flat structure).}
    \label{exflat}
\end{figure}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lrrrr@{}}
        %\toprule
        %\textbf{Tag} &  \textbf{Frequency} & \textbf{Head Error} &
        %\textbf{Dep. Error} & \textbf{Both Wrong} \\
        %\midrule
        %\texttt{subst} & 8999 & 11\% & 12\% & 9\% \\
        %\texttt{verb} & 5889 & 8\% & 7\% & 5\% \\
        %\texttt{prep} & 4841 & 18\% & 15\% & 12\% \\
        %\texttt{adj} & 3114 & 8\% & 10\% & 6\% \\
        %\texttt{pron} & 2400 & 4\% & 10\% & 3\% \\
        %\texttt{det} & 2385 & 5\% & 5\% & 3\% \\
        %\texttt{adv} & 1350 & 13\% & 6\% & 4\% \\
        %\texttt{konj} & 1300 & 6\% & 0\% & 0\% \\
        %\texttt{sbu} & 1115 & 6\% & 6\% & 4\% \\
        %\texttt{inf-merke} & 532 & 6\% & 10\% & 5\% \\
        %\texttt{clb} & 98 & 4\% & 0\% & 0\% \\
        %\texttt{ukjent} & 59 & 32\% & 20\% & 15\% \\
        %\texttt{interj} & 25 & 24\% & 8\% & 8\% \\
        %\midrule
        %Total & 32107 & 10\% & 10\% & 7\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Error rates from parsing with the original tag set.}
    %\label{parsererrorrate}
%\end{table}

\section{Tag Set Experiments}
\label{sec:tagsetexperiments}
We will modify the tags for nouns, verbs, adjectives, determiners and pronouns
in NDT by appending selected sets of morphological features to each tag in
order to increase the linguistic information expressed by the tags. For each of
these categories, we will provide a brief recap of the linguistic
considerations as discussed in Section \ref{sec:posndt} and present the set of
available morphological features before going into how these features were used
in the tag set modifications and the results of tagging and parsing with these
modifications. Finally, we present error analysis of tagging and parsing with
the most promising tag set modification in terms of parser performance.

For each tag, we first experiment with each of the features in isolation before
employing various combinations of them. We base our choices of combinations on
how promising the features are and what we deem worth investigating in terms of
linguistic utility, in order to see how the features might interact.

To gain a better understanding of what constructions constitute challenges for
the parser, and similarly the most challenging categories for the tagger, we
perform error analysis of the most promising tag set modification for a
particular category in terms of parser performance (specifically, labeled
attachment score). Precision and recall for labeled attachment are reported by
the aforementioned \texttt{eval.pl} script (as used in the CoNLL shared tasks),
from which we calculate the F-score, which serves as the harmonic mean of
precision and recall. The corresponding precision, recall and F-score for
tagging are computed by our tailor-made tagger error analysis
script\footnote{See \url{https://github.com/petterhh/ndt-tools}}.

We do not perform statistical significance testing of the differences in parser
accuracy scores for all tag set modifications. Instead, we test for statistical
significance for the most successful tag set modification for each respective
category in Section \ref{sec:optimizedtagset}, where we combine the best tag
set modification for each category to a final, optimized tag set.

For the parser error analysis, we use frequency-weighted differences in
F-scores, where the difference in F-score from the baseline (i.e., the original
tag set) for a particular dependency relation is weighted by its relative
frequency in the gold standard development set. Weighting the differences by
relative frequency is crucial because improvements from baseline are more
significant for more frequent relations; the more frequent a relation is, the
greater the effect of said improvement. We report the five most improved
dependency relations.

The complete overview of the tag set modifications used in the experiments are
presented in Appendix \ref{chap:apptagsets}.

\subsection{Nouns}
\label{sec:expnouns}
In Norwegian, there is agreement in gender, definiteness and number between
nouns and their modifiers (adjectives and determiners), motivating us to
investigate how these properties interact in syntactic parsing. In addition to
gender, definiteness and number, nouns are marked with type\footnote{Note that
    we throughout our experiments with all five categories only consider main
    types when experimenting with the type feature.  Secondary types, i.e.,
    types that come in addition to the main type and serve as less
    linguistically informative, e.g., \texttt{fork} (abbreviation) and
    \texttt{høflig} (polite), are discarded. Norwegian, like many other
    Germanic languages, has separate forms for polite/formal possessives,
    indicated with capital first letter, e.g., \emph{Deres} `your(s)'. They
    are, however, practically nonexistent in current Norwegian, with only two
    occurrences in NDT.} and case in NDT. The features and their respective set
of values are presented in Table \ref{substfeatures}.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lp{5cm}p{5cm}@{}}
        \toprule
        \textbf{Feature} & \textbf{Values} & \textbf{Description} \\
        \midrule
        Definiteness & \texttt{be, ub} & Definite, indefinite \\
        Gender & \texttt{fem, mask, nøyt} & Feminine, masculine, neuter \\
        Number & \texttt{ent, fl} & Singular, plural \\
        Type & \texttt{appell, prop} & Common, proper \\
        Case & \texttt{gen} & Genitive \\
        \bottomrule
    \end{tabular}
    \caption{Overview of available morphological features for nouns.}
    \label{substfeatures}
\end{table}

The results from tagging and parsing with modifications to nouns are reported
in Table \ref{substresults}. As nouns constitute the largest class in the
treebank by far (as shown in Table \ref{baselinetagerror}), the effects are
greatest for noun tokens in terms of overall change in performance. Apart from
case, none of the tag set modifications improves the tagging. However, they all
give rise to increases in parser accuracy scores. Genitive case marks
possession, hence nouns marked with genitive case are quite different from
other nouns, taking a noun phrase as complement. Distinguishing on type is
useful and informative, as evident by the presence of separate tags for proper
and common nouns in many tag sets, such as those of PTB, SUC and UD (described
in Section \ref{sec:extagsets}). When introducing the distinction of type, we
see a large increase in parser accuracy scores, with an LAS of 88.07\% and UAS
of 91.11\%, both exceeding the baseline by more than a percentage point.
Definiteness is the most informative feature for parsing, achieving LAS of
88.27\% and UAS of 91.42\%

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Feature(s)} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        --- & \textbf{94.14\%} & 97.47\% & 87.01\% & 90.19\% \\
        Case & 93.77\% & \textbf{97.48\%} & 87.63\% & 90.72\% \\
        Definiteness & 89.67\% & 97.00\% & 88.27\% & 91.42\% \\
        Gender & 89.54\% & 96.09\% & 87.21\% & 90.36\% \\
        Number & 90.04\% & 96.37\% & 87.97\% & 91.00\% \\
        Type & 91.90\% & 96.92\% & 88.07\% & 91.11\% \\
        Case \& definiteness & 89.65\% & 97.03\% & 88.39\% & 91.44\% \\
        Type \& case & 91.73\% & 96.92\% & 88.46\% & 91.51\% \\
        Type \& definiteness & 89.65\% & 96.99\% & 88.44\% & 91.48\% \\
        Type \& number & 90.02\% & 96.37\% & 87.95\% & 90.95\% \\
        Type, case \& definiteness & 89.61\% & 97.05\% & \textbf{88.81\%} &
        \textbf{91.73\%} \\
        Type, definiteness \& number & 88.39\% & 96.46\% & 88.06\% & 91.12\%
        \\
        \bottomrule
    \end{tabular}
    \caption{Results of experiments with modified PoS tags for nouns.}
    \label{substresults}
\end{table}

We then combined the most promising tag set modifications to investigate how
they might interact and assist each other. The most successful combinations in
terms of LAS are case and definiteness (88.39\%), type and case (88.46\%), and
type and definiteness (88.44\%). This led us to combine all of them in a final
experiment, in which we reach an LAS of 88.81\%, improving upon the baseline by
1.8 percentage points, while also achieving the second highest tagger accuracy
out of all the experiments.

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Features} & \textbf{Tag} & \textbf{Freq} & \textbf{Precision} &
        \textbf{Recall} & \textbf{F-score} \\
        \midrule
        Baseline & \texttt{subst} & 8944 & 97.69\% & 98.29\% & 97.99\% \\
        \midrule
        \multirow{8}{*}{Type, case \& definiteness}
        & \texttt{subst|appell|ub} & 4185 & 96.73\% & 96.80\% & 96.76\% \\
        & \texttt{subst|appell|be} & 2193 & 94.94\% & 98.36\%  & 96.62\% \\
        & \texttt{subst|prop} & 2022 & 94.40\% & 92.58\% & 93.48\% \\
        & \texttt{subst|prop|gen} & 154 & 88.55\% & 95.45\% & 91.87\% \\
        & \texttt{subst|appell|be|gen} & 148 & 95.89\% & 94.59\% & 95.24\% \\
        & \texttt{subst|appell} & 128 & 92.92\% & 82.03\% & 87.14\% \\
        & \texttt{subst} & 89 & 96.39\% & 89.89\% & 93.02\% \\
        & \texttt{subst|appell|ub|gen} & 25 & 95.65\% & 88.00\% & 91.67\% \\
        \bottomrule
    \end{tabular}
    \caption{Tagger performance with the most promising tag set
        modification for nouns, namely type, case and definiteness.}
    \label{substtagerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature(s)} & \textbf{Tag} & \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{F-score} \\
        %\midrule
        %Baseline & \texttt{subst} & 8944 & 8791 & 8999 & 97.99\% \\
        %\midrule
        %\multirow{8}{*}{Type, case \& definiteness}
        %& \texttt{subst|appell|ub} & 4185 & 4051 & 4188 & 96.76\% \\
        %& \texttt{subst|appell|be} & 2193 & 2157 & 2272 & 96.62\% \\
        %& \texttt{subst|prop} & 2022 & 1872 & 1983 & 93.48\% \\
        %& \texttt{subst|prop|gen} & 154 & 147 & 166 & 91.87\% \\
        %& \texttt{subst|appell|be|gen} & 148 & 140 & 146 & 95.24\% \\
        %& \texttt{subst|appell} & 128 & 105 & 113 & 87.14\% \\
        %& \texttt{subst} & 89 & 80 & 83 & 93.02\% \\
        %& \texttt{subst|appell|ub|gen} & 25 & 22 & 23 & 91.67\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tagger performance with the most promising tag set
        %modification for nouns, namely type, case and definiteness.}
    %\label{substtagerror}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature(s)} & \textbf{Tag} & \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{F-score} \\
        %\midrule
        %Baseline & \texttt{subst} & 8944 & 8791 & 8999 & 97.99\% \\
        %\midrule
        %\multirow{8}{*}{Type, case \& definiteness}
        %& \texttt{subst|appell|ub} & 4185 & 4051 & 4188 & 96.76\% \\
        %& \texttt{subst|appell|be} & 2193 & 2157 & 2272 & 96.62\% \\
        %& \texttt{subst|prop} & 2022 & 1872 & 1983 & 93.48\% \\
        %& \texttt{subst|prop|gen} & 154 & 147 & 166 & 91.87\% \\
        %& \texttt{subst|appell|be|gen} & 148 & 140 & 146 & 95.24\% \\
        %& \texttt{subst|appell} & 128 & 105 & 113 & 87.14\% \\
        %& \texttt{subst} & 89 & 80 & 83 & 93.02\% \\
        %& \texttt{subst|appell|ub|gen} & 25 & 22 & 23 & 91.67\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tagger performance with the most promising tag set
        %modification for nouns, namely type, case and definiteness.}
    %\label{substtagerror}
%\end{table}

The tagger error analysis for the combination of type, case and definiteness is
shown in Table \ref{substtagerror}. We observe that common nouns without marked
definiteness are the most challenging for the tagger, while definite common
nouns and indefinite common nouns achieve the highest F-scores, both close to
97\%, coincidently also being the most frequent tags. Turning to the parser
error analysis in Table \ref{substparseerror}, we find that the dependency
relation \textsc{det} (determiner), occurring 2435 times in the gold standard
development data, benefits most from the tag set modification, with an increase
in F-score of more than 4 percentage points. \textsc{flat} (flat structure),
the second most improved dependency relation, is mostly assigned to multi-words
units such as proper nouns, and benefits greatly from additional information
about nouns, especially knowing whether a noun is common or proper, reflected
in the large improvement in parser accuracy scores.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Features} & \textbf{Deprel} & \textbf{Freq} &
        \textbf{Baseline} & \textbf{W/ feat} & \textbf{Diff} \\
        \midrule
        \multirow{5}{*}{Type, case \& definiteness}
        & \textsc{det} & 2435 & 90.99\% & 95.18\% & 0.3176 \\
        & \textsc{flat} & 691 & 76.62\% & 89.00\% & 0.2664 \\
        & \textsc{putfyll} & 4433 & 94.64\% & 96.04\% & 0.1938 \\
        & \textsc{subj} & 3102 & 89.09\% & 91.08\% & 0.1927 \\
        & \textsc{atr} & 4010 & 83.34\% & 84.83\% & 0.1855 \\
        \bottomrule
    \end{tabular}
    \caption{The five most improved dependency relations in terms of F-score,
        ranked by their weighted difference, for the most promising tag set
        modification for nouns, namely type, case and definiteness.}
    \label{substparseerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature Type} & \textbf{Tag} &  \textbf{Frequency} &
        %\textbf{Head Error} & \textbf{Dep. Error} & \textbf{Both Wrong} \\
        %\midrule
        %Baseline & \texttt{subst} & 8999 & 11\% & 12\% & 9\% \\
        %\midrule
        %\multirow{3}{*}{Definiteness}
        %& \texttt{subst|ub} & 4219 & 8\% & 9\% & 6\% \\
        %& \texttt{subst|be} & 2417 & 6\% & 9\% & 5\% \\
        %& \texttt{subst} & 2340 & 8\% & 10\% & 7\% \\
        %\midrule
        %\multirow{4}{*}{Type and def.}
        %& \texttt{subst|appell|ub} & 4209 & 8\% & 9\% & 6\% \\
        %& \texttt{subst|appell|be} & 2419 & 7\% & 8\% & 5\% \\
        %& \texttt{subst|prop} & 2152 & 7\% & 8\% & 6\% \\
        %& \texttt{subst} & 197 & 24\% & 25\% & 21\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Selected error rates from parsing with noun modifications.}
    %\label{nounerror}
%\end{table}

\subsection{Verbs}
Verbs in NDT may take on six different feature values, shown in Table
\ref{verbfeatures}. Note that both voice and mood have only a single value,
\texttt{pass} (passive) and \texttt{imp} (imperative), respectively. Verbs
without \texttt{pass} are implicitly active, and verbs which are not
imperative, are implicitly indicative.

%((The third grammatical mood in
%Norwegian, conjunctive, is basically non-existent in Norwegian, only seen in
%idioms (("faste vendinger"?)) in very formal language, expressing deontic
%modality.))

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lp{5cm}p{5cm}@{}}
        \toprule
        \textbf{Feature} & \textbf{Values} & \textbf{Description} \\
        \midrule
        Mood & \texttt{imp} & Imperative \\
        Tense & \texttt{inf, pres, pret, perf-part} & Infinitive, present,
        preterite, past perfect (participle) \\
        Voice & \texttt{pass} & Passive \\
        \bottomrule
    \end{tabular}
    \caption{Overview of available morphological features for verbs.}
    \label{verbfeatures}
\end{table}

Initially, we modified the tag set by introducing each of the features in
separate experiments, before trying out various combinations of these. For
combinations with voice and tense, we included both voice and tense in the tag,
as the tags need to be injective and all tokens with voice marked also have
tense marked.

%((We actually see an occurrence of a verb with
%\texttt{pret} and \texttt{pass} in the features, namely \emph{etterlystes}, but
%this is not normalized, as verbs in preterite can not be so-called
%\emph{s-passives}, hence an inaccuracy in the annotation.))

In an additional experiment, we mapped the verb tenses (mood, in the case of
imperative) to finiteness. Finite verbs are verbs that can serve as root in an
independent clause, i.e., verbs in imperative, present or preterite, which
nonfinite verbs, i.e., infinitive and past perfect (participle), cannot. This
distinction is syntactically grounded, and we might therefore expect it to
positively impact the syntactic parsing, as finite verbs and nonfinite verbs
appear in completely different syntactic constructions. All verbs have
finiteness, hence this distinction has broad coverage.

As noted in Section \ref{subsec:verb}, NDT does not have a separate tag for
auxiliary verbs. As these act quite differently from main verbs, which they
often precede, this distinction would be quite useful and interesting to
investigate in our experiments. However, NDT does not have features for
auxiliary verbs, so we would have to make use of the syntactic structure of the
verbs to implement this distinction. This is beyond the scope of this thesis,
hence not implemented. In the conversion of NDT to UD \cite{Ovr:Hoh:16}, the
tagger error analysis revealed that the \texttt{AUX} (auxiliary verb) tag of the
UD PoS tag set obtains an F-score of 91\%, indicating that the distinction
between auxiliary and main verbs is difficult for the tagger to make. As all
auxiliary verbs in Norwegian may also take the form of a main verb (from which
they are grammaticalized), this drop in accuracy is expected.

As Table \ref{verbresults} shows, the introduction of more fine-grained
distinctions rarely lead to an increase in tagger accuracy. In fact, none of
them improve upon the baseline; the original tag set achieves the highest
accuracy overall. This serves to indicate that more fine-grained distinctions
for verbs might not be beneficial for tagging Norwegian. However, more
fine-grained linguistically motivated distinctions yield better parses in many
cases. In three of the seven altered tag sets, namely for mood, mood and tense,
and the finiteness mapping, we see a rise in LAS and UAS.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Feature(s)} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        --- & \textbf{94.14\%} & \textbf{97.47\%} & 87.01\% & 90.19\% \\
        Mood & 94.12\% & 97.43\% & 87.04\% & 90.19\% \\
        Tense & 93.74\% & 97.30\% & 86.97\% & 90.18\% \\
        Voice & 94.13\% & 97.45\% & 86.96\% & 90.09\% \\
        Mood \& tense & 93.74\% & 97.31\% & 87.12\% & 90.31\% \\
        Voice \& tense & 93.74\% & 97.28\% & 86.99\% & 90.15\% \\
        Mood, tense \& voice & 93.74\% & 97.27\% & 86.83\% & 90.05\% \\
        Finiteness & 93.72\% & 97.35\% & \textbf{87.30\%} &
        \textbf{90.43\%} \\
        \bottomrule
    \end{tabular}
    \caption{Results of experiments with modified PoS tags for verbs.}
    \label{verbresults}
\end{table}

Imperative clauses are fundamentally different from indicative clauses, as they
lack an overt subject; they are implicitly addressed at the
reader(s)/listener(s). This is illustrated by the increase (albeit marginal) in
LAS when introducing the distinction of mood, from 87.01\% to 87.04\%, even if
mood (\texttt{imp}) accounts for only 23 tokens in the development data. The
combination of mood and tense leads to LAS of 87.12\% and UAS of 90.31\%, both
of which outperform the baseline. However, when combining voice, mood and tense
together, we get a drop in LAS and UAS, to 86.83\% and 90.05\%, respectively.

The mapping to finiteness proved to greatly improve the parsing, as we saw the
overall largest parser accuracy scores, with 87.30\% for LAS and 90.43\% for
UAS, 0.29 and 0.24 percentage points higher than the baseline, respectively.
This coincides with the observations seen for Swedish in \citeA{Ovr:08}, where
finiteness was found to be a very beneficial linguistic feature for parsing.
Looking at the parser error analysis in Table \ref{verbparseerror}, we see that
\textsc{finv} `finite verb' and \textsc{infv} `nonfinite verb' are among the
five most improved dependency relations, the others being \textsc{koord}
`coordination', \textsc{spred} `subject predicative' and \textsc{konj}
`conjunction'. The `law' of coordination of likes states that two elements can
be coordinated only if they are of the same syntactic category. This is
reflected in the improvement for finite and infinite verbs, as we generally
coordinate verbs with the same finiteness.  The error analysis from tagging
with marked finiteness in Table \ref{verbtagerror} shows that nonfinite verbs
are more challenging than finite verbs for the tagger, which could be caused by
them being less than half as frequent.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Feature} & \textbf{Tag} & \textbf{Freq} & \textbf{Precision} &
        \textbf{Recall} & \textbf{F-score} \\
        \midrule
        Baseline & \texttt{verb} & 5932 & 97.79\% & 97.08\% & 97.44\% \\
        \midrule
        \multirow{2}{*}{Finiteness}
        & \texttt{verb|fin} & 3999 & 97.93\% & 97.10\% & 97.51\% \\
        & \texttt{verb|infin} & 1933 & 93.05\% & 94.88\% & 93.95\% \\
        \bottomrule
    \end{tabular}
    \caption{Tagger performance with the most promising tag set
        modification for verbs, namely finiteness.}
    \label{verbtagerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature(s)} & \textbf{Tag} & \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{F-score} \\
        %\midrule
        %Baseline & \texttt{verb} & 5932 & 5759 & 5889 & 97.44\% \\
        %\midrule
        %\multirow{2}{*}{Finiteness}
        %& \texttt{verb|fin} & 3999 & 3883 & 3965 & 97.51\% \\
        %& \texttt{verb|infin} & 1933 & 1834 & 1971 & 93.95\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tagger performance with the most promising tag set
        %modification for verbs, namely finiteness.}
    %\label{verbtagerror}
%\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Feature} & \textbf{Deprel} & \textbf{Freq} &
        \textbf{Baseline} & \textbf{W/ feat} & \textbf{Diff} \\
        \midrule
        \multirow{5}{*}{Finiteness}
        & \textsc{koord} & 1344 & 76.54\% & 78.51\% & 0.0825 \\
        & \textsc{spred} & 1025 & 84.47\% & 85.56\% & 0.0348 \\
        & \textsc{finv} & 2097 & 96.83\% & 97.22\% & 0.0257 \\
        & \textsc{infv} & 1761 & 95.39\% & 95.80\% & 0.0223 \\
        & \textsc{konj} & 1297 & 93.32\% & 93.83\% & 0.0206 \\
        \bottomrule
    \end{tabular}
    \caption{The five most improved dependency relations in terms of F-score,
        ranked by their weighted difference, for the most promising tag set
        modification for verbs, namely finiteness.}
    \label{verbparseerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature Type} & \textbf{Tag} &  \textbf{Frequency} &
        %\textbf{Head Error} & \textbf{Dep. Error} & \textbf{Both Wrong} \\
        %\midrule
        %Baseline & \texttt{verb} & 5889 & 8\% & 7\% & 5\% \\
        %\midrule
        %\multirow{3}{*}{Voice \& tense}
        %& \texttt{verb} & 5744 & 8\% & 7\% & 5\% \\
        %& \texttt{verb|inf|pass} & 73 & 5\% & 8\% & 5\% \\
        %& \texttt{verb|pres|pass} & 69 & 14\% & 13\% & 9\% \\
        %\midrule
        %\multirow{5}{*}{Mood \& tense}
        %& \texttt{verb|pres} & 2968 & 9\% & 10\% & 5\% \\
        %& \texttt{verb|inf} & 1291 & 1\% & 2\% & 1\% \\
        %& \texttt{verb|pret} & 986 & 12\% & 9\% & 8\% \\
        %& \texttt{verb|perf-part} & 688 & 7\% & 15\% & 6\% \\
        %& \texttt{verb|imp} & 23 & 43\% & 39\% & 39\% \\
        %& \texttt{verb} & 1 & 100\% & 100\% & 100\% \\
        %\midrule
        %\multirow{3}{*}{Finiteness}
        %& \texttt{verb|fin} & 3821 & 9\% & 7\% & 6\% \\
        %& \texttt{verb|infin} & 1888 & 4\% & 7\% & 3\% \\
        %& \texttt{verb} & 222 & 10\% & 13\% & 9\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Selected error rates from parsing with verb modifications.}
    %\label{verberror}
%\end{table}

\subsection{Adjectives}
Adjectives are words that describe or modify other words, mostly nouns, by
assigning them certain attributes. In Norwegian, adjectives agree with the noun
they modify in gender, number and definiteness. In NDT, there are five
different features for adjectives, presented in Table \ref{adjfeatures}. Four
of the five types are bracket tags, which do not concern the morphological form
directly, with the last value being \texttt{fork} `abbreviation', none of which
serve as particularly linguistically informative.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lp{5cm}p{5cm}@{}}
        \toprule
        \textbf{Feature} & \textbf{Values} & \textbf{Description} \\
        \midrule
        Definiteness & \texttt{be, ub} & Definite, indefinite \\
        Degree & \texttt{komp, pos, sup} & Comparative, positive, superlative \\
        Gender & \texttt{m/f, nøyt} & Masculine/feminine, neuter \\
        Number & \texttt{ent, fl} & Singular, plural \\
        Type & \texttt{<adv>, <ordenstall>, <perf-part>, <pres-part>, fork} &
        Adverb, ordinal number, past participle, present participle,
        abbreviation \\
        \bottomrule
    \end{tabular}
    \caption{Overview of available morphological features for adjectives.}
    \label{adjfeatures}
\end{table}

The tagging and parsing results in Table \ref{adjresults} show that none of the
tag set modifications lead to improvements in tagger accuracy. Degree and type
are the most promising features in terms for tagger performance, with reported
accuracy of 97.41\% and 97.40\%, respectively. All but two tag set
modifications outperform the baseline parser accuracy scores, the most
successful being degree, with a reported LAS of 87.29\% and UAS of 90.44\%,
improvements of 0.28 and 0.25 percentage points, respectively, from the
baseline. Other promising features include definiteness and type.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Feature(s)} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        --- & \textbf{94.14\%} & \textbf{97.47\%} & 87.01\% & 90.19\% \\
        Definiteness & 93.45\% & 96.84\% & 87.14\% & 90.29\% \\
        Degree & 94.13\% & 97.41\% & \textbf{87.29\%} & \textbf{90.44\%} \\
        Gender & 93.56\% & 96.89\% & 87.10\% & 90.25\% \\
        Number & 93.51\% & 96.71\% & 86.99\% & 90.10\% \\
        Type & 94.12\% & 97.40\% & 87.11\% & 90.25\% \\
        Definiteness \& degree & 93.45\% & 96.81\% & 87.23\% & 90.39\% \\
        Definiteness \& gender & 92.94\% & 96.31\% & 87.18\% & 90.39\% \\
        Definiteness \& number & 93.48\% & 96.78\% & 87.27\% &
        \textbf{90.44\%} \\
        Degree \& gender & 93.56\% & 96.87\% & 87.00\% & 90.16\% \\
        Degree \& number & 93.49\% & 96.76\% & 87.13\% & 90.26\% \\
        Definiteness, degree \& number & 93.47\% & 96.81\% & 87.14\% & 90.30\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of experiments with modified PoS tags for adjectives.}
    \label{adjresults}
\end{table}

Turning to combinations of features, definiteness and number achieve the best
results, very close to that of degree, with 0.02 percentage points lower LAS
and identical UAS. Adjectives agree with their head noun and determiner in
definiteness and number, making this an expected improvement. The combination
of definiteness and degree is also quite promising, obtaining LAS of 87.23\%
and UAS of 90.39\%. It is interesting that none of the combinations surpass the
experiment with degree alone, which indicates that degree does not interact
with the other features in any syntactically significant way.

The tagger evaluation in Table \ref{adjtagerror} reveals that the 233
tokens without marked degree constitute a challenging group for the tagger, as
the tagger obtains an accuracy of mere 72.53\%. For tokens with degree marked,
superlative is the least challenging, with accuracy exceeding 99\%.
Comparative comes in second with 97.78\%, while the most frequent degree,
positive, reaches an accuracy of 95.68\%. The parser error analysis in Table
\ref{adjparseerror} shows that adverbials, for which there are 5101 occurrences
in the gold standard development data, gain most from the distinction of
degree, with an increase in F-score from 80.06\% to 80.82\%. As noted in
Section \ref{sec:baselineexp}, adverbials are often difficult to parse, mainly
due to structural ambiguity arising with the problem of PP attachment.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Feature} & \textbf{Tag} & \textbf{Freq} & \textbf{Precision} &
        \textbf{Recall} & \textbf{F-score} \\
        \midrule
        Baseline & \texttt{adj} & 3144 & 95.89\% & 94.97\% & 95.43\% \\
        \midrule
        \multirow{4}{*}{Degree}
        & \texttt{adj|pos} & 2485 & 96.44\% & 94.93\% & 95.68\% \\
        & \texttt{adj|komp} & 273 & 98.88\% & 96.70\% & 97.78\% \\
        & \texttt{adj|sup} & 153 & 99.34\% & 98.69\% & 99.02\% \\
        & \texttt{adj} & 233 & 72.53\% & 72.53\% & 72.53\% \\
        \bottomrule
    \end{tabular}
    \caption{Tagger performance with the most promising tag set
        modification for adjectives, namely degree.}
    \label{adjtagerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature(s)} & \textbf{Tag} & \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{F-score} \\
        %\midrule
        %Baseline & \texttt{adj} & 3144 & 2986 & 3114 & 95.43\% \\
        %\midrule
        %\multirow{4}{*}{Degree}
        %& \texttt{adj|pos} & 2485 & 2359 & 2446 & 95.68\% \\
        %& \texttt{adj|komp} & 273 & 264 & 267 & 97.78\% \\
        %& \texttt{adj|sup} & 153 & 151 & 152 & 99.02\% \\
        %& \texttt{adj} & 233 & 169 & 233 & 72.53\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tagger performance with the most promising tag set
        %modification for adjectives, namely degree.}
    %\label{adjtagerror}
%\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Feature} & \textbf{Deprel} & \textbf{Freq} &
        \textbf{Baseline} & \textbf{W/ feat} & \textbf{Diff} \\
        \midrule
        \multirow{5}{*}{Degree}
        & \textsc{adv} & 5101 & 80.06\% & 80.82\% & 0.1208 \\
        & \textsc{flat} & 691 & 76.62\% & 78.62\% & 0.0430 \\
        & \textsc{dobj} & 1982 & 86.53\% & 87.19\% & 0.0409 \\
        & \textsc{koord} & 1344 & 76.54\% & 77.25\% & 0.0299 \\
        & \textsc{infv} & 1761 & 95.39\% & 95.78\% & 0.0214 \\
        \bottomrule
    \end{tabular}
    \caption{The five most improved dependency relations in terms of F-score,
        ranked by their weighted difference, for the most promising tag set
        modification for adjectives, namely degree.}
    \label{adjparseerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature Type} & \textbf{Tag} &  \textbf{Frequency} &
        %\textbf{Head Error} & \textbf{Dep. Error} & \textbf{Both Wrong} \\
        %\midrule
        %Baseline & \texttt{adj} & 3114 & 8\% & 10\% & 6\% \\
        %\midrule
        %\multirow{3}{*}{Definiteness}
        %& \texttt{adj|ub} & 1640 & 9\% & 12\% & 6\% \\
        %& \texttt{adj} & 970 & 8\% & 9\% & 6\% \\
        %& \texttt{adj|be} & 498 & 5\% & 4\% & 3\% \\
        %\midrule
        %\multirow{3}{*}{Gender}
        %& \texttt{adj} & 1535 & 7\% & 8\% & 5\% \\
        %& \texttt{adj|nøyt} & 864 & 10\% & 12\% & 6\% \\
        %& \texttt{adj|m/f} & 699 & 7\% & 10\% & 5\% \\
        %\midrule
        %\multirow{6}{*}{Type}
        %& \texttt{adj} & 2777 & 8\% & 9\% & 6\% \\
        %& \texttt{adj|<perf-part>} & 118 & 13\% & 21\% & 9\% \\
        %& \texttt{adj|<ordenstall>} & 97 & 4\% & 4\% & 2\% \\
        %& \texttt{adj|<pres-part>} & 79 & 11\% & 15\% & 9\% \\
        %& \texttt{adj|<adv>} & 35 & 20\% & 3\% & 0\% \\
        %& \texttt{adj|fork} & 3 & 33\% & 33\% & 33\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Selected error rates from parsing with adj modifications.}
    %\label{adjerror}
%\end{table}

\subsection{Determiners}
Determiners occur with a noun or noun phrase, of which they determine or
specify the reference. In Norwegian, determiners agree with their head noun in
gender, number and definiteness. NDT operates with four features for
determiners, viz., type, gender, definiteness and number. In Table
\ref{detfeatures}, these features and their respective set of available values
are presented.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lp{5cm}p{5cm}@{}}
        \toprule
        \textbf{Feature} & \textbf{Values} & \textbf{Description}\\
        \midrule
        Definiteness & \texttt{be, ub} & Definite, indefinite \\
        Gender & \texttt{fem, mask, nøyt} & Feminine, masculine, neuter \\
        Number & \texttt{ent, fl} & Singular, plural \\
        Type & \texttt{dem, forst, kvant, poss, sp} & Demonstrative, amplifier,
        quantifier, possessive, interrogative \\
        \bottomrule
    \end{tabular}
    \caption{Overview of available morphological features for determiners.}
    \label{detfeatures}
\end{table}

For a number of reasons, we do not report results from combinations of features
for determiners, as the features could not be combined in any meaningful way.
For instance, determiners in plural never have marked definiteness, hence this
combination would only apply to determiners in singular and roughly correspond
to the distinction on definiteness alone. Moreover, determiners in plural are
not distinguished in terms of gender (thus ruling out the combination of number
and gender), neither are definite determiners. Another aspect taken into
consideration is that the most promising distinction, definiteness, applies to
such a small number of tokens that more fine-grained distinctions would be
overly sparse.

%For the experiments with type, we excluded \texttt{res}, \texttt{høflig} and
%\texttt{<adj>}, as these are merely secondary types. Furthermore, \texttt{res}
%only applies to \emph{hverandres} `each other's', and we generally avoid tags
%that apply only to a single word, while \texttt{høflig} does not provide
%relevant morphological or syntactic information

The results from the experiments with determiners are shown in Table
\ref{detresults}.  Introducing the type led to an increase in tagger accuracy
by 0.14 percentage points to 97.61\%, while marginally impacting the parsing,
with LAS of 87.00\%, 0.01 percentage points below the baseline, and UAS of
90.11\%, 0.08 percentage points below the baseline. The fact that the baseline
remains the same is especially worth noting, as increased tag set granularity
almost inevitably leads to a drop in the MFT baseline. The increase in tagger
accuracy when introducing the distinction of type is noteworthy, as we expected
the finer granularity to lead to a decrease in accuracy. This serves to
indicate that more fine-grained distinctions for determiners, which is a quite
disparate category in the treebank, may be quite useful for tagging. However,
as it has negative impact on the syntactic parsing, we can conclude that the
type of a determiner does not assist in generalizing syntactic patterns, as
most determiners, regardless of type, appear in the same syntactic
constructions (i.e., before a noun or noun phrase).

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Feature} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        --- & \textbf{94.14\%} & 97.47\% & 87.01\% & 90.19\% \\
        Definiteness & 94.13\% & 97.49\% & \textbf{87.30\%} &
        \textbf{90.42\%} \\
        Gender & 93.86\% & 97.28\% & 87.09\% & 90.31\% \\
        Number & 94.06\% & 97.49\% & 87.04\% & 90.18\% \\
        Type & 94.14\% & \textbf{97.61\%} & 87.00\% & 90.11\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of experiments with modified PoS tags for determiners.}
    \label{detresults}
\end{table}

Gender, on the other hand, improved the parsing, but complicated the tagging,
as the various genders are often difficult to differentiate, especially so in
the case of masculine and feminine, which share many of the same determiners.
The number of a determiner, i.e., singular or plural, led to a small increase
in tagger accuracy and LAS, while marginally lower UAS, 90.18\%, 0.01
percentage points lower than that of the original tag set. Almost all
determiners have number, and the introduction of this distinction led to small
increases in tagger accuracy and LAS, but marginally lower UAS. The
introduction of definiteness to the determiners led to the best parsing
results, LAS of 87.30\% and UAS of 90.42\%, while also increasing the tagger
accuracy slightly. The increase in LAS and UAS is rather interesting, as Table
\ref{dettagerror} shows that this change applies to only 121 tokens. As this
accounts for a very small number of tokens, coupled with the previously noted
considerations, we did not consider further fine-grained modifications with
definiteness. This goes to show that tokens with overt definiteness have
noticeable impact on the syntactic parsing, and that distinguishing on
definiteness is very beneficial.

When we look at the tagger performance for definiteness in Table
\ref{dettagerror}, we see that indefinite determiners achieve an F-score of
100\%, while definite determiners reach 96.08\% and the remaining determiners
without marked definiteness obtain an F-score of 95.56\%. The tagger perfectly
tags the indefinite determiners, which constitutes a quite closed class, with
only four tokens, viz., \emph{egen} (amplifier `own', feminine/masculine),
\emph{eget} (amplifier `own', neuter), \emph{annen} (demonstrative `other',
feminine/masculine) and \emph{annet} (demonstrative `other', neuter), all
singular. Comparing the results to the baseline, we see a decrease of 0.04
percentage points for the `base' tag \texttt{det}.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Feature} & \textbf{Tag} & \textbf{Freq} & \textbf{Precision} &
        \textbf{Recall} & \textbf{F-score} \\
        \midrule
        Baseline & \texttt{det} & 2408 & 96.06\% & 95.14\% & 95.60\% \\
        \midrule
        \multirow{3}{*}{Definiteness}
        & \texttt{det} & 2287 & 95.99\% & 95.15\% & 95.56\% \\
        & \texttt{det|ub} & 72 & 100.00\% & 100.00\% & 100.00\% \\
        & \texttt{det|be} & 49 & 92.45\% & 100.00\% & 96.08\% \\
        \bottomrule
    \end{tabular}
    \caption{Tagger performance with the most promising tag set
        modification for determiners, namely definiteness.}
    \label{dettagerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature} & \textbf{Tag} & \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{F-score} \\
        %\midrule
        %Baseline & \texttt{det} & 2408 & 2291 & 2385 & 95.60\% \\
        %\midrule
        %\multirow{3}{*}{Definiteness}
        %& \texttt{det} & 2287 & 2176 & 2267 & 95.56\% \\
        %& \texttt{det|ub} & 72 & 72 & 72 & 100\% \\
        %& \texttt{det|be} & 49 & 49 & 53 & 96.08\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tagger performance with the most promising tag set
        %modification for determiners, namely definiteness.}
    %\label{dettagerror}
%\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Feature} & \textbf{Deprel} & \textbf{Freq} &
        \textbf{Baseline} & \textbf{W/ feat} & \textbf{Diff} \\
        \midrule
        \multirow{5}{*}{Definiteness}
        & \textsc{adv} & 1597 & 80.06\% & 80.71\% & 0.1034 \\
        & \textsc{dobj} & 1982 & 86.53\% & 87.32\% & 0.0485 \\
        & \textsc{koord} & 1344 & 76.54\% & 77.60\% & 0.0443  \\
        & \textsc{konj} & 1297 & 93.32\% & 93.90\% & 0.0236 \\
        & \textsc{subj} & 3102 & 89.09\% & 89.33\% & 0.0230 \\
        \bottomrule
    \end{tabular}
    \caption{The five most improved dependency relations in terms of F-score,
        ranked by their weighted difference, for the most promising tag set
        modification for determiners, namely definiteness.}
    \label{detparseerror}
\end{table}

The error analysis of parsing with definiteness for determiners is shown in
Table \ref{detparseerror}, and it is evident that adverbials (\textsc{adv})
benefit the most from the distinction of definiteness, together with
\textsc{dobj} `direct object', \textsc{koord} `coordination', \textsc{konj}
`conjunction' and \textsc{subj} `subject'. Definiteness is known to be a
distinguishing property for syntactic arguments, as noted by \citeA{Cro:03},
who found a cross-lingual tendency for subjects to be definite and objects to
be indefinite.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature Type} & \textbf{Tag} &  \textbf{Frequency} &
        %\textbf{Head Error} & \textbf{Dep. Error} & \textbf{Both Wrong} \\
        %\midrule
        %Baseline & \texttt{det} & 2385 & 5\% & 5\% & 3\% \\
        %\midrule
        %\multirow{6}{*}{Type}
        %& \texttt{det|kvant} & 1407 & 5\% & 6\% & 4\% \\
        %& \texttt{det|dem} & 673 & 6\% & 4\% & 3\% \\
        %& \texttt{det|poss} & 263 & 6\% & 4\% & 4\% \\
        %& \texttt{det|forst} & 62 & 5\% & 5\% & 5\% \\
        %& \texttt{det|sp} & 12 & 8\% & 8\% & 8\% \\
        %& \texttt{det} & 3 & 0\% & 0\% & 0\% \\
        %\midrule
        %\multirow{4}{*}{Gender}
        %& \texttt{det|mask} & 946 & 5\% & 3\% & 3\% \\
        %& \texttt{det} & 928 & 6\% & 7\% & 4\% \\
        %& \texttt{det|nøyt} & 501 & 5\% & 3\% & 3\% \\
        %& \texttt{det|fem} & 23 & 4\% & 4\% & 4\% \\
        %\midrule
        %\multirow{3}{*}{Definiteness}
        %& \texttt{det} & 2267 & 6\% & 5\% & 4\% \\
        %& \texttt{det|ub} & 72 & 3\% & 3\% & 3\% \\
        %& \texttt{det|be} & 53 & 4\% & 6\% & 4\% \\
        %\midrule
        %\multirow{3}{*}{Number}
        %& \texttt{det|ent} & 1502 & 5\% & 4\% & 3\% \\
        %& \texttt{det} & 71 & 8\% & 11\% & 8\% \\
        %& \texttt{det|fl} & 52 & 6\% & 7\% & 4\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Selected error rates from parsing with determiner modifications.}
    %\label{deterror}
%\end{table}

\subsection{Pronouns}
Pronouns include personal, reflexive, reciprocal and interrogative pronouns in
NDT. Pronouns can be assigned five features, i.e., case, gender, number, person
and type, with their respective values shown in Table \ref{pronfeatures}. As
possessive pronouns are assigned to the \texttt{det} class, they are not
discussed here.
%The single pronoun with the \texttt{poss} feature is the personal possessive
%interrogative \emph{hvis} `whose'.  For experiments with type of pronouns, we
%excluded \texttt{poss}, \texttt{hum}, \texttt{sp} and \texttt{høflig}, as
%these are merely secondary types, further specifying the pronoun in addition
%to their main type.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lp{5cm}p{5cm}@{}}
        \toprule
        \textbf{Feature} & \textbf{Values} & \textbf{Description} \\
        \midrule
        Case & \texttt{akk, nom} & Accusative, nominative \\
        Gender & \texttt{fem, fem|mask, mask, nøyt} & Feminine,
        feminine/masculine, masculine, neuter \\
        Number & \texttt{ent, fl} & Singular, plural \\
        Person & \texttt{1, 2, 3} & 1st, 2nd, 3rd \\
        Type & \texttt{pers, refl, res, sp} & Personal, reflexive, reciprocal,
        interrogative \\
        \bottomrule
    \end{tabular}
    \caption{Overview of available morphological features for pronouns.}
    \label{pronfeatures}
\end{table}

The results in Table \ref{pronresults} show that number, person and type are
the most informative features for parsing, with LAS of 87.21\%, 87.22\% and
87.19\%, respectively. However, when combining number and person, we observe a
drop by more than 0.2 percentage points, indicating that these features do not
interact in any syntactically distinctive way. The most interesting observation
is that all experiments exceed the baseline tagger accuracy, the most improved
being the most fine-grained distinction, namely type, case and number combined,
obtaining a tagger accuracy of 97.52\%. This shows that the introduction of
more fine-grained distinctions for pronouns is beneficial and aids the PoS
tagger in disambiguating ambiguous words. While case alone yields an LAS of
87.08\%, we found that the combination of type and case, which is the most
successful experiment in terms of parser performance, yields the second highest
tagging accuracy of 97.51\%. The reason for this is that pronouns of different
type and personal pronouns of different case exhibit quite different properties
and appear in different constructions. Pronouns in nominative case (i.e.,
subjects) primarily occur before the main verb, while pronouns in accusative
case (i.e., objects) occur after the main verb, as Norwegian exhibits so-called
V2 word order, requiring that the finite verb of a declarative clause appears
in the second position, hence its name. The combination of type and number
comes in close to the performance of type and case, with LAS of 87.27\% and UAS
identical to that of type and case.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Feature(s)} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        --- & \textbf{94.14\%} & 97.47\% & 87.01\% & 90.19\% \\
        Case & 94.12\% & 97.50\% & 87.08\% & 90.21\% \\
        Gender & 94.13\% & 97.48\% & 87.06\% & 90.23\% \\
        Number & 94.13\% & 97.49\% & 87.21\% & 90.33\% \\
        Person & 94.14\% & 97.49\% & 87.22\% & 90.32\% \\
        Type & 94.14\% & 97.48\% & 87.19\% & 90.40\% \\
        Number \& person & 94.13\% & 97.49\% & 96.98\% & 90.16\% \\
        Type \& case & 94.12\% & 97.51\% & \textbf{87.30\%} &
        \textbf{90.41\%} \\
        Type \& number & 94.13\% & 97.49\% & 87.27\% & \textbf{90.41\%} \\
        Type \& person & 94.14\% & 97.49\% & 87.00\% & 90.14\% \\
        Type, case \& number & 94.12\% & \textbf{97.52\%} & 87.11\% & 90.36\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of experiments with modified PoS tags for pronouns.}
    \label{pronresults}
\end{table}

Gender did not have notable impact on the parsing, which is not surprising,
seeing as the various genders have roughly equal properties. There is no
agreement between pronouns and verbs in neither gender, number nor person in
Norwegian, unlike English, for instance, where there is agreement in number and
person (e.g., \emph{I am} vs. \emph{You are} vs. \emph{He is}). This
agreement is the reason for Penn Treebank having separate tags for verbs in
third person singular present (\texttt{VBZ}), non-third person singular present
(\texttt{VBP}), etc., as we saw in Chapter \ref{chap:posandtagsets}.

Turning to the tagger error analysis in Table \ref{prontagerror}, we see that
reflexive and reciprocal pronouns are tagged with an F-score of 100\%. The
reason for this is that \emph{seg} is unambiguously the only reflexive pronoun
in Norwegian, and \emph{hverandre} `each other' is unambiguously the only
reciprocal pronoun, which simplifies the tagging, even though they are
infrequent. Interrogative pronouns (\texttt{pron|sp}) receive an F-score of
99.19\%, while personal pronouns without marked case is the most challenging
class for the tagger, with a reported F-score of 92.13\%. Personal pronouns in
nominative or accusative reach an F-score exceeding 98\%.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Features} & \textbf{Tag} & \textbf{Freq} & \textbf{Precision} &
        \textbf{Recall} & \textbf{F-score} \\
        \midrule
        Baseline & \texttt{pron} & 2369 & 96.08\% & 97.34\% & 96.71\% \\
        \midrule
        \multirow{7}{*}{Type \& case}
        & \texttt{pron|pers|nom} & 1121 & 97.38\% & 99.55\% & 98.46\% \\
        & \texttt{pron|pers} & 825 & 91.31\% & 92.97\% & 92.13\% \\
        & \texttt{pron|pers|akk} & 203 & 100.00\% & 97.54\% & 98.75\% \\
        & \texttt{pron|refl} & 143 & 100.00\% & 100.00\% & 100.00\% \\
        & \texttt{pron|sp} & 62 & 100.00\% & 98.39\% & 99.19\% \\
        & \texttt{pron} & 9 & 100.00\% & 88.89\% & 94.12\% \\
        & \texttt{pron|res} & 6 & 100.00\% & 100.00\% & 100.00\% \\
        \bottomrule
    \end{tabular}
    \caption{Tagger performance with the most promising tag set
        modification for pronouns, namely type and case.}
    \label{prontagerror}
\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature(s)} & \textbf{Tag} & \textbf{Gold} & \textbf{Correct} &
        %\textbf{System} & \textbf{F-score} \\
        %\midrule
        %Baseline & \texttt{pron} & 2369 & 2306 & 2400 & 96.71\% \\
        %\midrule
        %\multirow{7}{*}{Type \& case}
        %& \texttt{pron|pers|nom} & 1121 & 1116 & 1146 & 98.46\% \\
        %& \texttt{pron|pers} & 825 & 767 & 840 & 92.13\% \\
        %& \texttt{pron|pers|akk} & 203 & 198 & 198 & 98.75\% \\
        %& \texttt{pron|refl} & 143 & 143 & 143 & 100\% \\
        %& \texttt{pron|sp} & 62 & 61 & 61 & 99.19\% \\
        %& \texttt{pron} & 9 & 8 & 8 & 94.12\% \\
        %& \texttt{pron|res} & 6 & 6 & 6 & 100\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Tagger performance with the most promising tag set
        %modification for pronouns, namely type and case.}
    %\label{prontagerror}
%\end{table}

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrrrr@{}}
        \toprule
        \textbf{Features} & \textbf{Deprel} & \textbf{Freq} &
        \textbf{Baseline} & \textbf{W/ feat} & \textbf{Diff} \\
        \midrule
        \multirow{5}{*}{Type \& case}
        & \textsc{adv} & 5101 & 80.05\% & 80.46\% & 0.0632 \\
        & \textsc{koord} & 1344 & 76.54\% & 77.70\% & 0.0485 \\
        & \textsc{subj} & 3102 & 89.09\% & 89.47\% & 0.0369 \\
        & \textsc{atr} & 4010 & 83.34\% & 83.59\% & 0.0318 \\
        & \textsc{putfyll} & 4433 & 94.64\% & 94.81\% & 0.0241\\
        \bottomrule
    \end{tabular}
    \caption{The five most improved dependency relations in terms of F-score,
        ranked by their weighted difference, for the most promising tag set
        modification for pronouns, namely type and case.}
    \label{pronparseerror}
\end{table}

The parser error analysis in Table \ref{pronparseerror} shows that adverbial
(\textsc{adv}), coordination (\textsc{koord}), subject (\textsc{subj}),
attribute (\textsc{atr}) and prepositional complement (\textsc{putfyll}) are
the dependency relations benefiting the most from the distinction of type and
case. Adverbials and coordination are the most improved, just as for
determiners and adjectives. As they are notoriously challenging for parsers, as
seen in Section \ref{sec:baselineexp}, the additional linguistic information
greatly assists the parser in parsing these constructions.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}llrrrr@{}}
        %\toprule
        %\textbf{Feature Type} & \textbf{Tag} &  \textbf{Frequency} &
        %\textbf{Head Error} & \textbf{Dep. Error} & \textbf{Both Wrong} \\
        %\midrule
        %Baseline & \texttt{pron} & 2400 & 4\% & 10\% & 3\% \\
        %\midrule
        %\multirow{5}{*}{Type}
        %& \texttt{pron|pers} & 2179 & 4\% & 10\% & 2\% \\
        %& \texttt{pron|refl} & 143 & 1\% & 10\% & 1\% \\
        %& \texttt{pron|sp} & 61 & 21\% & 20\% & 13\% \\
        %& \texttt{pron} & 8 & 0\% & 12\% & 0\% \\
        %& \texttt{pron|res} & 6 & 0\% & 0\% & 0\% \\
        %\midrule
        %\multirow{3}{*}{Case}
        %& \texttt{pron|nom} & 1146 & 2\% & 2\% & 1\% \\
        %& \texttt{pron} & 918 & 8\% & 22\% & 6\% \\
        %& \texttt{pron|akk} & 341 & 2\% & 6\% & 1\% \\
        %\midrule
        %\multirow{7}{*}{Type \& case}
        %& \texttt{pron|pers|nom} & 1146 & 2\% & 1\% & 2\% \\
        %& \texttt{pron|pers} & 840 & 5\% & 22\% & 5\% \\
        %& \texttt{pron|pers|akk} & 198 & 2\% & 6\% & 2\% \\
        %& \texttt{pron|refl} & 143 & 1\% & 10\% & 1\% \\
        %& \texttt{pron|sp} & 61 & 21\% & 20\% & 13\% \\
        %& \texttt{pron} & 8 & 0\% & 12\% & 0\% \\
        %& \texttt{pron|res} & 6 & 0\% & 0\% & 0\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Selected error rates from parsing with pron modifications. Only
        %personal pronouns have case, except for \emph{seg}.}
    %\label{pronerror}
%\end{table}

\section{Optimized Tag Set} \label{sec:optimizedtagset} The most successful tag
set modification for each category and their respective results are presented
in Table \ref{respectiveresults}. Nouns benefit by far the most from the
introduction of more fine-grained linguistically motivated distinctions, with
an LAS of 88.81\% and UAS of 91.73\%. We observe that the most promising tag
set modifications for verbs, adjectives, determiners and pronouns all reach LAS
of \textasciitilde 87.30\% and UAS of \textasciitilde 90.40\%. To investigate
the overall effect of these tag set modifications, we tested each of the
improvements in parser accuracy scores from baseline for statistical
significance using Dan Bikel's randomized parsing evaluation comparator
script\footnote{Available as \texttt{compare.pl} at
    \url{http://ilk.uvt.nl/conll/software.html}}, as used in the CoNLL shared
tasks. For the most successful tag set modification for each of the categories
seen in Table \ref{respectiveresults}, the difference in LAS from the original
tag set is statistically significant at significance level 0.05 (\emph{p}-value
< 0.05), as are all differences in UAS, except for verbs with finiteness
(\emph{p}-value 0.15) and pronouns with type and case (\emph{p}-value 0.06).

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llllll@{}}
        \toprule
        \textbf{Category} & \textbf{Feature(s)} & \textbf{MFT} &
        \textbf{Accuracy} & \textbf{LAS} & \textbf{UAS} \\
        \midrule
        \emph{Baseline} & --- & 94.14\% & 97.47\% & 87.01\% & 90.19\% \\
        Noun & Type, case \& definiteness & 89.61\% & 97.05\% & 88.81\% &
        91.73\% \\
        Verb & Finiteness & 93.72\% & 97.35\% & 87.30\% & 90.43\% \\
        Adjective & Degree & 94.13\% & 97.41\% & 87.29\% & 90.44\% \\
        Determiner & Definiteness & 94.13\% & 97.49\% & 87.30\% & 90.42\% \\
        Pronoun & Type \& case & 94.12\% & 97.51\% & 87.30\% & 90.41\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of tagging and parsing with the most successful tag set
        modification for each category.}
    \label{respectiveresults}
\end{table}

An overview of the final, optimized tag set can be found in Table
\ref{optimizedtagset}, comprising three new tags for adjectives, two for
determiners, six for pronouns, seven for nouns and two for verbs, totaling 20
tags. Appending these to the original tag set comprising 19 tags, we reach a
total of 39 tags for NDT.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Tag} & \textbf{Description} \\
        \midrule
        \texttt{adj|komp} & Comparative adjective \\
        \texttt{adj|pos} & Positive adjective \\
        \texttt{adj|sup} & Superlative adjective \\
        \texttt{det|be} & Definite determiner \\
        \texttt{det|ub} & Indefinite determiner \\
        \texttt{pron|pers} & Personal pronoun \\
        \texttt{pron|pers|akk} & Personal pronoun, accusative \\
        \texttt{pron|pers|nom} & Personal pronoun, nominative \\
        \texttt{pron|refl} & Reflexive pronoun \\
        \texttt{pron|res} & Reciprocal pronoun \\
        \texttt{pron|sp} & Interrogative pronoun \\
        \texttt{subst|appell} & Common noun \\
        \texttt{subst|appell|be} & Common noun, definite \\
        \texttt{subst|appell|be|gen} & Common noun, definite, genitive \\
        \texttt{subst|appell|ub} & Common noun, indefinite \\
        \texttt{subst|appell|ub|gen} & Common noun, indefinite, genitive \\
        \texttt{subst|prop} & Proper noun \\
        \texttt{subst|prop|gen} & Proper noun, genitive \\
        \texttt{verb|fin} & Finite verb \\
        \texttt{verb|infin} & Nonfinite verb \\
        \bottomrule
    \end{tabular}
    \caption{The optimized tag set.}
    \label{optimizedtagset}
\end{table}

The results from tagging and parsing with the optimized tag set are reported in
Table \ref{finalresults}, compared to the initial tag sets. The parser achieves
an LAS of 88.87\% and UAS of 91.78\%, which constitutes substantial increases
from the baseline, by 1.86 and 1.59 percentage points, respectively.  The
increase from type, case and definiteness for nouns alone is no more than 0.06
percentage points and not statistically significant, but as all the other tag
set modifications are far behind in terms of parser accuracy scores, this does
not serve as particularly shocking.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Tag set} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        Original & \textbf{94.14\%} & \textbf{97.47\%} & 87.01\% & 90.19\% \\
        Full & 85.12\% & 93.46\% & 87.13\% & 90.32\% \\
        Optimized & 89.20\% & 96.85\% & \textbf{88.87\%} & \textbf{91.78\%} \\
        \bottomrule
    \end{tabular}
    \caption{Results of tagging and parsing with the optimized tag set, compared to
        the initial tag sets.}
    \label{finalresults}
\end{table}

In the next chapter, we will evaluate various taggers and parsers on NDT using
this optimized tag set in order to identify the optimal pipeline for syntactic
parsing of Norwegian based on NDT.

%\subsection{Universal Dependencies}
%Universal Dependencies (UD) is a project that is developing
%cross-linguistically consistent treebank annotation for many languages, with
%the goal of facilitating multilingual parser development, cross-lingual
%learning, and parsing research from a language typology perspective.

%The results are rather disheartening, see table \ref{udeval}.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllll@{}}
        %\toprule
        %\textbf{Tag set} & \textbf{MFT} & \textbf{Accuracy} &
        %\textbf{LAS} & \textbf{UAS} \\
        %\midrule
        %Baseline & \textbf{94.14\%} & \textbf{97.47\%} & \textbf{87.01\%} &
        %\textbf{90.19\%} \\
        %Preliminary UD & 90.82\% & 96.68\% & 84.98\% & 88.26\% \\
        %Final UD & 90.82\% & 96.68\% & 84.84\% & 88.11\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of experiments with Universal Dependencies.}
    %\label{udeval}
%\end{table}

\chapter{Optimized Pipeline for Norwegian Dependency Parsing}
\label{chap:optpipeline}
In Chapter \ref{chap:experiments}, we optimized a tag set for syntactic parsing
of the Norwegian Dependency Treebank. We will now train and evaluate the
state-of-the-art PoS taggers and dependency parsers we introduced in Chapter
\ref{chap:background} on the Norwegian Dependency Treebank using our optimized
tag set. Additionally, the automatically assigned tags from the most successful
tagger will be used to train and evaluate the various parsers to assess the
effects of automatically assigned tags on parsing. We will finally run the most
successful tagger and parser on the held-out test data with the optimized tag
set and contrast these results to those obtained using the original tag set.

\section{PoS Tagger}
There is a plethora of available PoS taggers, but we will consider only at a
select few publicly available state-of-the-art taggers representing a range of
contemporary approaches, viz., the aforementioned TnT
v2.2\footnote{\url{http://www.coli.uni-saarland.de/~thorsten/tnt/}}
\cite{Bra:00}, HunPos v1.0\footnote{\url{https://code.google.com/p/hunpos/}}
\cite{Hal:Kor:Ora:07}, the Stanford tagger
v3.6.0\footnote{\url{http://nlp.stanford.edu/software/tagger.shtml}}
\cite{Tou:Kle:Man:03} and SVMTool
v1.3.1\footnote{\url{http://www.cs.upc.edu/~nlp/SVMTool/}} \cite{Gim:Mar:04}.
TnT is an HMM-based trigram tagger, while HunPos is a re-implementation of TnT,
using tag bigrams instead of unigrams to estimate the emission probability. The
Stanford tagger is based on maximum entropy, also known as logistic regression,
and cyclic dependency networks. Lastly, SVMTool is an implementation of Support
Vector Machines (SVM). See Section \ref{ssec:taggingapproaches} for further
details on these taggers. The taggers were trained on the training set and
evaluated on the development data set of the Norwegian Dependency Treebank,
using the original and the optimized tag set. The run times of training and
tagging were computed using the UNIX command \texttt{time}. The taggers were
all run on the High Performance Computing (HPC) cluster at the Department of
Informatics, University of Oslo.

TnT and HunPos were run with default settings and evaluated using the
TnT-included \texttt{tnt-diff}, which calculates the accuracy.

The Stanford tagger comes with a variety of included architectures, determining
what features are used to build the model. We use the included \emph{generic},
described as language-independent in the documentation, and
\emph{bidirectional5words}, reported to be the most accurate in the manual.

SVMTool is highly flexible and tunable, but we limited our runs to
experimenting with the option of strategy, employing three of the available
seven. Apart from this, SVMTool was run with default settings. The strategies
make use of a set of five predefined models, detailed in the documentation of
the tagger. The training time represents the time spent learning the model(s)
required for the strategy in question. Strategy 0 is the default strategy,
making use of model 0 and tagging the data in a greedy on-line fashion in a
single pass. With strategy 1, the data is tagged in two passes. In the first
pass, the unseen morphological context remains ambiguous, while in the second
pass, the tag predicted in the first pass is known and used as a feature.
Strategy 2 tags the data in a single pass, but uses both Model 0 and Model 2,
choosing the optimal one in a given case; if all the words in the unseen
context is already known, it uses Model 0. Otherwise, it uses Model 2.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllrr@{}}
        %\toprule
        %\textbf{Tagger} & \textbf{Setup} &
        %\textbf{Accuracy} & \textbf{Training Time} & \textbf{Tagging Time} \\
        %\midrule
        %Baseline & MFT & 94.14\% & 0.9s & 0.1s \\
        %HunPos & Default & 97.73\% & 1.7s & 0.5s \\
        %Stanford & Generic & 96.82\% & 2m 14s & 2.7s \\
        %Stanford & Bidirectional5words & 97.01\% & 3m 41s & 20.9s \\
        %SVMTool & Strategy 0 & 97.70\% & 3m 59s & 12s \\
        %SVMTool & Strategy 1 & \textbf{97.90\%} & 7m 56s & 23s \\
        %SVMTool & Strategy 2 & 97.72\% & 7m 46s & 15s \\
        %TnT & Default & 97.47\% & 0.5s & 0.3s \\
        %\bottomrule
    %\end{tabular}
    %\caption{Evaluation of training and tagging with the original,
        %coarse-grained tag set.}
    %\label{coarsetaggereval}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllrr@{}}
        %\toprule
        %\textbf{Tagger} & \textbf{Setup} &
        %\textbf{Accuracy} & \textbf{Training Time} & \textbf{Tagging Time} \\
        %\midrule
        %Baseline & MFT & 89.20\% & 0.9s & 0.1s \\
        %HunPos & Default & 96.92\% & 1.6s & 0.6s \\
        %Stanford & Generic & 94.24\% & 5m 22s & 7.1s \\
        %Stanford & Bidirectional5words & 94.43\% & 7m 47s & 3m 27s \\
        %SVMTool & Strategy 0 & 97.19\% & 5m 16s & 14.2s \\
        %SVMTool & Strategy 1 & \textbf{97.40\%} & 10m 51s & 29.4s \\
        %SVMTool & Strategy 2 & 97.04\% & 10m 21s & 17.1s \\
        %TnT & Default & 96.85\% & 0.5s & 0.4s \\
        %\bottomrule
    %\end{tabular}
    %\caption{Evaluation of training and tagging with the optimized tag set.}
    %\label{optimizedtaggereval}
%\end{table}

\subsection{Results on Original and Optimized Tag Set}
In a realistic setting, both speed and accuracy are important factors to
consider for PoS taggers and syntactic parsers, especially considering the
trade-off between the two, as very fast systems tend to suffer in terms of
accuracy, while the most accurate systems often lag behind in terms of speed.
In the following, we will focus on accuracy, as we are optimizing in terms of
performance. However, for some applications, high speed may be more critical
than high performance, and we therefore include the run times of training and
running the systems, providing insights that can hopefully aid other potential
users in choosing a tagger and/or parser that best meets their needs.

We report the time spent training and tagging with the various taggers on the
optimized tag set in Table \ref{taggertimeeval}. SVMTool with strategy 0 is
trained in 5 minutes, while the two other strategies spend roughly twice that.
Strategies 0 and 2 tag the data in 14 and 17 seconds, respectively, while
strategy 1 spends 29 seconds. The Stanford tagger employing the \emph{generic}
architecture spends roughly 5 minutes training, while training the
\emph{bidirectional5words} variant takes almost 8 minutes. Stanford with the
\emph{bidirectional5words} architecture, which is the slowest tagger, tags the
development data in just under 40 seconds, while the \emph{generic} variant
spends 7 seconds. TnT was able to learn from the 244k tokens in the training
data in under 1 second, while also spending less than a second tagging the
development data. HunPos is not far behind, spending 1.6 seconds training,
while tagging the development data in 0.9 seconds.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llrr@{}}
        \toprule
        \textbf{Tagger} & \textbf{Setup} & \textbf{Training Time} &
        \textbf{Tagging Time} \\
        \midrule
        %Baseline & MFT & 0.9s & 0.1s \\
        HunPos & Default & 1.6s & 0.6s \\
        Stanford & Generic & 5m 22s & 7.1s \\
        Stanford & Bidirectional5words & 7m 47s & 39.1s \\
        SVMTool & Strategy 0 & 5m 16s & 14.2s \\
        SVMTool & Strategy 1 & 10m 51s & 29.4s \\
        SVMTool & Strategy 2 & 10m 21s & 17.1s \\
        TnT & Default & 0.5s & 0.4s \\
        \bottomrule
    \end{tabular}
    \caption{Duration of training and testing the various taggers on the
        training and development data, respectively, using the optimized tag
        set.}
    \label{taggertimeeval}
\end{table}

\begin{table}
    \vspace{1ex}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llcc@{}}
        \toprule
        & & \multicolumn{2}{c}{\textbf{Accuracy}} \\
        \cmidrule(lr){3-4}
        \textbf{Tagger} & \textbf{Setup} & \textbf{Original} &
        \textbf{Optimized} \\
        \midrule
        Baseline & MFT & 94.14\% & 89.20\% \\
        HunPos & Default & 97.73\% & 96.92\% \\
        Stanford & Generic & 96.82\% & 94.24\% \\
        Stanford & Bidirectional5words & 97.01\% & 94.43\% \\
        SVMTool & Strategy 0 & 97.70\% & 97.19\% \\
        SVMTool & Strategy 1 & \textbf{97.90\%} & \textbf{97.40\%} \\
        SVMTool & Strategy 2 & 97.72\% & 97.04\% \\
        TnT & Default & 97.47\% & 96.85\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of tagging the development data with the original tag set
        and the optimized tag set.}
    \label{taggereval}
\end{table}

In Table \ref{taggereval}, we report the performance of the taggers on the
development data set with the original and the optimized tag set. It reveals
that tagging with the original tag set yields better tagger accuracy than the
optimized tag set, which is expected due to the increase in complexity with the
20 additional tags in the optimized tag set. We see that SVMTool employing
strategy 1 achieves the highest accuracy of 97.40\% with the optimized tag set,
followed by strategy 0 (97.19\%) and strategy 2 (97.04\%).  Stanford tagger
with the \emph{generic} architecture obtains the lowest accuracy of 94.24\%
with the optimized tag set, followed by the \emph{bidirectional5words} variant,
which obtains an accuracy of 94.43\%. TnT reaches the third lowest accuracy
using the optimized tag set of 96.85\%, while HunPos comes in fourth with
96.92\%.
%perform very closely to one another with the original tag set, 97.70\% for
%strategy 0 and 97.72\% for strategy 2, while on the optimized tag set,

As these results show that SVMTool employing strategy 1 is the best tagger for
NDT, we will use its assigned tags to investigate the effects of automatically
assigned PoS tags on syntactic parsing.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllrl@{}}
        %\toprule
        %\textbf{Tagger} & \textbf{Setup} & \textbf{Accuracy} & \textbf{Training
            %Time} & \textbf{Tagging Time} \\
        %\midrule
        %Baseline & MFT & 85.12\% & 0.9s & 0.2s \\
        %HunPos & Default & 93.58\% & 2.1s & 0.9s \\
        %Stanford & Generic & \% &  & \\
        %Stanford & Bidirectional 5 words & \% &  & \\
        %SVMTool & Strategy 0 & 93.51\% & 24m 30s & 42s \\
        %SVMTool & Strategy 1 & \textbf{93.74\%} & 48m 20s & 1m 19s \\
        %SVMTool & Strategy 2 & 93.50\% & 45m 38s & 44s \\
        %TnT & Default & 93.46\% & 0.7s & 0.6s\\
        %\bottomrule
    %\end{tabular}
    %\caption{Evaluation of training and tagging with the full, fine-grained tag set.}
    %\label{finetaggereval}
%\end{table}

\section{Syntactic Parser}
In similar fashion to the PoS taggers, the parsers were run on the High
Performance Computing (HPC) cluster at the Department of Informatics,
University of Oslo. They were trained on the training set and subsequently
tested on the development set of NDT, the run times of which were computed
using the UNIX command \texttt{time}.

MaltParser v1.8.1\footnote{\url{http://www.maltparser.org}}
\cite{Niv:Hal:Nil:07} is a transition-based dependency parser based on a
deterministic parser strategy and treebank-induced classifiers; Mate
v3.61\footnote{\url{http://code.google.com/p/mate-tools}} \cite{Boh:10} is
graph-based and uses maximum spanning trees with third-order features; RBG
v1.1\footnote{\url{http://www.github.com/taolei87/RBGParser}}
\cite{Lei:Xin:Zha:14} is a graph-based dependency parser employing tensor
decomposition and a low-rank factorization method; TurboParser
v2.3.0\footnote{\url{http://www.cs.cmu.edu/~ark/TurboParser/}}
\cite{Mar:Alm:Smi:13} is a graph-based third-order non-projective dependency
parser. These parsers are further detailed in Section \ref{ssec:depparsers}.
The parser accuracy scores, namely LAS and UAS, were computed by the
\texttt{eval.pl} script used in the CoNLL shared tasks.

RBGParser was run both with default settings, employing third-order features,
and `basic' settings, employing first-order features, in order to investigate
whether we could get a considerable decrease in training and parsing time while
maintaining high accuracy. MaltParser was first run with default settings and
then after a round of optimization with the optimization tool
MaltOptimizer\footnote{\url{http://nil.fdi.ucm.es/maltoptimizer/}}
\cite{Bal:Niv:12}, the duration of which was added to the training time. Mate
and TurboParser were run out-of-the-box with default settings.

As morphological features generally require separate morphological analysis and
hence are not included in non-annotated data, these were removed from the input
data for the most realistic comparison. Furthermore, as parsers differ with
respect to the use of morphological features, a direct comparison between
parsers that to varying degree make use of morphological features would be
`unfair' and misleading. A parser employing these features may report a higher
accuracy than one that excludes them, while not necessarily implying
superiority. We want to focus on how the PoS tags may affect the parsing and
isolate the task of PoS tagging in order to investigate the effect of
part-of-speech information.

We furthermore want to investigate the interplay between gold standard and
automatically assigned tags and how the choice of tag configuration affects the
parser results. We therefore ran the parsers with three configurations: (i)
train and test on gold standard tags; (ii) train on gold standard tags and test
on automatically assigned tags from SVMTool; (iii) train and test on
automatically assigned tags from SVMTool.

%\subsection{Results on Coarse Tag Set}
%As the results showed that Mate reported the best LAS, as well as next best
%UAS, while also being reasonably fast, we ended up going for Mate. It was one
%of the fastest, being surpassed only by non-optimized MaltParser and RBG with
%`basic' settings in terms of speed. Although MaltParser is exceptionally fast,
%its accuracy is markedly inferior to that of the other parsers, making it a
%poor choice.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllll@{}}
        %\toprule
        %& \multicolumn{2}{c}{\textbf{Inc. Features}} &
        %\multicolumn{2}{c}{\textbf{Exc. Features}} \\
        %\cmidrule(lr){2-3}
        %\cmidrule(lr){4-5}
        %\textbf{Parser} &  \textbf{LAS} & \textbf{UAS} & \textbf{LAS} &
        %\textbf{UAS} \\
        %\midrule
        %MaltParser v1.8.1 & 83.90\% & 87.52\% & 83.90\% & 87.52\% \\
        %MaltParser v1.8.1 \emph{optimized} & 86.65\% & 89.61\% & 87.99\% &
        %90.66\% \\
        %Mate v3.61 & 90.15\% & 92.51\% & \textbf{90.15\%} & 92.51\% \\
        %RBG v1.1 \emph{default} & 91.48\% & 94.00\% & 89.78\% &
        %\textbf{92.61\%} \\
        %RBG v1.1 \emph{basic} & 90.46\% & 92.95\% & 88.62\% & 91.32\% \\
        %TurboParser v2.2 & \textbf{91.83\%} & \textbf{94.11\%} & 89.67\% &
        %92.23\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the original tag set,
        %including and excluding morphological features, respectively. Trained
        %and tested on gold standard tags.}
    %\label{originalfeatureseval}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllrr@{}}
        %\toprule
        %\textbf{Parser} & \textbf{LAS} & \textbf{UAS} & \textbf{Training Time}
        %& \textbf{Parsing Time} \\
        %\midrule
        %MaltParser v1.8.1 & 83.90\% & 87.52\% & 36s & 3s \\
        %MaltParser v1.8.1 \emph{optimized} & 87.99\% & 90.66\% &
        %1h 11m 40s & 5s \\
        %Mate v3.61 & \textbf{90.15\%} & 92.51\% & 24m 9s & 38s \\
        %RBG v1.1 \emph{default} & 89.78\% & \textbf{92.61\%} & 1h 42m 25s & 54s \\
        %RBG v1.1 \emph{basic} & 88.62\% & 91.32\% & 13m & 19s \\
        %TurboParser v2.2 & 89.67\% & 92.23\%  & 1h 52m 43s & 59s \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the original tag set. Trained
        %and tested on gold standard tags.}
    %\label{originalgoldgoldeval}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllrr@{}}
        %\toprule
        %\textbf{Parser} &  \textbf{LAS} & \textbf{UAS} & \textbf{Training
            %Time} & \textbf{Parsing Time} \\
        %\midrule
        %MaltParser v1.8.1& 79.51\% & 83.92\% & 36s & 3s \\
        %MaltParser v1.8.1 \emph{optimized} & 83.36\% & 87.04\% &
        %1h 11m 40s & 5s \\
        %Mate v3.61 & \textbf{85.68\%} & 88.98\% & 24m 9s & 40s \\
        %RBG v1.1 \emph{default} & 85.37\% & \textbf{89.13\%} &
        %1h 42m 25s & 1m 8s \\
        %RBG v1.1 \emph{basic} & 84.50\% & 88.11\% & 13m & 18s \\
        %TurboParser v2.2& 85.10\% & 88.58\%  & 1h 52m 43s & 1m \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the original tag set. Trained
        %on gold standard tags and tested on system/predicted tags.}
    %\label{originalgoldsyseval}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllrr@{}}
        %\toprule
        %\textbf{Parser} &  \textbf{LAS} & \textbf{UAS} & \textbf{Training Time}
        %& \textbf{Parsing Time} \\
        %\midrule
        %MaltParser v1.8.1 & 80.29\% & 84.73\% & 40s & 4s \\
        %MaltParser v1.8.1 \emph{optimized} & 84.65\% & 88.08\% & 1h 10m 46s & 6s \\
        %Mate v3.61 & \textbf{87.01\%} & 90.19\% & 27m 4s & 40s \\
        %RBG v1.1 \emph{default} & 86.62\% & \textbf{90.24\%} & 1h 53m 17s & 1m 15s \\
        %RBG v1.1 \emph{basic} & 85.62\% & 89.18\% & 13m 56s &  20s \\
        %TurboParser v2.2 & 86.39\% & 89.79\%  & 2h 6m 25s & 1m 11s \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the original tag set. Trained
        %and tested on system/predicted tags.}
    %\label{originalsyssyseval}
%\end{table}

\subsection{Results on Optimized Tag Set}
We previously noted the trade-off between speed and performance and the fact
that even though we optimize in terms of performance, the speed of the various
parsers are of interest. In Table \ref{optimizedtimeeval}, we present the run
times of the various syntactic parsers, trained and tested on gold standard
tags using the optimized tag set. MaltParser with default settings is by far
the fastest, spending only 41 seconds training on the training data.
MaltParser optimized, RBG with default settings and TurboParser, on the other
hand, spend 1 hour and 15 minutes, 1 hour and 46 minutes, and 1 hour and 44
minutes, respectively. Mate is trained in 17 minutes and TurboParser in roughly
15 minutes. All parsers are able to parse the development data in less than a
minute. The two variants of MaltParser parses the development data in less than
10 seconds, while Mate spends 34 seconds and RBG with basic settings spends 17
seconds. TurboParser and RBG with default settings parse the data in just
under a minute.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lrr@{}}
        \toprule
        \textbf{Parser} & \textbf{Training Time} & \textbf{Parsing Time} \\
        \midrule
        MaltParser & 41s & 4s \\
        MaltParser \emph{optimized} & 1h 15m 42s & 7s \\
        Mate & 17m 48s & 34s \\
        RBG \emph{default} & 1h 46m 36s & 59s \\
        RBG \emph{basic} & 14m 57s & 17s \\
        TurboParser & 1h 43m 6s & 56s \\
        \bottomrule
    \end{tabular}
    \caption{Duration of training and testing the various parsers on the
        training and development data, respectively, trained and tested on gold
        standard tags using the optimized tag set.}
    \label{optimizedtimeeval}
\end{table}

Turning to the results in Table \ref{parsereval}, it is clear that Mate, with
the third fastest training and the fourth fastest parsing, is the best parser,
with an LAS of 91.83\% when trained and tested using gold standard tags, only
behind RBG with default settings in terms of UAS when training on gold standard
tags and testing on automatically assigned tags (91.46\% vs.  91.60\%). It is
worth noting that the authors of RBG, \citeA{Lei:Xin:Zha:14}, report UAS as
their evaluation metric and optimize their parser in terms of UAS. MaltParser
with default settings obtains the lowest LAS and UAS in all experiments
(86.22\% and 89.53\% for LAS and UAS, respectively, when using gold standard
tags for both training and testing), followed by its optimized counterpart
(similarly 89.93\% and 92.17\%). RBG with basic settings is consistently the
fourth best parser and obtains an LAS of 90.58\% when trained and tested on
gold standard tags, while running it with default settings yields the third
best LAS in all configurations (91.52\% when trained and tested on gold
standard tags) and UAS very close to or better than that of Mate. TurboParser
consistently obtains the second best LAS (91.67\% when using gold standard tags
for both training and testing) and the third best UAS. Using automatically
assigned tags instead of gold standard tags for training and testing leads to
an expected drop in LAS of more than 3 percentage points for all parsers.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        & \multicolumn{3}{c}{\textbf{LAS}}
        & \multicolumn{3}{c}{\textbf{UAS}} \\
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
        \textbf{Parser} & \textbf{G-G} & \textbf{G-A} & \textbf{A-A} &
        \textbf{G-G} & \textbf{G-A} & \textbf{A-A} \\
        \midrule
        MaltParser & 86.22\% & 82.88\% & 82.96\% & 89.53\% & 87.10\% & 87.16\%
        \\
        MaltParser \emph{optimized} & 89.93\% & 86.38\% & 86.32\% & 92.17\% &
        89.53\% & 89.51\% \\
        Mate & \textbf{91.83\%} & \textbf{88.39\%} & \textbf{88.80\%} &
        \textbf{94.03\%} & 91.46\% & \textbf{91.84\%} \\
        RBG \emph{default} & 91.52\% & 88.17\% & 88.35\% & 94.02\% &
        \textbf{91.60\%} & 91.72\% \\
        RBG \emph{basic} & 90.58\% & 87.26\% & 87.55\% & 92.96\% & 90.56\% &
        90.79\% \\
        TurboParser & 91.67\% & 88.25\% & 88.43\% & 93.88\% & 91.41\% & 91.56\%
        \\
        \bottomrule
    \end{tabular}
    \caption{Results of parsing the development data using the optimized tag
        set. \emph{G} denotes gold standard tags, \emph{A} denotes
        automatically assigned tags from SVMTool. For instance, \emph{G-G}
        denotes training and testing on gold standard tags. It is clear that
        training and testing on automatically assigned tags is superior to
        training on gold standard tags and testing on automatically assigned
        tags across the board. Mate is the best parser overall, only being
        surpassed by RBG with default settings in terms of UAS when training on
        gold standard tags and testing on automatic tags.}
    \label{parsereval}
\end{table}

We observe an expected trend for all parsers when going from training and
testing on gold standard tags to training on gold standard tags and testing on
automatically assigned tags, as we see a substantial drop in parser accuracy
scores. What strikes us as very interesting is that the parsers (except for
MaltParser optimized) parse the development data with automatic tags more
accurately when trained on automatic tags rather than on gold standard tags.
This coincides with the results we found when parsing with Mate on
automatically assigned tags from TnT in Section \ref{sec:tagsandfeatures}. We
expected that gold standard tags would always be the best choice, as
automatically assigned PoS tags contain errors which could obfuscate the
training. These results indicate that this is not the case.  Using
automatically assigned tags for both training and testing is statistically
significantly better at a significance level of 0.05 than training on gold
standard tags and testing on automatically assigned tags for Mate, RBG
\emph{basic} and Turbo in terms of both LAS and UAS. For RBG with default
settings, the difference is statistically significant only in terms of LAS.
%None of the differences in results seen with MaltParser are statistically
%significant.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lll@{}}
        %\toprule
        %\textbf{Parser} & \textbf{LAS} & \textbf{UAS} \\
        %\midrule
        %MaltParser v1.8.1 & 86.22\% & 89.53\% \\
        %MaltParser v1.8.1 \emph{optimized} & 89.93\% & 92.17\% \\
        %Mate v3.61 & 91.83\% & 94.03\% \\
        %RBG v1.1 \emph{default} & 91.52\% & 94.02\% \\
        %RBG v1.1 \emph{basic} & 90.58\% & 92.96\% \\
        %TurboParser v2.3 & 91.67\% & 93.88\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the optimized tag set. Trained
        %and tested on gold standard tags.}
    %\label{optimizedgoldgoldeval}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lll@{}}
        %\toprule
        %\textbf{Parser} &  \textbf{LAS} & \textbf{UAS} \\
        %\midrule
        %MaltParser v1.8.1 & 82.88\% & 87.10\% \\
        %MaltParser v1.8.1 \emph{optimized} & 86.38\% & 89.53\% \\
        %Mate v3.61 & 88.39\% & 91.46\% \\
        %RBG v1.1 \emph{default} & 88.17\% & 91.60\% \\
        %RBG v1.1 \emph{basic} & 87.26\% & 90.56\% \\
        %TurboParser v2.3 & 88.25\% & 91.41\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the optimized tag set. Trained
        %on gold standard tags and tested on system/predicted tags.}
    %\label{optimizedgoldsyseval}
%\end{table}

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lll@{}}
        %\toprule
        %\textbf{Parser} &  \textbf{LAS} & \textbf{UAS} \\
        %\midrule
        %MaltParser v1.8.1 & 82.96\% & 87.16\% \\
        %MaltParser v1.8.1 \emph{optimized} & 86.32\% & 89.51\% \\
        %Mate v3.61 & 88.80\% & 91.84\% \\
        %RBG v1.1 \emph{default} & 88.35\% & 91.72\% \\
        %RBG v1.1 \emph{basic} & 87.55\% & 90.79\% \\
        %TurboParser v2.3 & 88.43\% & 91.56\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results of running parsers on the optimized tag set. Trained
        %and tested on system/predicted tags.}
    %\label{optimizedsyssyseval}
%\end{table}

\section{Final Evaluation}
With SVMTool (employing strategy 1) established as the best tagger and Mate as
the best parser on our optimized tag set, we can finally evaluate on the
held-out test data set and compare our findings to the original tag set to
contrast the tag sets and make clear the effects of our optimization. As in our
previous runs, we experiment with both gold standard tags and automatically
assigned tags: (i) train on gold standard tags and test on gold standard tags;
(ii) train on automatically assigned tags and test on automatically assigned
tags. We exclude training on gold standard tags and testing on automatically
assigned tags as that combination was found to be consistently inferior to
training and testing on automatically assigned tags in our previous
experiments.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        & & \multicolumn{2}{c}{\textbf{LAS}}
        & \multicolumn{2}{c}{\textbf{UAS}} \\
        \cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
        \textbf{Data Set} & \textbf{Configuration} & \textbf{Original} &
        \textbf{Optimized} & \textbf{Original} & \textbf{Optimized} \\
        \midrule
        \multirow{2}{*}{Dev}
        & Gold--Gold & 90.15\% & 91.83\% & 92.51\% & 94.03\% \\
        & Auto--Auto & 86.73\% & 88.80\% & 89.99\% & 91.84\% \\
        \midrule
        \multirow{2}{*}{Test}
        & Gold--Gold & 90.55\% & 92.12\% & 92.97\% & 94.20\% \\
        & Auto--Auto & 86.76\% & 88.28\% & 90.13\% & 91.22\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of parsing the development data set and test data set with
        the setup we found most successful, i.e., tagging with SVMTool and
        parsing with Mate, compared to the results obtained with the original
        tag set. \emph{Gold--Gold} denotes training and testing on gold
        standard tags, \emph{Auto--Auto} denotes training and testing on
        automatically assigned tags from SVMTool.}
    \label{finaloptimizedeval}
\end{table}

Table \ref{finaloptimizedeval} presents the results of parsing the development
data set and the held-out test data set with the original tag set and the
optimized tag set using either gold standard tags or automatically assigned
tags from SVMTool. Parsing the development data set with our optimized tag set
yields very promising results, with improvements ranging from 1.52 percentage
points (UAS, training and testing on gold standard tags) to 2.07 percentage
points (LAS, training and testing on automatically assigned tags) compared to
the original tag set, all of which are statistically significant at a
significance level of 0.05. Turning to the parser results on the held-out test
data set, we see increases in parser accuracy scores of more than 1 percentage
point in all experiments when using our optimized tag set, all of which are
statistically significant at a significance level of 0.05. The largest increase
is seen for LAS when training and testing on gold standard tags, from 90.55\%
with the original tag set to 92.12\% with the optimized tag set\footnote{It is
    worth comparing our parser results with the first published results on
    parsing the Norwegian Dependency Treebank \cite{Sol:Skj:Ovr:14} using the
    original tag set and gold standard tags. Parsing their held-out test data
    set with Mate, they reported an LAS of 90.41\% and UAS of 92.84\%. However,
    as they used a different data set split, their results are not strictly
    comparable to our results and should perhaps be taken with a grain of
    salt.}, constituting 1.57 percentage points. The improvement seen for LAS
when using automatically assigned tags is very close, from 86.76\% to 88.28\%
(1.52 percentage points). These results indicate that the additional
linguistic information in the tags of our optimized tag set greatly aids
syntactic parsing and that optimizing an existing PoS tag set for a downstream
application can be useful and beneficial.


%and (ii)
%that our optimization of the tag set leads to a large increase in parser
%accuracy scores.
%This motivates the optimization of tag set and could motivate
%and inspire others to optimize an existing PoS tag set to their task.

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllll@{}}
        %\toprule
        %& \multicolumn{2}{c}{\textbf{LAS}}
        %& \multicolumn{2}{c}{\textbf{UAS}} \\
        %\cmidrule(lr){2-3}
        %\cmidrule(lr){4-5}
        %\textbf{Architecture} & \textbf{Original} & \textbf{Optimized} &
        %\textbf{Original} & \textbf{Optimized} \\
        %\midrule
        %Gold--Gold & 90.55\% & 92.12\% & 92.97\% & 94.20\% \\
        %Auto--Auto & 86.76\% & 88.28\% & 90.13\% & 91.22\% \\
        %\bottomrule
    %\end{tabular}
    %\caption{Results from parsing the held-out test data with the setup we
        %found most successful, i.e., tagging with SVMTool and parsing with
        %Mate. \emph{Gold} denotes gold standard tags, \emph{Auto} denotes
        %automatically assigned tags from SVMTool.}
    %\label{testoptimizedeval}
%\end{table}

\chapter{Conclusion \& Future Work}
\label{chap:conclusion}
In this thesis, we have developed an optimized PoS tag set for syntactic
dependency parsing of Norwegian. The optimized tag set is based on the tag set
of the Norwegian Dependency Treebank \cite{Sol:Skj:Ovr:14}, the first treebank
of its kind for Norwegian. The tag set was optimized by augmenting the
original, coarse-grained tag set containing 12 morphosyntactic tags with
additional linguistic information represented in the various morphological
features assigned to tokens in the treebank. The improvements in parser
performance with our optimized tag set indicate that a more fine-grained PoS
tag set
%we can modify a PoS tag set with linguistically motivated distinctions that,
may assist syntactic parsers in recognizing and generalizing syntactic
patterns, while potentially compromising the performance of PoS taggers.
%in recognizing and generalizing syntactic patterns that are not apparent from
%the coarse PoS tags (in the original tag set).
Our optimized tag set was attained through experiments with the range of
morphological features of the parts-of-speech available in the treebank, namely
noun, verb, adjective, determiner and pronoun. These experiments also served as
an assessment of how and to what extent the various morphological features aid
syntactic parsing.

The linguistic and computational considerations for designing PoS tag sets and
how these might often conflict was discussed in Chapter
\ref{chap:posandtagsets}, where we also presented a qualitative comparison of
the PoS tag set of the Norwegian Dependency Treebank and those of other
comparable treebanks, such as Penn Treebank and Stockholm-Umeå Corpus.
Furthermore, we provided an in-depth survey of the parts-of-speech in Norwegian
and their respective properties, before detailing how these are represented in
the Norwegian Dependency Treebank. The morphological properties of the PoS tags
in the Norwegian Dependency Treebank served as basis for our experiments with
tag set modifications, where we identified the most informative morphological
features for syntactic parsing of Norwegian.

In preparation to conducting our experiments with linguistically motivated tag
set modifications, we established a concrete setup for the experiments,
outlined in Chapter \ref{chap:expsetup}. As a standardized data set split
(training/development/test) of the Norwegian Dependency Treebank was not yet
established, we introduced a data set split that will be distributed with the
treebank and proposed as the new standard, annotated with tags from our
optimized tag set. Our data set split was used in the conversion of the
Norwegian Dependency Treebank to annotations adhering to the Universal
Dependencies scheme \cite{Ovr:Hoh:16}, where much of the same experimental
setup was used and the results of evaluating the converted treebank were compared to
our results obtained on the original treebank.
%@, serving as an indication of the usefulness and importance of our work.
%The converted treebank with annotations in the Universal Dependencies scheme
%was evaluated using SVMTool and Mate and compared to our results obtained on
%the original treebank.
We created a mapping for carrying out the tag set
modifications that maps the relevant existing tags to new, more fine-grained
tags including more relevant morphological features. We then presented the
evaluation metrics used to evaluate the performance of PoS taggers and
syntactic parsers in our tag set experiments and how we altered the treebank to
simulate a realistic setting. As it is common practice to compare the accuracy
of PoS taggers to a pre-computed baseline, we looked at various ways of
computing the commonly used Most Frequent Tag (MFT) baseline, which provided
the baseline accuracy for our experiments. Finally, the sum of these components
were combined into a pipeline which we employed in each of our experiments with
tag set modifications.

The experimental setup established in Chapter \ref{chap:expsetup} was put to
use in Chapter \ref{chap:experiments}, where we conducted our experiments with
tag set modifications and assessed the results of these experiments to optimize
the tag set of the Norwegian Dependency Treebank. In order to establish initial
figures to which we could compare the results of our experiments, we conducted
baseline experiments with our two initial tag sets, i.e., the original tag set
comprising a total of 19 tags, and the full tag set, which was created by
concatenating the coarse PoS tag of each token with its set of morphological
features, yielding a total of 368 tags. The results of these experiments showed
that the morphological features were informative and assisted syntactic parsing
of Norwegian. We then turned to our main experiments with modified PoS tags,
where we for each part-of-speech experimented with a range of morphological
features to find the most syntactically informative ones, i.e., those that led
to the best parser accuracy scores. In addition to utilizing each of the
features in isolation, we combined the most promising features to see if the
features interacted in a syntactically informative way, which often proved to
be the case. For nouns, the most informative features are type, case and
definiteness, and similarly type and case for pronouns. Finiteness is the most
informative verbal feature, and for determiners, information about definiteness
yields the best parser accuracy scores, while degree is the most syntactically
informative feature for adjectives. We performed error analysis of both
tagging and parsing with the most promising tag set modification for each
part-of-speech to further analyze how they assisted the tagging and parsing.
Finally, the most promising tag set modifications for each respective category
were combined to a final, optimized tag set which proved to give a large rise in
parser accuracy scores from our initial experiments.

With the optimized tag set established, we evaluated a range of
state-of-the-art PoS taggers and syntactic dependency parsers on the task of
tagging and parsing the Norwegian Dependency Treebank in Chapter
\ref{chap:optpipeline}. We found SVMTool to be the best tagger and Mate the
best parser, which we finally used to parse the held-out test data. Our results
revealed an increase in parser accuracy scores from the original tag set of
more than 1 percentage point across the board. Furthermore, we found that the
combination of training and testing on automatically assigned tags is superior
to training on gold standard tags and testing on automatically assigned tags.
The interplay between these tag configurations is to our knowledge severely
underinvestigated, and, contrary to our empirical results, the general
assumption seems to be that gold standard tags are always the preferred choice,
even when testing with automatically assigned tags.

This work has presented a systematic, empirical investigation of how an
existing PoS tag set can be modified and optimized for the task of syntactic
dependency parsing, complemented by evaluation of a range of state-of-the-art
PoS taggers and syntactic parsers applied to Norwegian. This has resulted in
concrete contributions to the Norwegian NLP community: (i) a data set split
(training/development/test) of the Norwegian Dependency Treebank; (ii) a PoS
tag set optimized for syntactic dependency parsing of Norwegian; (iii) a PoS
tagger model (based on SVMTool) trained on the treebank; and (iv) a syntactic
parser model (based on Mate) trained on the treebank. These resources are made
publicly available\footnote{See
    \url{https://www.github.com/petterhh/ndt-tools}} with the hope that they
can be found useful by researchers and others interested in applying and
advancing NLP applications for Norwegian.

\section{Future Work}
\label{sec:futurework}
There are several aspects of this thesis that can be further explored in future
work, including extrinsic evaluation of the effects of PoS tag sets on other
downstream NLP applications besides parsing, such as sentiment analysis and
named entity recognition. These applications often require tagged data, but are
markedly different from syntactic parsing, hence the evaluation would involve
investigating an entirely different aspect of the effects of tag set
granularity.
%, but could still make use of the morphological features in the
%treebank in a similar fashion to our experiments.

%this will investigate the effects of different tag set granularities farther
%down the pipeline.
%Other NLP applications that require merely tagged, and not
%parsed data,  serves as a completely different investigation.

Tokenization is the piece left of the puzzle that is a complete pipeline from
raw text to parsed data for Norwegian.
%(as lemmatization is not needed for neither tagging nor parsing).
An off-the-shelf tokenizer for Norwegian is thus an important tool required for
the facilitation of research and employment of NLP applications for Norwegian,
as the tokenizer included in the Oslo-Bergen Tagger cannot be run separately
and the tagger cannot reproduce the annotation choices of the Norwegian
Dependency Treebank.

We have focused on Norwegian Bokmål in our work, which is the most prevalent
written standard of Norwegian. However, the Norwegian Dependency Treebank also
contains data in Nynorsk, the other official written standard of Norwegian,
which can be used in similar fashion to the Bokmål portion of the treebank for
training and evaluating various NLP applications. In the future, we would be
interested in investigating how and to what extent the two varieties of
Norwegian differ with respect to various morphosyntactic aspects, as they are
very similar in terms of both morphology and syntax.

\begin{appendices}
    \chapter{Data Set Split}
    \label{chap:appsplit}
    The data set split (training/development/test) of the Norwegian Dependency
    Treebank and the files comprising each data set are described in the
    following. Each file contains a maximum of 100 sentences.

    \section{Training Data}
    The training data set of NDT consists of 15696 sentences distributed over
    180 files, shown in Table \ref{trainingdatasetsplit}.
    %180 files, 15696 sentences%, 11 MB
    %\begin{itemize}
        %\item ap001\_0000 -- ap012\_0002 (53 files)
        %\item bt001\_0000 -- bt005\_0001 (28 files)
        %\item db001a\_0000 -- db013\_0004 (42 files)
        %\item kk001\_0000 -- kk006\_0000 (10 files)
        %\item sp-bm001\_0000 -- sp-bm001\_0008 (9 files)
        %\item vg001\_0000 -- vg002\_0003 (8 files)
        %\item blogg-bm001\_0000 -- blogg-bm003\_0000 (9 files)
        %\item nou001\_0000 -- nou004\_0000 (10 files)
        %\item st001\_0000 -- st005\_0000 (11 files)
    %\end{itemize}

    \begin{table}
        \centering
        \smaller[0.5]
        \begin{tabular}{@{}llr@{}}
            \toprule
            \textbf{Source} & \textbf{File Interval} & \textbf{\# Files} \\
            \midrule
            Aftenposten &  ap001\_0000 -- ap012\_0002 & 53 \\
            Bergens Tidende &  bt001\_0000 -- bt005\_0001 & 28 \\
            Dagbladet &  db001a\_0000 -- db013\_0004 & 42 \\
            Klassekampen &  kk001\_0000 -- kk006\_0000 & 10 \\
            Sunnmørsposten &  sp-bm001\_0000 -- sp-bm001\_0008 & 9 \\
            Verdens Gang & vg001\_0000 -- vg002\_0003 & 8 \\
            Blogs & blogg-bm001\_0000 -- blogg-bm003\_0000 & 9 \\
            Government reports &  nou001\_0000 -- nou004\_0000 & 10 \\
            Parliament transcripts &  st001\_0000 -- st005\_0000 & 11 \\
            \midrule
            Total & & 180 \\
            \bottomrule
        \end{tabular}
        \caption{Overview of the files comprising the training data set of
            NDT.}
        \label{trainingdatasetsplit}
    \end{table}

    \section{Development Data}
    The development data set of NDT consists of 2410 sentences distributed over
    26 files, shown in Table \ref{devdatasetsplit}.

    \begin{table}
        \centering
        \smaller[0.5]
        \begin{tabular}{@{}llr@{}}
            \toprule
            \textbf{Source} & \textbf{File Interval} & \textbf{\# Files} \\
            \midrule
            Aftenposten &  ap012\_0003 -- ap014\_0002 & 7 \\
            Bergens Tidende &  bt005\_0002 -- bt005\_0005 & 4 \\
            Dagbladet &  db013\_0005 -- db014\_0002 & 5 \\
            Klassekampen &  kk006\_0001 -- kk007\_0000 & 2 \\
            Sunnmørsposten &  sp-bm002\_0000 -- sp-bm002\_0001 & 2 \\
            Verdens Gang & vg002\_0004 & 1 \\
            Blogs & blogg-bm003\_0001 -- blogg-bm003\_0002 & 2 \\
            Government reports & nou004\_0001 & 1 \\
            Parliament transcripts &  st005\_0001 -- st005\_0002 & 2 \\
            \midrule
            Total & & 26 \\
            \bottomrule
        \end{tabular}
        \caption{Overview of the files comprising the development data set of
            NDT.}
        \label{devdatasetsplit}
    \end{table}

    \section{Test Data}
    The test data set of NDT consists of 1939 sentences distributed over 26
    files, shown in Table \ref{testdatasetsplit}.

    %26 files, 1939 sentences%, 1.3 MB.
    %\begin{itemize}
        %\item ap014\_0003 -- ap015\_0002 (7 files)
        %\item bt005\_0006 -- bt006\_0001 (4 files)
        %\item db014\_0003 -- db014\_0007 (5 files)
        %\item kk007\_0001 -- kk008\_0000 (2 files)
        %\item sp-bm003\_0000 -- sp-bm003\_0001 (2 files)
        %\item vg002\_0005 (1 file)
        %\item blogg-bm003\_0003 -- blogg-bm003\_0004 (2 files)
        %\item nou004\_0002 (1 file)
        %\item st005\_0003 -- st005\_0004 (2 files)
    %\end{itemize}

    \begin{table}
        \centering
        \smaller[0.5]
        \begin{tabular}{@{}llr@{}}
            \toprule
            \textbf{Source} & \textbf{File Interval} & \textbf{\# Files} \\
            \midrule
            Aftenposten &  ap014\_0003 -- ap015\_0002 & 7 \\
            Bergens Tidende &  bt005\_0006 -- bt006\_0001 & 4 \\
            Dagbladet &  db014\_0003 -- db014\_0007 & 5 \\
            Klassekampen &  kk007\_0001 -- kk008\_0000 & 2 \\
            Sunnmørsposten &  sp-bm003\_0000 -- sp-bm003\_0001 & 2 \\
            Verdens Gang & vg002\_0005 & 1 \\
            Blogs & blogg-bm003\_0003 -- blogg-bm003\_0004 & 2 \\
            Government reports & nou004\_0002 & 1 \\
            Parliament transcripts &  st005\_0003 -- st005\_0004 & 2 \\
            \midrule
            Total & & 26 \\
            \bottomrule
        \end{tabular}
        \caption{Overview of the files comprising the test data set of NDT.}
        \label{testdatasetsplit}
    \end{table}

    %\section{By Source}
    %\begin{itemize}
        %\item Aftenposten (ap): 67 files (5866 sentences) in total. 53/7/7 split.
        %\item Bergens Tidende (bt): 36 files (3335 sentences) in total. 28/4/4 split.
        %\item Dagbladet (db): 52 files (4436 sentences) in total. 42/5/5 split.
        %\item Klassekampen (kk): 14 files (919 sentences) in total. 10/2/2 split.
        %\item Sunnmørsposten (sp-bm): 13 files (1201 sentences) in total. 9/2/2 split.
        %\item Verdens Gang (vg): 10 files (868 sentences) in total. 8/1/1 split.
        %\item Blogs (blogg): 13 files (1150 sentences) in total. 9/2/2 split.
        %\item Government reports (nou): 12 files (990 sentences) in total. 10/1/1 split.
        %\item Parliament transcripts (st): 15 files (1280 sentences) in total. 11/2/2 split.
    %\end{itemize}

    \chapter{Tag Sets}
    \label{chap:apptagsets}
    The following provides an overview of the tag set modifications, i.e., the
    additional tags, used in our tag set experiments, as described in Section
    \ref{sec:tagsetexperiments}.

    \section{Noun}
    \begin{table}
        \centering
        \smaller[1]
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Feature(s)} & \textbf{Tag(s)} \\
            \midrule
            Case & \texttt{subst|gen} \\
            \midrule
            \multirow{2}{*}{Definiteness}
            & \texttt{subst|be} \\
            & \texttt{subst|ub} \\
            \midrule
            \multirow{3}{*}{Gender}
            & \texttt{subst|fem} \\
            & \texttt{subst|mask} \\
            & \texttt{subst|nøyt} \\
            \midrule
            \multirow{2}{*}{Number}
            & \texttt{subst|ent} \\
            & \texttt{subst|fl} \\
            \midrule
            \multirow{2}{*}{Type}
            & \texttt{subst|appell} \\
            & \texttt{subst|prop} \\
            \midrule
            \multirow{4}{*}{Case \& definiteness}
            & \texttt{subst|be} \\
            & \texttt{subst|be|gen} \\
            & \texttt{subst|ub} \\
            & \texttt{subst|ub|gen} \\
            \midrule
            \multirow{4}{*}{Type \& case}
            & \texttt{subst|appell} \\
            & \texttt{subst|appell|gen} \\
            & \texttt{subst|prop} \\
            & \texttt{subst|prop|gen} \\
            \midrule
            \multirow{3}{*}{Type \& definiteness}
            & \texttt{subst|appell|be} \\
            & \texttt{subst|appell|ub} \\
            & \texttt{subst|prop} \\
            \midrule
            \multirow{4}{*}{Type \& number}
            & \texttt{subst|appell} \\
            & \texttt{subst|appell|ent} \\
            & \texttt{subst|appell|fl} \\
            & \texttt{subst|prop} \\
            \midrule
            \multirow{7}{*}{Type, case \& definiteness}
            & \texttt{subst|appell} \\
            & \texttt{subst|appell|be} \\
            & \texttt{subst|appell|be|gen} \\
            & \texttt{subst|appell|ub} \\
            & \texttt{subst|appell|ub|gen} \\
            & \texttt{subst|prop} \\
            & \texttt{subst|prop|gen} \\
            %\midrule
            %\multirow{7}{*}{Type, definiteness \& gender}
            %& \texttt{subst|appell|be|fem} \\
            %& \texttt{subst|appell|be|mask} \\
            %& \texttt{subst|appell|be|nøyt} \\
            %& \texttt{subst|appell|ub|fem} \\
            %& \texttt{subst|appell|ub|mask} \\
            %& \texttt{subst|appell|ub|nøyt} \\
            %& \texttt{subst|prop} \\
            \midrule
            \multirow{5}{*}{Type, definiteness \& number}
            & \texttt{subst|appell|be|ent} \\
            & \texttt{subst|appell|be|fl} \\
            & \texttt{subst|appell|ub|ent} \\
            & \texttt{subst|appell|ub|fl} \\
            & \texttt{subst|prop} \\
            %\midrule
            %\multirow{13}{*}{Type, definiteness, gender \& number}
            %& \texttt{subst|appell|be|fem|ent} \\
            %& \texttt{subst|appell|be|fem|fl} \\
            %& \texttt{subst|appell|be|mask|ent} \\
            %& \texttt{subst|appell|be|mask|fl} \\
            %& \texttt{subst|appell|be|nøyt|ent} \\
            %& \texttt{subst|appell|be|nøyt|fl} \\
            %& \texttt{subst|appell|ub|fem|ent} \\
            %& \texttt{subst|appell|ub|fem|fl} \\
            %& \texttt{subst|appell|ub|mask|ent} \\
            %& \texttt{subst|appell|ub|mask|fl} \\
            %& \texttt{subst|appell|ub|nøyt|ent} \\
            %& \texttt{subst|appell|ub|nøyt|fl} \\
            %& \texttt{subst|prop} \\
            \bottomrule
        \end{tabular}
        \caption{Tag set modifications for nouns (\texttt{subst}).}
        \label{substtagsets}
    \end{table}

    \section{Verb}
    \begin{table}
        \centering
        \smaller[1]
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Feature(s)} & \textbf{Tag(s)} \\
            \midrule
            Mood & \texttt{verb|imp} \\
            \midrule
            \multirow{4}{*}{Tense}
            & \texttt{verb|inf} \\
            & \texttt{verb|perf-part} \\
            & \texttt{verb|pres} \\
            & \texttt{verb|pret} \\
            \midrule
            Voice & \texttt{verb|pass} \\
            \midrule
            \multirow{5}{*}{Mood \& tense}
            & \texttt{verb|imp} \\
            & \texttt{verb|inf} \\
            & \texttt{verb|perf-part} \\
            & \texttt{verb|pres} \\
            & \texttt{verb|pret} \\
            \midrule
            \multirow{6}{*}{Voice \& tense}
            & \texttt{verb|inf} \\
            & \texttt{verb|perf-part} \\
            & \texttt{verb|pres} \\
            & \texttt{verb|pret} \\
            & \texttt{verb|inf|pass} \\
            & \texttt{verb|pres|pass} \\
            \midrule
            \multirow{7}{*}{Mood, tense \& voice}
            & \texttt{verb|imp} \\
            & \texttt{verb|inf} \\
            & \texttt{verb|perf-part} \\
            & \texttt{verb|pres} \\
            & \texttt{verb|pret} \\
            & \texttt{verb|inf|pass} \\
            & \texttt{verb|pres|pass} \\
            \midrule
            \multirow{2}{*}{Finiteness}
            & \texttt{verb|fin} \\
            & \texttt{verb|infin} \\
            \bottomrule
        \end{tabular}
        \caption{Tag set modifications for verbs (\texttt{verb}).}
        \label{verbtagsets}
    \end{table}

    \section{Adjective}
    \begin{table}
        \centering
        \smaller[1]
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Feature(s)} & \textbf{Tag(s)} \\
            \midrule
            \multirow{2}{*}{Definiteness}
            & \texttt{adj|be} \\
            & \texttt{adj|ub} \\
            \midrule
            \multirow{3}{*}{Degree}
            & \texttt{adj|komp} \\
            & \texttt{adj|pos} \\
            & \texttt{adj|sup} \\
            \midrule
            \multirow{2}{*}{Gender}
            & \texttt{adj|m/f} \\
            & \texttt{adj|nøyt} \\
            \midrule
            \multirow{2}{*}{Number}
            & \texttt{adj|ent} \\
            & \texttt{adj|fl} \\
            \midrule
            \multirow{5}{*}{Type}
            & \texttt{adj|<adv>} \\
            & \texttt{adj|<ordenstall>} \\
            & \texttt{adj|<perf-part>} \\
            & \texttt{adj|<pres-part>} \\
            & \texttt{adj|fork} \\
            \midrule
            \multirow{6}{*}{Definiteness \& degree}
            & \texttt{adj|be|pos} \\
            & \texttt{adj|be|sup} \\
            & \texttt{adj|komp} \\
            & \texttt{adj|pos} \\
            & \texttt{adj|ub|pos} \\
            & \texttt{adj|ub|sup} \\
            \midrule
            \multirow{4}{*}{Definiteness \& gender}
            & \texttt{adj|be} \\
            & \texttt{adj|ub} \\
            & \texttt{adj|ub|m/f} \\
            & \texttt{adj|ub|nøyt} \\
            \midrule
            \multirow{2}{*}{Definiteness \& number}
            & \texttt{adj|be|ent} \\
            & \texttt{adj|ub|ent} \\
            \midrule
            \multirow{5}{*}{Degree \& gender}
            & \texttt{adj|komp} \\
            & \texttt{adj|pos} \\
            & \texttt{adj|pos|m/f} \\
            & \texttt{adj|pos|nøyt} \\
            & \texttt{adj|sup} \\
            \midrule
            \multirow{5}{*}{Degree \& number}
            & \texttt{adj|komp} \\
            & \texttt{adj|pos} \\
            & \texttt{adj|pos|ent} \\
            & \texttt{adj|pos|fl} \\
            & \texttt{adj|sup} \\
            \midrule
            \multirow{5}{*}{Definiteness, degree \& number}
            & \texttt{adj|be|pos|ent} \\
            & \texttt{adj|ub|pos|ent} \\
            & \texttt{adj|komp} \\
            & \texttt{adj|pos} \\
            & \texttt{adj|sup} \\
            \bottomrule
        \end{tabular}
        \caption{Tag set modifications for adjectives (\texttt{adj}).}
        \label{adjtagsets}
    \end{table}

    \section{Determiner}
    \begin{table}
        \centering
        \smaller[1]
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Featur} & \textbf{Tag(s)} \\
            \midrule
            \multirow{2}{*}{Definiteness}
            & \texttt{det|be} \\
            & \texttt{det|ub} \\
            \midrule
            \multirow{3}{*}{Gender}
            & \texttt{det|fem} \\
            & \texttt{det|mask} \\
            & \texttt{det|nøyt} \\
            \midrule
            \multirow{2}{*}{Number}
            & \texttt{det|ent} \\
            & \texttt{det|fl} \\
            \midrule
            \multirow{5}{*}{Type}
            & \texttt{det|dem} \\
            & \texttt{det|forst} \\
            & \texttt{det|kvant} \\
            & \texttt{det|poss} \\
            & \texttt{det|sp} \\
            \bottomrule
        \end{tabular}
        \caption{Tag set modifications for determiners (\texttt{det}).}
        \label{dettagsets}
    \end{table}

    \section{Pronoun}
    \begin{table}
        \centering
        \smaller[1.5]
        \begin{tabular}{@{}ll@{}}
            \toprule
            \textbf{Feature(s)} & \textbf{Tag(s)} \\
            \midrule
            \multirow{2}{*}{Case}
            & \texttt{pron|akk} \\
            & \texttt{pron|nom} \\
            \midrule
            \multirow{3}{*}{Gender}
            & \texttt{pron|fem} \\
            & \texttt{pron|fem|mask} \\
            & \texttt{pron|mask} \\
            & \texttt{pron|nøyt} \\
            \midrule
            \multirow{2}{*}{Number}
            & \texttt{pron|ent} \\
            & \texttt{pron|fl} \\
            \midrule
            \multirow{3}{*}{Person}
            & \texttt{pron|1} \\
            & \texttt{pron|2} \\
            & \texttt{pron|3} \\
            \midrule
            \multirow{4}{*}{Type}
            & \texttt{pron|pers} \\
            & \texttt{pron|refl} \\
            & \texttt{pron|res} \\
            & \texttt{pron|sp} \\
            \midrule
            \multirow{6}{*}{Number \& person}
            & \texttt{pron|ent|1} \\
            & \texttt{pron|ent|2} \\
            & \texttt{pron|ent|3} \\
            & \texttt{pron|fl|1} \\
            & \texttt{pron|fl|2} \\
            & \texttt{pron|fl|3} \\
            \midrule
            \multirow{6}{*}{Type \& case}
            & \texttt{pron|pers} \\
            & \texttt{pron|pers|akk} \\
            & \texttt{pron|pers|nom} \\
            & \texttt{pron|refl} \\
            & \texttt{pron|res} \\
            & \texttt{pron|sp} \\
            \midrule
            \multirow{5}{*}{Type \& number}
            & \texttt{pron|pers|ent} \\
            & \texttt{pron|pers|fl} \\
            & \texttt{pron|refl} \\
            & \texttt{pron|res} \\
            & \texttt{pron|sp} \\
            \midrule
            \multirow{6}{*}{Type \& person}
            & \texttt{pron|pers|1} \\
            & \texttt{pron|pers|2} \\
            & \texttt{pron|pers|3} \\
            & \texttt{pron|refl} \\
            & \texttt{pron|res} \\
            & \texttt{pron|sp} \\
            \midrule
            \multirow{8}{*}{Type, case \& number}
            & \texttt{pron|pers} \\
            & \texttt{pron|pers|akk|ent} \\
            & \texttt{pron|pers|akk|fl} \\
            & \texttt{pron|pers|nom|ent} \\
            & \texttt{pron|pers|nom|fl} \\
            & \texttt{pron|refl} \\
            & \texttt{pron|res} \\
            & \texttt{pron|sp} \\
            \bottomrule
        \end{tabular}
        \caption{Tag set modifications for pronouns (\texttt{pron}).}
        \label{prontagsets}
    \end{table}
\end{appendices}

\backmatter
\bibliographystyle{apacite}
\bibliography{../references}

\end{document}
