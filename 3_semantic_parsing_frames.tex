\chapter{Semantic Dependency Parsing with Frames}
\label{chap:semantic}

This chapter presents the state-of-the-art in semantic dependency parsing. The goal of semantic dependency parsing can be superficially defined as a representation of `Who did what to whom', possibly adding `when' and `where' to the equation. This is similar to the aims of Semantic Role Labeling (SRL), which according to \cite{Jur:Mar:09} is a shallow semantic representation of \textit{semantic roles}, which are defined as the abstract role that the argument of a predicate can take in an event. However, semantic dependency parsing usually has a broader scope in its representation, attempting to identify various semantic phenomena, such ass negation, and other scopal dependencies that are not part of the scope of SRL.

To be more specific on the goals of this chapter, we will examine what \citeA{Oepen:15} define as \textit{Broad Coverage Semantic Dependency Parsing}:

\begin{displayquote}
... [T]he problem of recovering sentence-internal predicate-argument relationships for \textit{all} \textit{content} \textit{words}, i.e. the semantic structure constituting the relational core of sentence meaning.
\end{displayquote}

The emphasis on predicate-argument dependencies for all content words is the reason for focusing on dependency parsing techniques that can output a graph structure instead of a tree. In Chapter \ref{chap:background} we ended with a note on this aspect of dependency parsing, and in this chapter we will clarify and expand further upon where we left off.

The target representations used in the research on syntactic dependency parsing, and the results that such parsers are able to produce, have been largely limited to tree data structures. A tree can be defined as an acyclic directed graph, i.e. every node is reachable from a root node by exactly one directed path. This structure impose certain restrictions, such as a unique root, connectedness, and lack of reentries (single-head constraint), on the type of dependencies that can be represented.

The restrictions that trees impose thus limit certain aspects of semantic analysis, such as lexical units with multiple heads, or when it would be advantageous to leave certain semantically void lexemes outside of the dependency structure. In order to mitigate the restrictions imposed by tree-based parsing, efforts have been made to develop graph-structured target representations. Data-driven parsing techniques have been developed that can be trained on representations where the annotations of a sentence establish a dependency graph. These parsing techniques are capable of producing dependency structure that are better apt at capturing sentence semantics than tree-based parsing.

In this chapter we will focus on the 2014 and 2015 shared tasks on Broad-Coverage Semantic Dependency Parsing (SDP) \cite{Oepen:14, Oepen:15}. The two shared tasks have provided a large annotated corpora in 4 different annotation schemes for training and testing. These are annotations on the Wall Street Journal (WSJ) corpus of the Penn Treebank (PTB) for the SDP 2014, and the added Brown corpus of the same Treebank for the SDP 2015 \cite{Mar:San:Mar:93}.

Several researchers submitted their results to the two shared tasks, with many achieving state-of-the-art accuracy in their submissions. We will first examine the target representations made available for the SDP 2014 and 2015 shared tasks, and then move on to a presentation of the submissions and results of the participating teams.

The SDP task of 2015 also include annotated corpora for Czech and Chinese (Mandarin) languages. We have decided to limit our research and analysis to the English language, and will therefore not include these results in our thesis. However, it is worth mentioning that these target representations exist, and that several submissions to the SDP tasks also submitted results from parsers trained on the target representations in these two languages.

\section{Target Representations}
\label{sec:representations}

Target representations are an integral part of data-driven parsing. They are the foundation on which data-driven parsers are trained in order to predict the most plausible dependency structure for a given sentence. The four distinct representations we will examine use different annotation schemes. The first three representations we will examine are called DM, PAS and PCEDT, which were used for the SDP 2014 shared tasks. For the SDP 2015 task, the PCEDT target representation was replaced by PSD, and so-called \textit{Frames} where added to the DM and PSD representations. In Tables \ref{DM}, \ref{PAS}, \ref{PCEDT} and \ref{PSD} we have visually represented the annotations of DM, PAS, PCEDT and PSD on the sentence:

\begin{displayquote}
Bramalea said it expects to complete the issue by the end of the month.
\end{displayquote}

This sentence has been chosen in order to highlight certain aspects of a semantic dependency graph: some lexical units are left unattached, we have a few examples of lexical units with more than one head (breaking the so-called single-head constraint that a tree would impose), and dependencies that cross, making the graphs non-projective. It is worth noting that a tree can be non-projective, and that this is not a special case for graph representations, but is often desired in order to fully represent longer predicate-argument dependencies that create crossing dependencies across a sentence.

The data-sets that we will examine are all represented in the SDP format\footnote{See \url{http://sdp.delph-in.net/2014/data.html} and \url{http://sdp.delph-in.net/2015/data.html} for the technical details on the data format of SDP 2014 and 2015 respectively.}. We will now examine how these target representations have been constructed, and present some higher-level statistics on their content.


\begin{figure}
    \centering
    \smaller[]
    % \smaller[]
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
        \end{deptext}
        \depedge[edge unit distance=1.4ex]{2}{11}{TWHEN}
        \deproot[edge unit distance=4ex]{2}{root}
        \depedge[edge unit distance=1.5ex]{4}{14}{APP}
        \depedge[edge unit distance=0.7ex]{2}{8}{PAT}
        \depedge[edge unit distance=1.4ex]{11}{6}{PAT}
        \depedge[edge unit distance=1.4ex]{6}{1}{ACT}
        \depedge[edge unit distance=1.2ex]{11}{3}{ACT}
        \depedge[edge unit distance=.8ex]{6}{4}{EFF}
    \end{dependency}
    \caption{PCEDT representation.}
    \label{PCEDT}
\end{figure}

\begin{figure}
    \centering
    \smaller[]
    % \smaller[]
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{verb\_ARG1}
        \depedge{6}{3}{verb\_ARG1}
        \depedge{2}{4}{verb\_ARG2}
        \depedge{4}{6}{verb\_ARG2}
        \depedge{4}{3}{verb\_ARG1}
        \depedge{9}{6}{prep\_ARG1}
        \depedge{6}{8}{verb\_ARG2}
        \depedge{5}{6}{comp\_ARG1}
        \depedge{7}{8}{det\_ARG1}
        \depedge[edge unit distance=5ex]{9}{11}{prep\_ARG2}
        \depedge[edge unit distance=6ex]{10}{11}{det\_ARG1}
        \depedge{12}{14}{prep\_ARG2}
        \depedge{12}{11}{prep\_ARG1}
        \depedge{13}{14}{det\_ARG1}
    \end{dependency}
    \caption{PAS representation.}
    \label{PAS}
\end{figure}

\begin{figure}
    \centering
    \smaller[]
    \smaller[]
    % \tiny
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
            named:x-c \& v\_to:e-i-h-i \& pron:x \& v:e-i-h \& \_ \& v:e-i-p \& q:i-h-h \& n:x \& p:e-u-i \& q:i-h-h \& n\_of:x-i \& \_ \& q:i-h-h \& n:x \& \_ \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{ARG1}
        \depedge{6}{3}{ARG1}
        \depedge{4}{3}{ARG1}
        \depedge{2}{4}{ARG2}
        \depedge{9}{6}{ARG1}
        \depedge{4}{6}{ARG2}
        \depedge{6}{8}{ARG2}
        \depedge{7}{8}{BV}
        \depedge{9}{11}{ARG2}
        \depedge{10}{11}{BV}
        \depedge{11}{14}{ARG1}
        \depedge{13}{14}{BV}
    \end{dependency}
    \caption{DM representation.}
    \label{DM}
\end{figure}

\begin{figure}
    \centering
    \smaller[]
    % \smaller[]
    \begin{dependency}[]
        \begin{deptext}[column sep=0.2em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
            \_ \& ev-w2833f1 \& \_ \& ev-w1239f1 \& \_ \& ev-w620f1 \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{ACT-arg}
        \depedge{6}{3}{ACT-arg}
        \depedge{4}{3}{ACT-arg}
        \depedge{2}{4}{EFF-arg}
        \depedge{4}{6}{PAT-arg}
        \depedge{6}{8}{PAT-arg}
        \depedge[edge unit distance=2.1ex]{6}{11}{TWHEN}
        \depedge{11}{14}{APP}
    \end{dependency}
    \caption{PSD representation.}
    \label{PSD}
\end{figure}

\begin{displayquote}

\end{displayquote}

\paragraph{PCEDT: Prague Tectogrammatical Bi-Lexical Dependencies} The Prague Czech-English Dependency Treebank \cite{PCEDT}\footnote{See \url{http://ufal.mff.cuni.cz/pcedt2.0/}} is a dependency treebank over the WSJ from the PTB. The original English texts have been annotated along with annotated Czech translations. Similar to other treebanks from the PTB, these texts have been annotated with two layers of syntactic information: \textit{analytical} (a-layer) and \textit{tectogrammatical} (t-layer) \cite{Oepen:14}. The a-layer represents the so-called surface syntax, where the labels in the dependencies represent the syntactic information of the sentence. The t-layer is a layer where syntax and semantic dependencies are represented, and is based on the framework of the Functional Generative Description \cite{Sgall:86}. A conversion has been used in order to reach the SDP data format from this t-layer; see \citeA{Miyao:14} for details on the conversion from the t-layer of the PCEDT representation to the SDP representation.

\paragraph{PAS: Enju Predicate–Argument Structures} The Enju representation is based on Head-driven phrase structure grammar (HPSG), and is derived from the Enju HPSG treebank. This treebank is made by way of conversions from the phrase structure and predicate-argument representation of the PTB \cite{Oepen:14}. The PAS representation is extracted from the predicate-argument structures of the HPSDG Treebank. This predicate-argument structure represent bi-lexical semantic dependencies. As the PCEDT format, we refer the reader to \citeA{Miyao:14} for the technical details on the conversion to the SDP data format.

\paragraph{DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies} The semantic dependency graphs of the DM format are derived from the output of the ERG parser. This parser adds syntactic and semantic analysis by using the LinGO English Resource Grammar (LERG). LERG is, in a similar fashion to PAS, based on HPSG. It adds to the standard framework of HPSG by using Minimal Recursion Semantics for specifying semantic attributes, but does so without implementing the binding theory of HPSG \cite{Flickinger:00}. The DM representations are derived through a two-step `lossy' conversion. The first step in this conversion is to convert the MRSs to variable-free \textit{Elementary Dependency Structures}, and then a second step is applied where some information is lost in transforming the results of the previous step to the strictly bi-lexical SDP data format \cite{Miyao:14}.

\paragraph{PSD: Prague Semantic Dependencies} The PCEDT target representation is used as basis for arriving at the PSD\footnote{See \url{http://tinyurl.com/h8dfkcz} for more technical details on the PSD target representation and conversion from PCEDT.} target representation. This is made using a conversion. The PCEDT representation consists of dependency structures that are always rooted trees. This is due to technical aspects of the conversion from the t-layers mentioned above, to the PCEDT data format. For the SDP 2015 task, a conversion of the PCEDT data's t-layer was performed in order to reach true bi-lexical dependencies.

\subsection{Frames}
\label{sec:frames}

The SDP 2015 introduced frames, also referred to as sense distinctions, to the DM and PSD target representations. These are added as an extra layer to the sentence where multiple classes are used in order to add additional information on the content words of a sentence. If we examine the semantic dependency graphs in Figure \ref{DM} and \ref{PSD}, we see these classes below the sentence.

According to \citeA{Oepen:16}, DM frames encode more general `linking patterns', which are mappings from syntactic to semantic arguments, whereas PSD represents actual sense identifiers and show different values for distinct lexemes. They note further that PSD only annotates senses on verbal predicates, while DM provide frame identifiers for all semantically contentful nodes. Additionally, \citeA{Oepen:15} note that the DM frames are limited to argument structure distinctions, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames. They further note that the PSD frames draw on much richer sense inventory, based on the EngValLex database \cite{Cinkova:06}.

As we will see in Chapter \ref{chap:analysis}, we will focus on frames when in our own classification task in Chapter \ref{chap:experiments}. We will thus return to frames in these two chapters with a more in-depth analysis of the DM and PSD data sets, results of the parsers examined in this chapter, and a more exploratory approach to the frame classes themselves.

\section{Data sets}
\label{sec:data-sets}

The data-sets for the SDP 2014 and 2015 vary slightly. In the SDP 2014, the three annotations are over the same set of texts: Sections 00-21 of the WSJ corpus. A set of sentences where excluded from the data sets where (a) no gold-standard analysis existed; (b) it was not possible to align the tokens of each sentence on-to-one for all three representations; (c) there where cycles in at least on of the graphs of a sentence. After this cleanup, the SDP 2014 data set counts 34,004 sentences (or a total of 745,543 tokens) for the training split (Sections 00-20), and 1,348 sentences (or a total of 29,808 tokens) for the test split (Section 21) \cite{Oepen:14}. The SDP 2015 shared tasks used the same data set, but added a balanced corpus from the Brown Corpus. Also, the DM graphs where extracted from a later and improved release of the DeepBank (version 1.1). The exclusion of sentences from the data sets where a bit lower for SDP 2015, and the training set counts 35,657 sentences (or a total of 802,717 tokens; roughly eight percent more than for SDP 2014). For the test set 1,410 sentences (or a total of 31,948 tokens) from the WSJ Section 21 was reserved for in-domain testing, and 1,849 sentences (or a total of 31,583 tokens) from the Brown Corpus was reserved for out-of-domain testing \cite{Oepen:15}.

We exclude the 2014 SDP data set when we further explore the data sets. We will focus on the results from the 2015 SDP tasks due to the fact that this is an extension of the previous year, but also due to the increase in overall performance from the participating research teams. In Chapter \ref{chap:analysis} we follow the same suit and focus solely on the SDP 2015 results in our contrastive analysis of the best scoring submissions.

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        & \multicolumn{3}{c}{In-domain}
        & \multicolumn{3}{c}{Out-of-domain} \\
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
        \textbf{} & \textbf{DM} & \textbf{PAS} & \textbf{PSD} &
        \textbf{DM} & \textbf{PSD} & \textbf{PSD} \\
        \midrule
        \# labels & 59 & 42 & 91 & 47 & 41 & 74 \\
        \% singletons & 22.97 & 4.38 & 35.76 & 25.40 & 5.84 & 39.11 \\
        edge density & 0.96 & 1.02 & 1.01 &  0.95 & 1.02 & 0.99 \\
        $\%_g$ trees & 2.30 & 1.22 & 42.19 & 9.68 & 2.38 & 51.43 \\
        $\%_g$ noncrossing & 69.03 & 59.57 & 64.58 & 74.58 & 65.28 & 74.26 \\
        $\%_g$ projective & 2.91 & 1.64 & 41.92 & 8.82 & 3.46 & 54.35 \\
        $\%_g$ fragmented & 6.55 & 0.23 & 0.69 & 4.71 & 0.65 & 1.73 \\
        $\%_n$ reentrancies & 27.44 & 29.36 & 11.42 & 26.14 & 29.36 & 11.46 \\
        $\%_g$ topless & 0.31 & 0.02 & – & 1.41 & – & – \\
        \# top nodes & 0.9969 & 0.9998 & 1.1276 & 0.9859 & 1.0000 & 1.2645 \\
        $\%_n$ non-top roots & 44.91 & 55.98 & 4.35 & 39.89 & 50.93 & 5.27 \\
        average treewidth & 1.30 & 1.72 & 1.61 & 1.31 & 1.69 & 1.50  \\
        maximum treewidth & 3 & 3 & 7  & 3 & 3 & 5 \\
        \midrule
        \# frames & 297 & – & 5426  & 172 & – & 1208 \\
        $\%_n$ frames & 13.52 & – & 16.77 & 15.79 & – & 19.50 \\
        \bottomrule
    \end{tabular}
    \caption{High-level statistics on the SDP 2015 data sets}
    \label{fig:data}
\end{table}

% \begin{table}
%     \centering  
%     \smaller[0.5]
%     \begin{tabular}{@{}lrrr@{}}
%         \toprule
%         & \textbf{DM} &  \textbf{PAS} & \textbf{PSD} & \\
%         \midrule
%         \texttt{adj} & 3144 & 95.89\% & 94.97\% \\
%         \bottomrule
%     \end{tabular}
%     \caption{Statistics of target representations for SDP 2015.}
%     \label{fig:data-in-domain}
% \end{table}

\subsection{Quantitative Analysis of Data Sets}
\label{sec:quantitative}

In Figure \ref{fig:data} we present some high-level statistics on the SD 2015 data set. The data are reproduced from \citeA{Oepen:15}. The PSD representation is the most fine-grained in terms of the linguistic variation of dependency labels with 91 unique labels. DM and PAS are in this respect more coarse-grained and similar, also sharing a more similar naming and type convention.

When examining the percentage of trees, projective vs. non-projective graphs, and reentrancies, wee see that PSD is the most `tree-oriented' of the three target representations. As mentioned above, the PSD target representation is based on the PCEDT representation, which only consists of dependency structures that are rooted trees. 

As we can observe in Figure \ref{fig:data}, the number of frames in PSD is approximately 18 times that of the DM data set. 

\section{Parsers and their Results}
\label{sec:parsers}

In this Section we will examine a set of state-of-the art semantic dependency parsers. As mentioned previously, we will examine the submissions from the SDP 2015, and not include the results from SDP 2014. This choice is based on the observation that many of the same teams submitted for both tasks, that the SDP 2015 shared tasks are an extension of the previous year, and that the scores for the submissions only saw an improvement the second year.

We can observe the performance of the SDP 2015 parsers in Table \ref{fig:results}, which have been reproduced from \citeA{Oepen:15}. There where 6 teams that submitted results from their parsing systems. Each team could submit two runs per track, and it is the best run (if more than one run were submitted) that is presented in Table \ref{fig:results}. The evaluation of each parser is based on the accuracy of the dependency graphs that they produce on the test set mentioned above, measured against the gold-standard testing data. The evaluation itself is based on the metrics: 

\begin{enumerate}
    \item Labeled precision, recall, and F$_1$, referred to as LP, LR and LF.
    \item Complete predications: Which for the DM and PAS target representation means measuring all outgoing dependency edges, and for the PSD target representation to the ones where the label has an `-arg' suffix. Here too precision, recall and F$_1$ score is used, referred to as PP, PR, and PF.
    \item Semantic frames: This is comprised of a complete predication with scores for the frame (or sense) identifier. As other scores, this score is also represented by precision, recall and F$_1$ score, referred to as FP, FR, and FF.
    \item Both complete predications and semantic frames evaluations are limited to those predicates that correspond to verbal parts of speech, as determined by the gold-standard part of speech.
\end{enumerate}

We will now examine each teams submissions in their own right.

\paragraph{Turku} 

\paragraph{Lisbon}

\paragraph{Peking}

\paragraph{Riga}

\paragraph{Minsk}

\paragraph{In-House}


\begin{table}
    \centering
    \smaller[]
    \smaller[]
    \smaller[0.5]
    \begin{tabular}{@{}cccccccccccccc@{}}
        \toprule
        \multicolumn{1}{c}{ }
        & \multicolumn{1}{c}{ }
        & \multicolumn{4}{c}{\textbf{DM}}
        & \multicolumn{4}{c}{\textbf{PAS}}
        & \multicolumn{4}{c}{\textbf{PSD}} \\
        \cmidrule(lr){3-6}
        \cmidrule(lr){7-10}
        \cmidrule(lr){11-14}
        &
        LF.av &
        LF & LP & LR & FF &
        LP & LR & LF & PF &
        LF & LP & LR & FF \\
        \midrule
        Turku\# & 86.81 & 88.29 & 89.52 & 87.09 & 58.39 & 95.58 & 95.94 & 95.21 & 87.99 & 76.57 & 78.24 & 74.97 & 56.85 \\
        Lisbon* & 86.23 & 89.44 & 90.52 & 88.39 & 00.20 & 91.67 & 92.45 & 90.90 & 84.18 & 77.58 & 79.88 & 75.41 & 00.06 \\
        Peking & 85.33 & 89.09 & 90.93 & 87.32 & 63.08 & 91.26 & 92.90 & 89.67 & 79.08 & 75.66 & 78.60 & 72.93 & 49.95 \\
        Lisbon & 85.15 & 88.21 & 89.84 & 86.64 & 00.15 & 90.88 & 91.87 & 89.92 & 81.74 & 76.36 & 78.62 & 74.23 & 00.03 \\
        Riga & 84.00 & 87.90 & 88.57 & 87.24 & 58.12 & 90.75 & 91.50 & 90.02 & 80.03 & 73.34 & 75.25 & 71.52 & 52.54 \\
        Turku* & 83.47 & 86.17 & 87.80 & 84.60 & 54.67 & 90.62 & 91.38 & 89.87 & 80.60 & 73.63 & 76.10 & 71.32 & 53.20 \\
        Minsk & 80.74 & 84.13 & 86.28 & 82.09 & 54.24 & 85.24 & 87.28 & 83.28 & 64.66 & 72.84 & 74.65 & 71.13 & 51.63 \\
        In-House* & 61.61 & 92.80 & 92.85 & 92.75 & 83.79 & 92.03 & 92.07 & 91.99 & 87.24 & – & – & – & – \\
        \bottomrule
        
        \\
        \toprule
        \multicolumn{1}{c}{ }
        & \multicolumn{1}{c}{ }
        & \multicolumn{4}{c}{\textbf{DM}}
        & \multicolumn{4}{c}{\textbf{PAS}}
        & \multicolumn{4}{c}{\textbf{PSD}} \\
        \cmidrule(lr){3-6}
        \cmidrule(lr){7-10}
        \cmidrule(lr){11-14}
        &
        LF.av &
        LF & LP & LR & FF &
        LP & LR & LF & PF &
        LF & LP & LR & FF \\
        \midrule
        Turku\# & 83.50 & 82.11 & 84.26 & 80.07 & 42.89 & 92.92 & 93.52 & 92.33 & 83.80 & 75.47 & 77.77 & 73.31 & 42.37 \\
        Lisbon* & 82.53 & 83.77 & 85.79 & 81.84 & 00.35 & 87.63 & 88.88 & 86.41 & 80.19 & 76.18 & 80.12 & 72.61 & 02.25 \\
        Lisbon & 81.15 & 81.75 & 84.81 & 78.90 & 00.27 & 86.88 & 88.52 & 85.30 & 78.47 & 74.82 & 78.68 & 71.31 & 02.09 \\
        Peking & 80.78 & 81.84 & 84.29 & 79.53 & 47.49 & 87.23 & 89.47 & 85.10 & 74.75 & 73.28 & 77.36 & 69.61 & 34.28 \\
        Riga & 79.23 & 80.69 & 81.69 & 79.72 & 41.88 & 86.63 & 87.56 & 85.72 & 76.26 & 70.37 & 73.23 & 67.71 & 40.76 \\
        Turku* & 78.85 & 79.01 & 81.54 & 76.63 & 39.15 & 85.95 & 86.95 & 84.98 & 76.38 & 71.59 & 74.92 & 68.55 & 38.75 \\
        Minsk & 75.79 & 77.24 & 80.24 & 74.46 & 42.18 & 80.44 & 83.07 & 77.96 & 62.00 & 69.68 & 72.26 & 67.27 & 41.25 \\
        In-House* & 59.24 & 89.69 & 89.80 & 89.58 & 76.39 & 88.03 & 88.10 & 87.96 & 81.69 & – & – & – & – \\
        \bottomrule
    \end{tabular}
    \caption{Results from the gold track (marked \#), open track (marked *) and closed track (unmarked) of the in-domain (top) and out-of-domain (bottom). LF.av indicates the average LF score across all representations, and is used to rank the systems in their overall performance.}
    \label{fig:results}
\end{table}