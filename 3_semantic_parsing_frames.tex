\chapter{Semantic Dependency Parsing with Frames}
\label{chap:semantic}

This chapter presents the state-of-the-art in semantic dependency parsing. The goal of semantic dependency parsing can be superficially defined as a representation of `Who did what to whom', possibly adding `when' and `where' to the equation. This is similar to the aims of Semantic Role Labeling (SRL), which according to \citeA{Jur:Mar:09} is a shallow semantic representation of \textit{semantic roles}. They define semantic roles as the abstract role that the argument of a predicate can take in an event. Semantic dependency parsing usually has a broader scope in its representation. In addition to the goals of SRL, semantic dependency parsing attempts to identify various semantic phenomena, such as negation, topicalization, relative clauses, and other scopal dependencies that are not part of the scope of SRL.

To be more specific on the goals of this chapter, we will examine what \citeA{Oepen:15} define as \textit{Broad Coverage Semantic Dependency Parsing}:

\begin{displayquote}
... [T]he problem of recovering sentence-internal predicate-argument relationships for \textit{all} \textit{content} \textit{words}, i.e. the semantic structure constituting the relational core of sentence meaning.
\end{displayquote}

The emphasis on predicate-argument dependencies for all content words is the reason for focusing on dependency parsing techniques that can output a graph structure instead of a tree. In Chapter \ref{chap:background} we ended with a note on this aspect of dependency parsing, and in this chapter we will clarify and expand further upon where we left off.

The target representations used in the research on syntactic dependency parsing, and the results that such parsers are able to produce, have been largely limited to tree data structures. A tree can be defined as an acyclic directed graph, i.e. every node is reachable from a root node by exactly one directed path. This structure impose certain restrictions, such as a unique root, connectedness, and lack of reentries (the so-called single-head constraint), on the type of dependencies that can be represented.

The restrictions that trees impose thus limit certain aspects of semantic analysis, such as dependents with more than one head, or when it would be advantageous to leave certain semantically void lexemes outside of the dependency structure. In order to mitigate the restrictions imposed by tree-based parsing, efforts have been made to develop graph-structured target representations. Data-driven parsing techniques have been developed that can be trained on representations where the annotations of a sentence establish a dependency graph. These parsing techniques are therefore capable of producing dependency structures that are better apt at capturing sentence semantics than parsing techniques that output a tree structure.

We will focus our attention on the SemEval-2014 Task 8 and SemEval-2015 Task 18 on Broad Coverage Semantic Dependency Parsing (referred to as SemEval-2014 and SemEval-2015 from now on respectively) \cite{Oepen:14, Oepen:15}. The two shared tasks have provided a large annotated corpora in 4 different annotation schemes for training and testing. These are annotations on the Wall Street Journal (WSJ) corpus of the Penn Treebank (PTB) for SemEval-2014, and the added Brown corpus of the same Treebank for SemEval-2015 shared task \cite{Mar:San:Mar:93}.

Several researchers submitted their results to the two shared tasks, with many achieving state-of-the-art accuracy on their results. We will first examine the target representations made available for SemEval-2014 and 2015, and then move on to a presentation of the technical aspects of behind the various submissions. We restrict the technical presentation to the SemEval-2015 task, which we will also do when examining higher level statistics on the data sets. Since the SemEval-2015 task is an extension of the previous year, we find it more useful to focus our attention on the data sets and submissions of that year. The results also saw an increase in overall performance in the SemEval-2015 shared task than the previous year. In Chapter \ref{chap:analysis} we follow the same suit and focus solely on the SemEval-2015 results in our contrastive analysis of the results of a selected group of teams and their submissions.

SemEval-2015 also include annotated corpora for Czech and Chinese (Mandarin) languages. We have decided to limit our research and analysis to the English language, and will therefore not include these results in our thesis. However, it is worth mentioning that these target representations exist, and that several submissions to the SemEval-2015 task also submitted results from parsers trained on the target representations in these two languages.

\section{Target Representations}
\label{sec:representations}

Target representations are an integral part of data-driven parsing. They are the foundation on which data-driven parsers are trained in order to predict the most plausible dependency structure for a given sentence. The four distinct representations we will examine use different annotation schemes. The first three representations we will examine are called DM, PAS and PCEDT, which were used for the SemEval-2014 shared tasks. For the SemEval-2015 task, the PCEDT target representation was replaced by PSD, and so-called \textit{Frames} where added to the DM and PSD representations. In Tables \ref{DM}, \ref{PAS}, \ref{PCEDT} and \ref{PSD} we have visually represented the annotations of DM, PAS, PCEDT and PSD on the sentence:

\begin{displayquote}
Bramalea said it expects to complete the issue by the end of the month.
\end{displayquote}

This sentence has been chosen in order to highlight certain aspects of a semantic dependency graph: some lexical units are left unattached, we have a few examples of lexical units with more than one head (breaking the so-called single-head constraint that a tree would impose), and dependencies that cross, making the graphs non-projective. It is worth noting that a tree can be non-projective, and that this is not a special case for graph representations, but is often desired in order to fully represent longer predicate-argument dependencies that create crossing dependencies across a sentence.

The data-sets that we will examine are all represented in the SDP format\footnote{See \url{http://sdp.delph-in.net/2014/data.html} and \url{http://sdp.delph-in.net/2015/data.html} for the technical details on the data format of SemEval-2014 and 2015 respectively.}. We will now examine how these target representations have been constructed, and present some higher-level statistics on their content.


\begin{figure}
    \centering
    \smaller[]
    % \smaller[]
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
        \end{deptext}
        \depedge[edge unit distance=1.4ex]{2}{11}{TWHEN}
        \deproot[edge unit distance=4ex]{2}{root}
        \depedge[edge unit distance=1.5ex]{4}{14}{APP}
        \depedge[edge unit distance=0.7ex]{2}{8}{PAT}
        \depedge[edge unit distance=1.4ex]{11}{6}{PAT}
        \depedge[edge unit distance=1.4ex]{6}{1}{ACT}
        \depedge[edge unit distance=1.2ex]{11}{3}{ACT}
        \depedge[edge unit distance=.8ex]{6}{4}{EFF}
    \end{dependency}
    \caption{PCEDT target representation.}
    \label{PCEDT}
\end{figure}

\begin{figure}
    \centering
    \smaller[]
    % \smaller[]
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{verb\_ARG1}
        \depedge{6}{3}{verb\_ARG1}
        \depedge{2}{4}{verb\_ARG2}
        \depedge{4}{6}{verb\_ARG2}
        \depedge{4}{3}{verb\_ARG1}
        \depedge{9}{6}{prep\_ARG1}
        \depedge{6}{8}{verb\_ARG2}
        \depedge{5}{6}{comp\_ARG1}
        \depedge{7}{8}{det\_ARG1}
        \depedge[edge unit distance=5ex]{9}{11}{prep\_ARG2}
        \depedge[edge unit distance=6ex]{10}{11}{det\_ARG1}
        \depedge{12}{14}{prep\_ARG2}
        \depedge{12}{11}{prep\_ARG1}
        \depedge{13}{14}{det\_ARG1}
    \end{dependency}
    \caption{PAS target representation.}
    \label{PAS}
\end{figure}

\begin{figure}
    \centering
    \smaller[]
    \smaller[]
    % \tiny
    \begin{dependency}[]
        \begin{deptext}[column sep=0.5em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
            named:x-c \& v\_to:e-i-h-i \& pron:x \& v:e-i-h \& \_ \& v:e-i-p \& q:i-h-h \& n:x \& p:e-u-i \& q:i-h-h \& n\_of:x-i \& \_ \& q:i-h-h \& n:x \& \_ \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{ARG1}
        \depedge{6}{3}{ARG1}
        \depedge{4}{3}{ARG1}
        \depedge{2}{4}{ARG2}
        \depedge{9}{6}{ARG1}
        \depedge{4}{6}{ARG2}
        \depedge{6}{8}{ARG2}
        \depedge{7}{8}{BV}
        \depedge{9}{11}{ARG2}
        \depedge{10}{11}{BV}
        \depedge{11}{14}{ARG1}
        \depedge{13}{14}{BV}
    \end{dependency}
    \caption{DM target representation.}
    \label{DM}
\end{figure}

\begin{figure}
    \centering
    \smaller[]
    % \smaller[]
    \begin{dependency}[]
        \begin{deptext}[column sep=0.2em, row sep=.1ex]
            Bramalea \& said \& it \& expects \& to \& complete \& the \& issue \& by \& the \& end \& of \& the \& month \& . \\
            \_ \& ev-w2833f1 \& \_ \& ev-w1239f1 \& \_ \& ev-w620f1 \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \& \_ \\
        \end{deptext}
        \deproot{2}{root}
        \depedge{2}{1}{ACT-arg}
        \depedge{6}{3}{ACT-arg}
        \depedge{4}{3}{ACT-arg}
        \depedge{2}{4}{EFF-arg}
        \depedge{4}{6}{PAT-arg}
        \depedge{6}{8}{PAT-arg}
        \depedge[edge unit distance=2.1ex]{6}{11}{TWHEN}
        \depedge{11}{14}{APP}
    \end{dependency}
    \caption{PSD target representation.}
    \label{PSD}
\end{figure}

\begin{displayquote}

\end{displayquote}

\paragraph{PCEDT: Prague Tectogrammatical Bi-Lexical Dependencies} The Prague Czech-English Dependency Treebank \cite{PCEDT}\footnote{See \url{http://ufal.mff.cuni.cz/pcedt2.0/}} is a dependency treebank over the WSJ from the PTB. The original English texts have been annotated along with annotated Czech translations. Similar to other treebanks from the PTB, these texts have been annotated with two layers of syntactic information: \textit{analytical} (a-layer) and \textit{tectogrammatical} (t-layer) \cite{Oepen:14}. The a-layer represents the so-called surface syntax, where the labels in the dependencies represent the syntactic information of the sentence. The t-layer is a layer where syntax and semantic dependencies are represented, and is based on the framework of the Functional Generative Description \cite{Sgall:86}. A conversion has been used in order to reach the SDP data format from this t-layer; see \citeA{Miyao:14} for details on the conversion from the t-layer of the PCEDT representation to the SDP representation.

\paragraph{PAS: Enju Predicate–Argument Structures} The Enju representation is based on Head-driven phrase structure grammar (HPSG), and is derived from the Enju HPSG treebank. This treebank is made by way of conversions from the phrase structure and predicate-argument representation of the PTB \cite{Oepen:14}. The PAS representation is extracted from the predicate-argument structures of the HPSDG Treebank. This predicate-argument structure represent bi-lexical semantic dependencies. As the PCEDT format, we refer the reader to \citeA{Miyao:14} for the technical details on the conversion to the SDP data format.

\paragraph{DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies} The semantic dependency graphs of the DM format are derived from the output of the ERG parser. This parser adds syntactic and semantic analysis by using the LinGO English Resource Grammar (LERG). LERG is, in a similar fashion to PAS, based on HPSG. It adds to the standard framework of HPSG by using Minimal Recursion Semantics for specifying semantic attributes, but does so without implementing the binding theory of HPSG \cite{Flickinger:00}. The DM representations are derived through a two-step `lossy' conversion. The first step in this conversion is to convert the MRSs to variable-free \textit{Elementary Dependency Structures}, and then a second step is applied where some information is lost in transforming the results of the previous step to the strictly bi-lexical SDP data format \cite{Miyao:14}.

\paragraph{PSD: Prague Semantic Dependencies} The PCEDT target representation is used as basis for arriving at the PSD\footnote{See \url{http://tinyurl.com/h8dfkcz} for more technical details on the PSD target representation and conversion from PCEDT.} target representation. This is made using a conversion. The PCEDT representation consists of dependency structures that are always rooted trees. This is due to technical aspects of the conversion from the t-layers mentioned above, to the PCEDT data format. For the SemEval-2015 task, a conversion of the PCEDT data's t-layer was performed in order to reach true bi-lexical dependencies.

\subsection{Semantic frames}
\label{sec:frames}

The SemEval-2015 introduced frames, also referred to as sense distinctions, to the DM and PSD target representations. These are added as an extra layer to the sentence where multiple classes are used in order to add additional information on the content words of a sentence. If we examine the semantic dependency graphs in Figure \ref{DM} and \ref{PSD}, we see these classes below the sentence.

According to \citeA{Oepen:16}, DM frames encode more general `linking patterns', which are mappings from syntactic to semantic arguments, whereas PSD represents actual sense identifiers and show different values for distinct lexemes. They note further that PSD only annotates senses on verbal predicates, while DM provide frame identifiers for all semantically contentful nodes. Additionally, \citeA{Oepen:15} note that the DM frames are limited to argument structure distinctions, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames. They further note that the PSD frames draw on much richer sense inventory, based on the EngValLex database \cite{Cinkova:06}.

As we will see in Chapter \ref{chap:analysis}, our analysis of the results of SemEval-2015 will lead us to further explore semantic frames and build our own \textit{frame classifier}. The details of this classifier and its results will be presented in Chapter \ref{chap:experiments}.

\section{Data sets}
\label{sec:data-sets}

The data-sets for the SemEval-2014 and 2015 vary slightly. In the SemEval-2014, the three annotations are over the same set of texts: Sections 00-21 of the WSJ corpus. A set of sentences where excluded from the data sets where (a) no gold-standard analysis existed; (b) it was not possible to align the tokens of each sentence on-to-one for all three representations; (c) there where cycles in at least on of the graphs of a sentence. After this cleanup, the SemEval-2014 data set counts 34,004 sentences (or a total of 745,543 tokens) for the training split (Sections 00-20), and 1,348 sentences (or a total of 29,808 tokens) for the test split (Section 21) \cite{Oepen:14}. The SemEval-2015 shared tasks used the same data set, but added a balanced corpus from the Brown Corpus. Also, the DM graphs where extracted from a later and improved release of the DeepBank (version 1.1). The exclusion of sentences from the data sets where a bit lower for SemEval-2015, and the training set counts 35,657 sentences (or a total of 802,717 tokens; roughly eight percent more than for SemEval-2014). For the test set 1,410 sentences (or a total of 31,948 tokens) from the WSJ Section 21 was reserved for in-domain testing, and 1,849 sentences (or a total of 31,583 tokens) from the Brown Corpus was reserved for out-of-domain testing \cite{Oepen:15}.

\begin{table}
    \centering
    \smaller[0.2]
    \begin{tabular}{@{}lllllll@{}}
        \toprule
        & \multicolumn{3}{c}{In-domain}
        & \multicolumn{3}{c}{Out-of-domain} \\
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
        \textbf{} & \textbf{DM} & \textbf{PAS} & \textbf{PSD} &
        \textbf{DM} & \textbf{PSD} & \textbf{PSD} \\
        \midrule
        \# labels & 59 & 42 & 91 & 47 & 41 & 74 \\
        \% singletons & 22.97 & 4.38 & 35.76 & 25.40 & 5.84 & 39.11 \\
        edge density & 0.96 & 1.02 & 1.01 &  0.95 & 1.02 & 0.99 \\
        $\%_g$ trees & 2.30 & 1.22 & 42.19 & 9.68 & 2.38 & 51.43 \\
        $\%_g$ noncrossing & 69.03 & 59.57 & 64.58 & 74.58 & 65.28 & 74.26 \\
        $\%_g$ projective & 2.91 & 1.64 & 41.92 & 8.82 & 3.46 & 54.35 \\
        $\%_g$ fragmented & 6.55 & 0.23 & 0.69 & 4.71 & 0.65 & 1.73 \\
        $\%_n$ reentrancies & 27.44 & 29.36 & 11.42 & 26.14 & 29.36 & 11.46 \\
        $\%_g$ topless & 0.31 & 0.02 & – & 1.41 & – & – \\
        \% top nodes & 0.996 & 0.999 & 1.127 & 0.985 & 1.000 & 1.264 \\
        $\%_n$ non-top roots & 44.91 & 55.98 & 4.35 & 39.89 & 50.93 & 5.27 \\
        average treewidth & 1.30 & 1.72 & 1.61 & 1.31 & 1.69 & 1.50  \\
        maximum treewidth & 3 & 3 & 7  & 3 & 3 & 5 \\
        \midrule
        \# frames & 297 & – & 5426  & 172 & – & 1208 \\
        $\%_n$ frames & 13.52 & – & 16.77 & 15.79 & – & 19.50 \\
        \bottomrule
    \end{tabular}
    \caption{High-level statistics on the SemEval-2015 data sets}
    \label{fig:data}
\end{table}

% \begin{table}
%     \centering  
%     \smaller[0.5]
%     \begin{tabular}{@{}lrrr@{}}
%         \toprule
%         & \textbf{DM} &  \textbf{PAS} & \textbf{PSD} & \\
%         \midrule
%         \texttt{adj} & 3144 & 95.89\% & 94.97\% \\
%         \bottomrule
%     \end{tabular}
%     \caption{Statistics of target representations for SemEval-2015.}
%     \label{fig:data-in-domain}
% \end{table}

\subsection{Quantitative Analysis of Data Sets}
\label{sec:quantitative}
% more here

In Figure \ref{fig:data} we present some high-level statistics on the SD 2015 data set. The data are reproduced from \citeA{Oepen:15}. The PSD representation is the most fine-grained in terms of the linguistic variation of dependency labels with 91 unique labels. DM and PAS are in this respect more coarse-grained and similar, also sharing a more similar naming and type convention.

When examining the percentage of trees, projective vs. non-projective graphs, and reentrancies, wee see that PSD is the most `tree-oriented' of the three target representations. As mentioned above, the PSD target representation is based on the PCEDT representation, which only consists of dependency structures that are rooted trees. 

As we can observe in Figure \ref{fig:data}, the number of frames in PSD is approximately 18 times that of the DM data set. 

\section{Submissions and Teams}
\label{sec:parsers}

In this Section we will examine a set of state-of-the art semantic dependency parsers. As mentioned previously, we will examine the submissions from the SemEval-2015, and not include the results from SemEval-2014. This choice is based on the observation that many of the same teams submitted for both tasks, that the SemEval-2015 shared tasks are an extension of the previous year, and that the scores of the submitted results saw an increase in overall performance.

% TO-DO: write more about the results.
We can observe the performance of the SemEval-2015 parsers in Table \ref{fig:results}, which have been reproduced from \citeA{Oepen:15}. There where 6 teams that submitted results from their parsing systems. Each team could submit two runs per track, and it is the best run (if more than one run were submitted) that is presented in Table \ref{fig:results}. 

The submissions could be made to different tracks. There was a \textit{closed} track: where systems could only use gold-standard semantic dependencies distributed by the organisers of the SemEval-2015 shared task for training. There was also an \textit{open} track: here the teams could use other resources, such as a syntactic parser, insofar as these did not use any methodology where the gold-standard syntactic or semantic analysis of the tasks' test data had been used in any way. For the open track, the organisers made available already parsed syntactic analysis of the training data. There was also a so-called idealized \textit{gold} track where gold-standard syntactic companion files in a variety of formats where provided for training \cite{Oepen:15}.

The evaluation of each parser is based on the accuracy of the dependency graphs that they produce on the test set mentioned above, measured against the gold-standard testing data. The evaluation itself is based on the metrics: 

\begin{enumerate}
    \item Labeled precision, recall, and F$_1$, referred to as LP, LR and LF.
    \item Complete predications: Which for the DM and PAS target representation means measuring all outgoing dependency edges, and for the PSD target representation to the ones where the label has an `-arg' suffix. Here too precision, recall and F$_1$ score is used, referred to as PP, PR, and PF.
    \item Semantic frames: This is comprised of a complete predication with scores for the frame (or sense) identifier. As other scores, this score is also represented by precision, recall and F$_1$ score, referred to as FP, FR, and FF.
    \item Both complete predications and semantic frame evaluations are limited to those predicates that correspond to verbal parts of speech, as determined by the gold-standard part of speech.
\end{enumerate}

We will now examine the parsing systems of the four best performing teams of SemEval-2015. We turn to the results of three of these systems in Chapter \ref{chap:analysis}, where we will examine, compare and contrast their results in a comprehensive manner.

\subsection{Peking} 
% ANALYSIS

The Peking team used two main approaches to solving the problem of semantic dependency parsing. The first approach consists of modifying transition-based models that are designed for handling trees to handle graphs. The second approach converts the training data, consisting of semantic dependency graphs, into tree structures using so-called tree approximation models. After the conversion is done, well-established methods for parsing dependency trees are used in order to train a parser. When predicting, a tree dependency structure is created, which is then converted to a graph dependency structure in order to reach the semantic dependency graph structure of the SemEval-2014 and 2015 data sets. This approach managed to produce high-quality parsing results. \citeA{Du:Peking:14} note that their experiments showed that graph-based models are more effective than transition-based models, and that a parser ensemble can boost parsing accuracy by taking the multiple outputs of several parsers and picking the most plausible results by way of a voting method.

The transition-based models for SemEval-2014 consist of 5 different transition models, which includes a so-called \textit{naive} approach using the shift-reduce method described in Section \ref{ssc:background:transition-based}, but with a $pop_k$ transition added: remove any element from the stack $\alpha$. They also use a transition based approach described in \cite{Titov:09}, which adds to the standard shift-reduce method a $pop$ transition: remove the top element from the stack $\alpha$, and a $swap$ transition: remove two top elements from the stack $\alpha$. The transition-based systems where trained twice on the training data, each sentence trained from start to end, but also backwards, from the last to the first element. With the 5 models, the system can thus return 10 parses of a given sentence.

The graph-based models for SemEval-2014 presented by the Peking team use tree approximations. \citeA{Du:Peking:14} argue that parsing based on graph spanning is a challenging task due to the fact that the graph structures represented by the data sets of SemEval-2014 are still relatively unexplored. In light of this observation, they developed a graph-to-tree transformation which was used to transform the SemEval-2014 data sets to trees. A tree parser is then used to train a model on the transformed data, and finally a tree-to-graph transformation was developed in order to transform the result of the parser back to the graph data structure of the SemEval-2014 data sets. These steps are \textit{lossy}, i.e. information is possibly degradable in the transformation processes, and can be lost, added or modified in the process.

The ensemble method that \citeA{Du:Peking:14} employ examines the output of 10 transition-based models, and 9 graph-based models, and use a simple voting scheme in order to combine their outputs. This is based on the frequency of the dependencies when combining all models, and given a threshold, if a dependency exceeds it, it is chosen to be part of the result. For the edges that have been chosen, the most frequent label from the 19 models is added. The models are scored by a weighting scheme, so that the graph-based models have a higher score on their edges and labels, since these proved to produce better overall results.

For SemEval-2015 the Peking team further developed this ensemble method, focusing more on the graph-based models. \citeA{Du:Peking:15} developed a \textit{weighted tree approximation model}, where the graph-to-tree transformation is based on weights for the type of transformation that is to take place. When the weights have been calculated, the transformation from a graph to a tree is solved by using maximum spanning tree (MST) algorithms. For SemEval-2015 the transformation is improved by adding additional labels to the trees for a set of specific cases when an edge is lost in the transformation. This allows for a more accurate tree-to-graph reversal. See \cite{Du:Peking:15} for the details on the transformations.

The same type of voting ensemble used in SemEval-2014 is used to combine the outputs of the 10 transition-based models, 9 tree approximation models, and 4 new weighted models added for SemEval-2015. The results showed an increase in overall accuracy, showing that the improvements to the tree approximation models proved to be fruitful.


\subsection{Riga}
% Articles are not easy to read, explanations are poor, and the references are not very clear.

The team behind the Riga system used the system developed by the Peking team for SemEval-2014 as basis for implementing their own semantic dependency parser with a model for predicting semantic frames for SemEval-2015. According to \citeA{Barzdins:Riga:15}, the Riga teams approach involves taking the Peking system, removing some less essential components, and adding a rule-based classifier for both graph parsing and frame labeling. The added system has been dubbed the \textit{C6.0 rule-based classifier}, and used for both graph parsing and frame labeling. See \citeA{Barzdins:Riga:14} for the technical details on the classifier.

The Riga team further developed the approach of Peking by modifying the tree approximation model. Instead of the \textit{lossy} method described above, the Riga team developed a fully reversible depth-first transformation. As a baseline they used the Mate parser, see technical details of this parser here: \cite{Boh:10}, which immediately produced results on par with the the highest results from SemEval-2014. For SemEval-2015, the Riga team describe three approaches that improved upon this baseline. These three approaches revolve around the \textit{lossless} tree-approximation method, where the information that might be lost in the graph-to-tree transformation is stored in the edges of the dependency tree. The Mate parser is then used to train a parser given these trees with additional edge information. A restructure algorithm is then used to transform the trees back to graphs \cite{Barzdins:Riga:15}.

As already mentioned, a rule-based classifier dubbed C6.0 was used as basis for developing a classifier for predicting frames. This approach involves a refining the C6.0 classifier from performing an exhaustive search to a greedy search algorithm with a multi-class classifier as basis. For training, a simple Laplace ratio prediction method is used to count the number of instances that a feature occurs and does not occur with any given frame in the data set. A greedy search is performer over all classes in order to predict frames. See \citeA{Barzdins:Riga:15} for the specific technical descriptions on the semantic parsing and frame prediction.

\subsection{Turku}
% ANALYSIS

For SemEval-2014 the Turku team developed a semantic dependency parser by combining several classifiers trained with different machine learning algorithms. The \textit{LIBSVM} package: see \citeA{Chang:LIBSVM:11} for a description, which is a binary support vector machine classifier, is used for detecting dependencies. For the dependency labels the \textit{SVM-multiclass} package by \citeA{Jo:SVM:99} is used to predict the semantic label of the edges predicted in the previous step. The Turku team participated in the open track of the SemEval-2014 task and used a large corpus of syntactic n-grams, and a \textit{word2vec} word similarity model, to improve the classification of labels by using the cosine similarity measures in the word2vec model: see \citeA{Kanerva:Turku:14} for specification on the models and training. The last step in this pipeline involves a classification of top nodes, since these are not classified in the first step. A support vector machine classifier is trained in order to predict the top nodes \cite{Kanerva:Turku:14}. 

For SemEval-2015 the Turku team improved upon the combined classifier approach from the previous year. Adding a so-called structured support vector machine, the new classifier would get a higher score by taking into account a more global view while training and classifying, instead of the local view of just the head-dependent relationships between lexical units that had been the previous approach. As \citeA{Kanerva:Turku:14} explain, their approach when dealing with the dependency relations is to predict each predicate independently, i.e. no other predicates or arguments affect the prediction. However, when predicting arguments for one predicate, a global view is kept for the already predicted arguments for this particular predicate. 

The team behind Turku where the only team that submitted results in both the open and gold tracks of SemEval-2015. They have made their parser openly available for others to use as an off-the-shelf parser\footnote{\url{https://github.com/jmnybl/Turku-Dependency-Parser}}, which is itself based on the parser of \citeA{Boh:Kuhn:12}. As we can observer in Table \ref{fig:results}, the Turku parser has the overall highest score of the parsing systems. However, this is in part attributed to the fact that the Turku team where the only team participating the the gold track, and thus use gold-standard syntactic companion files as part of training. If we examine the results for the open track, we see that the Turku submission performed substantially lower than the Lisbon team. 

\subsection{Lisbon} 
% ANALYSIS

The team behind Lisbon managed to produce state-of-the art results from their parsing system, examining Table \ref{fig:results} we see that in the in-domain data set it produced the second highest score in the open track, and fourth highest score in the closed track, and for the out-of-domain data set it produced the second highest score in the open track, and third highest score in the closed track. The Turku parser got a higher score in both data sets, but this can be attributed to the fact that those scores where in the gold track.

The Lisbon teams semantic dependency system is dubbed the \textit{TurboSemanticParser}\footnote{\url{http://labs.priberam.pt/Resources/TurboSemanticParser}}, and which is available as open source software. This system is the basis for the submissions for both SemEval-2014 and SemEval-2015. 

The \textit{TurboSemanticParser} consists of a feature-rich linear model that parametrize globally over first and second order dependencies (arcs, siblings, grandparents and co-parents). As noted by \citeA{Mar:Al:Lisbon:14}, the Lisbon system cast parsing as a structured prediction problem: \say{Let $x$ be a sentence and $Y(x)$ the set of possible dependency graphs. We assume each candidate graph $y \in Y(x)$ can be represented as a set of substructures (called parts) in an underlying set $S$ (\textit{e.g.,} predicates, arcs, pairs of adjacent arcs)}. A score function $f$ will then decompose a sum over the substructures, and the highest scoring semantic graph is then chosen for a given sentence using dynamic programming.

A so-called \textbf{alternating directions dual decomposition} is used in order to approximate the highest scoring graph in order to reduce the search field. This reduces the original problem of finding the highest scoring dependency graph into sub-problems. These sub-problems are \textit{predicate and arc-factored parts}, \textit{unique roles}, \textit{grandparents, arbitrary siblings and co-parents}, \textit{predicate automata} and \textit{argument automata}. For the technical details on training and parsing see: \citeA{Mar:Al:Lisbon:14}.


\begin{table}
    \centering
    \smaller[]
    \smaller[]
    \smaller[0.5]
    \begin{tabular}{@{}cccccccccccccc@{}}
        \toprule
        \multicolumn{1}{c}{ }
        & \multicolumn{1}{c}{ }
        & \multicolumn{4}{c}{\textbf{DM}}
        & \multicolumn{4}{c}{\textbf{PAS}}
        & \multicolumn{4}{c}{\textbf{PSD}} \\
        \cmidrule(lr){3-6}
        \cmidrule(lr){7-10}
        \cmidrule(lr){11-14}
        &
        LF.av &
        LF & LP & LR & FF &
        LF & LP & LR & PF &
        LF & LP & LR & FF \\
        \midrule
        Turku\# & 86.81 & 88.29 & 89.52 & 87.09 & 58.39 & 95.58 & 95.94 & 95.21 & 87.99 & 76.57 & 78.24 & 74.97 & 56.85 \\
        Lisbon* & 86.23 & 89.44 & 90.52 & 88.39 & 00.20 & 91.67 & 92.45 & 90.90 & 84.18 & 77.58 & 79.88 & 75.41 & 00.06 \\
        Peking & 85.33 & 89.09 & 90.93 & 87.32 & 63.08 & 91.26 & 92.90 & 89.67 & 79.08 & 75.66 & 78.60 & 72.93 & 49.95 \\
        Lisbon & 85.15 & 88.21 & 89.84 & 86.64 & 00.15 & 90.88 & 91.87 & 89.92 & 81.74 & 76.36 & 78.62 & 74.23 & 00.03 \\
        Riga & 84.00 & 87.90 & 88.57 & 87.24 & 58.12 & 90.75 & 91.50 & 90.02 & 80.03 & 73.34 & 75.25 & 71.52 & 52.54 \\
        Turku* & 83.47 & 86.17 & 87.80 & 84.60 & 54.67 & 90.62 & 91.38 & 89.87 & 80.60 & 73.63 & 76.10 & 71.32 & 53.20 \\
        Minsk & 80.74 & 84.13 & 86.28 & 82.09 & 54.24 & 85.24 & 87.28 & 83.28 & 64.66 & 72.84 & 74.65 & 71.13 & 51.63 \\
        In-House* & 61.61 & 92.80 & 92.85 & 92.75 & 83.79 & 92.03 & 92.07 & 91.99 & 87.24 & – & – & – & – \\
        \bottomrule
        
        \\
        \toprule
        \multicolumn{1}{c}{ }
        & \multicolumn{1}{c}{ }
        & \multicolumn{4}{c}{\textbf{DM}}
        & \multicolumn{4}{c}{\textbf{PAS}}
        & \multicolumn{4}{c}{\textbf{PSD}} \\
        \cmidrule(lr){3-6}
        \cmidrule(lr){7-10}
        \cmidrule(lr){11-14}
        &
        LF.av &
        LF & LP & LR & FF &
        LF & LP & LR & PF &
        LF & LP & LR & FF \\
        \midrule
        Turku\# & 83.50 & 82.11 & 84.26 & 80.07 & 42.89 & 92.92 & 93.52 & 92.33 & 83.80 & 75.47 & 77.77 & 73.31 & 42.37 \\
        Lisbon* & 82.53 & 83.77 & 85.79 & 81.84 & 00.35 & 87.63 & 88.88 & 86.41 & 80.19 & 76.18 & 80.12 & 72.61 & 02.25 \\
        Lisbon & 81.15 & 81.75 & 84.81 & 78.90 & 00.27 & 86.88 & 88.52 & 85.30 & 78.47 & 74.82 & 78.68 & 71.31 & 02.09 \\
        Peking & 80.78 & 81.84 & 84.29 & 79.53 & 47.49 & 87.23 & 89.47 & 85.10 & 74.75 & 73.28 & 77.36 & 69.61 & 34.28 \\
        Riga & 79.23 & 80.69 & 81.69 & 79.72 & 41.88 & 86.63 & 87.56 & 85.72 & 76.26 & 70.37 & 73.23 & 67.71 & 40.76 \\
        Turku* & 78.85 & 79.01 & 81.54 & 76.63 & 39.15 & 85.95 & 86.95 & 84.98 & 76.38 & 71.59 & 74.92 & 68.55 & 38.75 \\
        Minsk & 75.79 & 77.24 & 80.24 & 74.46 & 42.18 & 80.44 & 83.07 & 77.96 & 62.00 & 69.68 & 72.26 & 67.27 & 41.25 \\
        In-House* & 59.24 & 89.69 & 89.80 & 89.58 & 76.39 & 88.03 & 88.10 & 87.96 & 81.69 & – & – & – & – \\
        \bottomrule
    \end{tabular}
    \caption{SemEval-2015 results from the gold track (marked \#), open track (marked *) and closed track (unmarked) of the in-domain (top) and out-of-domain (bottom). LF.av indicates the average LF score across all representations, and is used to rank the systems in their overall performance.}
    \label{fig:results}
\end{table}

\section{Conclusions}

In this chapter we have reviewed state-of-the-art semantic dependency parsers by examining a chosen set of parsing systems from SemEval-2015. Overall, we can state that our review shows that the highest scoring systems are based no graph-based models rather than transition-based models among our chosen set. Tree-approximation models that use already well studied and highly developed syntactic dependency parsing systems can create highly accurate semantic dependency parsers with an intermediary step where graphs are transformed to trees.

From here on we will go more in-depth in studying the results of three parsing systems among the ones presented in this chapter. In the next chapter we focus on Peking, Turku and the Lisbon parsing systems. This choice is grounded in the fact that these are among the most highest scoring systems. The choice of disregarding the Riga system also stems from the fact that it is based on the Peking system, and the assumption that the type of errors that this system makes would resemble the Peking system seems legitimate. We now turn to our in-depth contrastive error analysis.